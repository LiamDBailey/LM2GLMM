---
title: "GLM: Intervals & Tests"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{3.1 Intervals & Tests}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
library(LM2GLMM)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/GLM_uncertainty/", fig.path = "./fig_knitr/GLM_uncertainty/", fig.width = 4, fig.height = 4, fig.align = "center")
```

## You will learn in this session


## Our toy models

```{r}
set.seed(1L)
Aliens <- data.frame(humans_eaten = round(runif(n = 100, min = 0, max = 15)))
Aliens$size  <- rnorm( n = 100, mean = 50 + 1.5 * Aliens$humans_eaten, sd = 5)
Aliens$eggs  <- rpois( n = 100, lambda = exp(-1 + 0.1 * Aliens$humans_eaten))
Aliens$happy <- rbinom(n = 100, size = 1, prob = plogis(-3 + 0.3 * Aliens$humans_eaten))
Aliens$all_eyes  <- round(runif(nrow(Aliens), min = 1, max = 12))
Aliens$blue_eyes <- rbinom(n = nrow(Aliens), size = Aliens$all_eyes, prob = plogis(-2 + 0.5 * Aliens$humans_eaten))
Aliens$pink_eyes <- Aliens$all_eyes - Aliens$blue_eyes

mod_gauss <- glm(size  ~ humans_eaten, family = gaussian(), data = Aliens)
mod_poiss <- glm(eggs  ~ humans_eaten, family = poisson(),  data = Aliens)
mod_binar <- glm(happy ~ humans_eaten, family = binomial(), data = Aliens)
mod_binom <- glm(cbind(blue_eyes, pink_eyes) ~ humans_eaten, family = binomial(), data = Aliens)
```


# Uncertainty in estimates

## Covariances between parameter estimates

Poisson family:

<br>

```{r}
X <- model.matrix(mod_poiss)
W <- matrix(0, ncol = nrow(mod_poiss$model), nrow = nrow(mod_poiss$model))
diag(W) <- mod_poiss$weights
t(X) %*% W %*% X
(XTWX <- crossprod(mod_poiss$R))
```

## Covariances between parameter estimates

Poisson family:

<br>

```{r}
vcov(mod_poiss)
phi <- 1
phi*solve(XTWX)
```

## Covariances between parameter estimates

Gaussian family:

<br>

```{r}
vcov(mod_gauss)
XTWX <- crossprod(mod_gauss$R)
phi <- mod_gauss$deviance / mod_gauss$df.residual
phi*solve(XTWX)
```

## Covariances between parameter estimates

Binomial family (binary case):

<br>

```{r}
vcov(mod_binar)
XTWX <- crossprod(mod_binar$R)
phi <- 1
phi*solve(XTWX)
```

## Covariances between parameter estimates

Binomial family (general case):

<br>

```{r}
vcov(mod_binom)
XTWX <- crossprod(mod_binom$R)
phi <- 1
phi*solve(XTWX)
```


## Distribution of parameter estimates

As for LM the parameter estimates are normally distributed.

```{r}
new_betas <- t(replicate(1000, {Aliens$new.y <- as.matrix(poisson()$simulate(mod_poiss, 1))
  coef(update(mod_poiss, new.y ~ .))}))
curve(pnorm(x, mean = coef(mod_poiss)[1], sd = sqrt(vcov(mod_poiss)[[1]])), from = min(new_betas[, 1]),
      to = max(new_betas[, 1]), ylab = "cdf", lwd = 3, xlab = "Intercept in mod_poiss")
plot(ecdf(new_betas[, 1]), col = "red", lwd = 2, add = TRUE)
```


## Distribution of parameter estimates

As for LM the parameter estimates are normally distributed.

```{r}
curve(pnorm(x, mean = coef(mod_poiss)[2], sd = sqrt(vcov(mod_poiss)[[4]])), from = min(new_betas[, 2]),
      to = max(new_betas[, 2]), ylab = "cdf", lwd = 3, xlab = "Slope in mod_poiss")
plot(ecdf(new_betas[, 2]), col = "red", lwd = 2, add = TRUE)
```


## Distribution of parameter estimates

As for LM the parameter estimates are normally distributed.

```{r}
new_betas <- t(replicate(1000, {Aliens$new.y <- as.matrix(binomial()$simulate(mod_binar, 1))
coef(update(mod_binar, new.y ~ .))}))
curve(pnorm(x, mean = coef(mod_binar)[1], sd = sqrt(vcov(mod_binar)[[1]])), from = min(new_betas[, 1]),
      to = max(new_betas[, 1]), ylab = "cdf", lwd = 3, xlab = "Intercept in mod_binar")
plot(ecdf(new_betas[, 1]), col = "red", lwd = 2, add = TRUE)
```


## Distribution of parameter estimates

As for LM the parameter estimates are normally distributed.

```{r}
curve(pnorm(x, mean = coef(mod_binar)[2], sd = sqrt(vcov(mod_binar)[[4]])), from = min(new_betas[, 2]),
      to = max(new_betas[, 2]), ylab = "cdf", lwd = 3, xlab = "Slope in mod_binar")
plot(ecdf(new_betas[, 2]), col = "red", lwd = 2, add = TRUE)
```




# Goodness of fit (work in progress)

## Goodness of fit?

```{r}
set.seed(1L)
x <- seq(1, 2, length = 100)
y <- exp(rnorm(n = length(x), mean = 2 + 1 * x, sd = 0.5))

mod_lm   <- lm(log(y) ~ x)
mod_glm  <- glm(y ~ x, family = gaussian(link = "log"))
logLikH0 <- replicate(1000, {
  new.y <- exp(as.matrix(simulate(mod_lm)))
  logLik(lm(log(new.y) ~ x)) - logLik(glm(new.y ~ x, family = gaussian(link = "log")))
  })
mean(logLikH0 < (logLik(mod_lm)-logLik(mod_glm)))
```

## Goodness of fit?


```{r}
set.seed(1L)
x <- seq(1, 2, length = 100)
y <- exp(2 + 1 * x) + rnorm(n = length(x), mean = 0, sd = 5)

mod_lm   <- lm(log(y) ~ x)
mod_glm  <- glm(y ~ x, family = gaussian(link = "log"))
difflogLikH0 <- replicate(1000, {
  new.y <- as.matrix(simulate(mod_glm))
  logLik(glm(new.y ~ x, family = gaussian(link = "log"))) - logLik(lm(log(new.y) ~ x))
  })
hist(difflogLikH0)
abline(v = logLik(mod_glm)-logLik(mod_lm), col = 2, lwd = 2)
mean((logLik(mod_glm)-logLik(mod_lm)) > logLikH0)


c(mean(logLikH0), mean(logLik(mod_glm)-logLik(mod_lm)))
```

## Goodness of fit?


```{r}
set.seed(1L)
x <- seq(1, 2, length = 100)
y <- exp(2 + 1 * x) + rnorm(n = length(x), mean = 0, sd = 5)
mod_glm  <- glm(y ~ x, family = gaussian(link = "log"))
logLikBoot <- replicate(1000, {
  new.y <- as.matrix(simulate(mod_glm))
  logLik(glm(new.y ~ x, family = gaussian(link = "log")))
  })
hist(logLikBoot, xlim = range(logLikBoot, logLik(mod_glm)))
abline(v = logLik(mod_glm), col = 2, lwd = 2)
mean(logLik(mod_glm) > logLikBoot)
```

## What you need to remember



# Table of content

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Overdispersion & Zero-inflation](./GLM_overdispersion.html)
* 3.4 [Let's practice more](./GLM_practice.html)

