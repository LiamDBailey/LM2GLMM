---
title: "GLM: Intervals & Tests"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{3.1 Intervals & Tests}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(car)
library(MASS)
library(spaMM)
spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/GLM_uncertainty/", fig.path = "./fig_knitr/GLM_uncertainty/", fig.width = 4, fig.height = 4, fig.align = "center", error = TRUE)
```

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Let's practice more](./GLM_practice.html)

## You will learn in this session

* that estimates are still roughly normally distributed (but only roughly)
* how to compute intervals for GLM
* that intervals based on likelihood are best for family with known dispersion
* that established methods don't always work well, so you should always run tests
* that parametric bootstrap can outperform other methods when done properly
* that intervals on scale other than $\eta$ must be done by hand or using ```spaMM```
* that tests from summary tables are only reliable for the gaussian family
* that LR tests are best for family with known dispersion
* that LR tests by parametric bootstrap are best in all situations and easy to do with ```spaMM```


## Our data

```{r data}
set.seed(1L)
Aliens <- simulate_Aliens_GLM()
head(Aliens)
attributes(Aliens)$param.eta$size
attributes(Aliens)$param.eta$blue_eyes
```


## Our toy models

<br>

```{r models}
mod_gauss <- glm(size  ~ humans_eaten, family = gaussian(), data = Aliens)
mod_poiss <- glm(eggs  ~ humans_eaten, family = poisson(),  data = Aliens)
mod_binar <- glm(happy ~ humans_eaten, family = binomial(), data = Aliens)
mod_binom <- glm(cbind(blue_eyes, pink_eyes) ~ humans_eaten, family = binomial(), data = Aliens)
```

# Uncertainty in estimates

## Covariances between parameter estimates

### gaussian family

<br>

```{r vcov gaussian}
X <- model.matrix(mod_gauss)
W <- matrix(0, ncol = nrow(mod_gauss$model), nrow = nrow(mod_gauss$model))
diag(W) <- mod_gauss$weights
XTWX <- t(X) %*% W %*% X
phi <- mod_gauss$deviance / mod_gauss$df.residual
phi*solve(XTWX)
vcov(mod_gauss)
```

## Covariances between parameter estimates

### gaussian family

<br>

Same using the QR decomposition:

```{r vcov gaussian 2}
(XTWX <- crossprod(mod_gauss$R))
phi <- mod_gauss$deviance / mod_gauss$df.residual
phi*solve(XTWX)
```


## Covariances between parameter estimates

### Poisson family

<br>

```{r vcov poisson}
vcov(mod_poiss)
XTWX <- crossprod(mod_poiss$R)
phi <- 1
phi*solve(XTWX)
```


## Covariances between parameter estimates

### Binomial family (binary case)

<br>

```{r vcov binar}
vcov(mod_binar)
XTWX <- crossprod(mod_binar$R)
phi <- 1
phi*solve(XTWX)
```

## Covariances between parameter estimates

### Binomial family (general case)

<br>

```{r vcov binom}
vcov(mod_binom)
XTWX <- crossprod(mod_binom$R)
phi <- 1
phi*solve(XTWX)
```


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu beta gaussian}
new_betas <- t(replicate(1000, update(mod_gauss, data = simulate_Aliens_GLM(N = 1000))$coef))
```

<div class="columns-2">
```{r qq gaussian 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq gaussian 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu beta gaussian bis}
new_betas <- t(replicate(1000, update(mod_gauss, data = simulate_Aliens_GLM(N = 10))$coef))
```

<div class="columns-2">
```{r qq gaussian bis 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq gaussian bis 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>

## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu poiss, warning = FALSE}
new_betas <- t(replicate(1000, update(mod_poiss, data = simulate_Aliens_GLM(N = 1000))$coef))
```

<div class="columns-2">
```{r qq poiss 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq poiss 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu poiss bis, warning = FALSE}
new_betas <- t(replicate(1000, update(mod_poiss, data = simulate_Aliens_GLM(N = 10))$coef))
```

<div class="columns-2">
```{r qq poiss bis 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq poiss bis 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>

## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu binar}
new_betas <- t(replicate(1000, update(mod_binar, data = simulate_Aliens_GLM(N = 1000))$coef))
```

<div class="columns-2">
```{r qq binar 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq binar 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu binar bis, warning = FALSE}
new_betas <- t(replicate(1000, update(mod_binar, data = simulate_Aliens_GLM(N = 10))$coef))
```

<div class="columns-2">
```{r qq binar bis 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq binar bis 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu binom}
new_betas <- t(replicate(1000, update(mod_binom, data = simulate_Aliens_GLM(N = 1000))$coef))
```

<div class="columns-2">
```{r qq binom 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq binom 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

The parameter estimates are asymptotically normally distributed.

```{r simu binom bis, warning = FALSE}
new_betas <- t(replicate(1000, update(mod_binom, data = simulate_Aliens_GLM(N = 10))$coef))
```

<div class="columns-2">
```{r qq binom bis 1}
qqnorm(new_betas[, 1])
qqline(new_betas[, 1], col = "red", lwd = 2)
```

```{r qq binom bis 2}
qqnorm(new_betas[, 2])
qqline(new_betas[, 2], col = "red", lwd = 2)
```
</div>


## Distribution of parameter estimates

### Conclusion

* Large dataset: the assumption of normality for the distribution of parameter estimates holds well.

* Small dataset: the assumption of normality for the distribution of parameter estimates holds well for ```gaussian()```, but it really does not for the other families.

<br>

### Open question

How small is small? -> Simulate as we did to check for your design.

<br>

### Challenge

Determine if the issue is steming from the link function, the family or both.

Tip: test by simulation using ```gaussian(link = "gaussian")``` and ```poisson(link = "identity")```.


# Confidence intervals on estimates

# CI by gaussian approximation

## 95% CI using gaussian approximation

### Computation
```{r confint fn}
confint.approx <- function(mod, print = FALSE) {
  
  intervals <- cbind(lwr = (mod$coefficients + qnorm(0.025) *  sqrt(diag(vcov(mod)))),
                     upr = (mod$coefficients + qnorm(0.975) *  sqrt(diag(vcov(mod)))))
  
  if (print) print(intervals)
  name.response <- mod$terms[[2]]
  if (length(name.response) == 3) name.response <- name.response[[2]]
  true.intercept <- attributes(mod$data)$param.eta[[paste(name.response)]][1]
  true.slope <- attributes(mod$data)$param.eta[[paste(name.response)]][2]
  if (print) cat("\n true parameters in 95% CI? \n")
  
  c(true.intercept > intervals["(Intercept)", "lwr"] & true.intercept < intervals["(Intercept)", "upr"],
    true.slope > intervals["humans_eaten", "lwr"] & true.slope < intervals["humans_eaten", "upr"])
}
```


## 95% CI using gaussian approximation

### Testing our function
```{r confint test 1}
confint.approx(mod_poiss, print = TRUE)
```


## 95% CI using gaussian approximation

### Coverage
```{r confint test 2}
set.seed(1L)
replicate(5, confint.approx(update(mod_poiss, data = simulate_Aliens_GLM())))
```


## 95% CI using gaussian approximation

### Coverage N = 10
```{r coverage1b, warning = FALSE}
set.seed(3L)
test_poiss <- replicate(1000, confint.approx(update(mod_poiss, data = simulate_Aliens_GLM(N = 10))))
test_binar <- replicate(1000, confint.approx(update(mod_binar, data = simulate_Aliens_GLM(N = 10))))
test_binom <- replicate(1000, confint.approx(update(mod_binom, data = simulate_Aliens_GLM(N = 10))))
test_gauss <- replicate(1000, confint.approx(update(mod_gauss, data = simulate_Aliens_GLM(N = 10))))
rbind(poiss = apply(test_poiss, 1, mean, na.rm = TRUE),
      binar = apply(test_binar, 1, mean, na.rm = TRUE),
      binom = apply(test_binom, 1, mean, na.rm = TRUE),
      gauss = apply(test_gauss, 1, mean, na.rm = TRUE))
```

Bad for all!


## 95% CI using gaussian approximation

### Coverage N = 2000
```{r coverage1c, error = TRUE}
set.seed(1L)
test_poiss <- replicate(1000, confint.approx(update(mod_poiss, data = simulate_Aliens_GLM(N = 2000))))
test_binar <- replicate(1000, confint.approx(update(mod_binar, data = simulate_Aliens_GLM(N = 2000))))
test_binom <- replicate(1000, confint.approx(update(mod_binom, data = simulate_Aliens_GLM(N = 2000))))
test_gauss <- replicate(1000, confint.approx(update(mod_gauss, data = simulate_Aliens_GLM(N = 2000))))
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

Good for all!


## 95% CI using gaussian approximation

### Coverage N = 100
```{r coverage1}
set.seed(1L)
test_poiss <- replicate(1000, confint.approx(update(mod_poiss, data = simulate_Aliens_GLM(N = 100))))
test_binar <- replicate(1000, confint.approx(update(mod_binar, data = simulate_Aliens_GLM(N = 100))))
test_binom <- replicate(1000, confint.approx(update(mod_binom, data = simulate_Aliens_GLM(N = 100))))
test_gauss <- replicate(1000, confint.approx(update(mod_gauss, data = simulate_Aliens_GLM(N = 100))))
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

Good for all but ```gaussian()``` which would need to rely on the quantiles from the student distribution!


## 95% CI using gaussian approximation

### Conclusion

* Large datasets: good coverage for all families.

* Small datasets: bad coverage specifically for family for which a dispersion parameters must be estimated. This is a problem for ```gaussian()``` but also for other families we have not seen yet (the ```quasiXXX()```).


## 95% CI on two parameters using gaussian approximation

```{r, message = FALSE}
library(ellipse)
el <- ellipse(mod_poiss)
plot(el, type = "l")
```

# CI by likelihood profiling

## Recall: likelihood profiling

```{r profile}
loglik <- sapply(candidate_slopes <- seq(-0.05, 0.15, 0.01), function(param) {
  logLik(glm(eggs ~ 1 + offset(param*humans_eaten),
                        family = poisson(), data = mod_poiss$model))[[1]]
})
plot(loglik ~ candidate_slopes, type = "l")
abline(v = coef(mod_poiss)[2], lty = 2, col = "blue")
abline(h = logLik(mod_poiss)[[1]] - 0.5*qchisq(0.95, df = 1), lty = 2, col = "red")
```

## 95% CI by likelihood profiling

### Computation by hand

```{r CI by profile hand}
where.is.limit <- function(param, which = 1) {
  if (which == 1) {
    logLik_mod <- logLik(glm(eggs ~ 0 + humans_eaten + offset(rep(param, nrow(mod_poiss$model))),
                       family = poisson(), data = mod_poiss$model))[[1]]
  } else if (which == 2) {
    logLik_mod <- logLik(glm(eggs ~ 1 + offset(param*humans_eaten),
                        family = poisson(), data = mod_poiss$model))[[1]]
  }
  
  logLik_goal <- logLik(mod_poiss)[[1]] - 0.5*qchisq(0.95, df = 1)
  return(abs(logLik_mod - logLik_goal))
  }
```

```{r CI by profile hand action, warning = FALSE}
intercept_lwr <- nlminb(start = -10, where.is.limit, which = 1)$par
intercept_upr <- nlminb(start = 10, where.is.limit, which = 1)$par
slope_lwr     <- nlminb(start = -1, where.is.limit, which = 2)$par
slope_upr     <- nlminb(start = 2, where.is.limit, which = 2)$par
rbind(c(intercept_lwr, intercept_upr), c(slope_lwr, slope_upr))
```


## 95% CI by likelihood profiling

### Computation using spaMM

```{r confint spaMM}
library(spaMM)
mod_poiss_spaMM <- fitme(eggs  ~ humans_eaten, family = poisson(),  data = Aliens)
confint(mod_poiss_spaMM, parm = 1)
confint(mod_poiss_spaMM, parm = 2)
```

<br>

This implementation is direclty equivalent to the one we just saw.

## 95% CI by likelihood profiling

### Computation using MASS
```{r confint mass}
library(MASS)
confint(mod_poiss)
```


## 95% CI by likelihood profiling

### Computation using MASS
```{r confint prof}
prof <- profile(mod_poiss, alpha = (1 - 0.95)/4)  ## not sure why this alpha, but confint.glm does that
pro <- prof$`(Intercept)`
pro[1:3, ]
m <- update(mod_poiss, . ~ 0 + offset(-1.17095138 + 0.10732524 * humans_eaten))
sqrt((m$deviance - mod_poiss$deviance)/1)
sp <- spline(x = pro[, "par.vals"][, 1], y = pro[, 1])
approx(sp$y, sp$x, xout = qnorm(c(0.025, 0.975)))$y
```


## 95% CI using likelihood profiling

### Coverage of ```confint.glm```
```{r}
confint.profile <- function(mod, print = FALSE) {
  
  suppressMessages(intervals <- MASS:::confint.glm(mod))
  
  if (print) print(intervals)
  name.response <- mod$terms[[2]]
  if (length(name.response) == 3) name.response <- name.response[[2]]  ## for  binomial with cbind
  true.intercept <- attributes(mod$data)$param.eta[[paste(name.response)]][1]
  true.slope <- attributes(mod$data)$param.eta[[paste(name.response)]][2]
  if (print) cat("\n true parameters in 95% CI? \n")
  
  c(true.intercept > intervals["(Intercept)", "2.5 %"] & true.intercept < intervals["(Intercept)", "97.5 %"],
    true.slope > intervals["humans_eaten", "2.5 %"] & true.slope < intervals["humans_eaten", "97.5 %"])
}
```


## 95% CI using likelihood profiling

### Coverage N = 100
```{r  coverage2, message = FALSE, warning = FALSE}
set.seed(1L)
test_poiss <- replicate(1000, confint.profile(update(mod_poiss, data = simulate_Aliens_GLM(N = 100))))
test_binar <- replicate(1000, confint.profile(update(mod_binar, data = simulate_Aliens_GLM(N = 100))))
test_binom <- replicate(1000, confint.profile(update(mod_binom, data = simulate_Aliens_GLM(N = 100))))
test_gauss <- replicate(1000, confint.profile(update(mod_gauss, data = simulate_Aliens_GLM(N = 100))))
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

Good for all but ```gaussian()```.

## 95% CI using likelihood profiling

### Coverage  N = 20
```{r  coverage3, message = FALSE, warning = FALSE}
set.seed(1L)
test_poiss <- replicate(1000, confint.profile(update(mod_poiss, data = simulate_Aliens_GLM(N = 20))))
test_binar <- replicate(1000, confint.profile(update(mod_binar, data = simulate_Aliens_GLM(N = 20))))
test_binom <- replicate(1000, confint.profile(update(mod_binom, data = simulate_Aliens_GLM(N = 20))))
test_gauss <- replicate(1000, confint.profile(update(mod_gauss, data = simulate_Aliens_GLM(N = 20))))
rbind(poiss = apply(test_poiss, 1, mean, na.rm = TRUE),
      binar = apply(test_binar, 1, mean, na.rm = TRUE),
      binom = apply(test_binom, 1, mean, na.rm = TRUE),
      gauss = apply(test_gauss, 1, mean, na.rm = TRUE))
```

Good for all!! but ```gaussian()```.


## 95% CI using likelihood profiling

### Coverage  N = 1000
```{r  coverage4, message = FALSE, warning = FALSE}
set.seed(1L)
test_poiss <- replicate(1000, confint.profile(update(mod_poiss, data = simulate_Aliens_GLM(N = 1000))))
test_binar <- replicate(1000, confint.profile(update(mod_binar, data = simulate_Aliens_GLM(N = 1000))))
test_binom <- replicate(1000, confint.profile(update(mod_binom, data = simulate_Aliens_GLM(N = 1000))))
test_gauss <- replicate(1000, confint.profile(update(mod_gauss, data = simulate_Aliens_GLM(N = 1000))))
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

Good for all.


## 95% CI using likelihood profiling

### Conclusion

Better than CI by gaussian approximation!

### Recommendation

Use student-based CI distribution for gaussian case, and confidence interval by likelihood profiling otherwise.

<br>

or use CI by parametric bootstrap?


# Confidence intervals by parametric bootstrap

## 95% CI by parametric bootstrap

### Parametric bootstrap using ```boot```
```{r boot}
confint.myboot2 <- function(mod, print = FALSE) {
  new_betas <- replicate(100, {
    mod$data$newY <- simulate(mod, 1)[, 1]
    update(mod, newY ~ ., data = mod$data)$coef})
  intervals <- rbind("(Intercept)" = c(NA, NA), "humans_eaten" = c(NA, NA))
  intervals[1, ] <- boot::boot.ci(list(t0 = coef(mod)[[1]],
                                  t = as.matrix(new_betas[1, ]), 
                                  R = 100), type = "basic")$basic[4:5]
  intervals[2, ] <- boot::boot.ci(list(t0 = coef(mod)[[2]],
                                  t = as.matrix(new_betas[2, ]),
                                  R = 100), type = "basic")$basic[4:5]
  if (print) print(intervals)
  name.response <- mod$terms[[2]]
  if (length(name.response) == 3) name.response <- name.response[[2]]  ## for  binomial with cbind
  true.intercept <- attributes(mod$data)$param.eta[[paste(name.response)]][1]
  true.slope <- attributes(mod$data)$param.eta[[paste(name.response)]][2]
  if (print) cat("\n true parameters in 95% CI? \n")
  c(true.intercept > intervals["(Intercept)", 1] & true.intercept < intervals["(Intercept)", 2],
    true.slope > intervals["humans_eaten", 1] & true.slope < intervals["humans_eaten", 2])
}
```


## 95% CI by parametric bootstrap

### Coverage N = 100
```{r  coverage boot2, message = FALSE, warning = FALSE}
set.seed(1L)
test_poiss <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 100)
  mod <- update(mod_poiss, data = new.data)
  confint.myboot2(mod)
  })
test_binar <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 100)
  mod <- update(mod_binar, data = new.data)
  confint.myboot2(mod)
  })
test_binom <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 100)
  mod <- update(mod_binom, data = new.data)
  confint.myboot2(mod)
  })
test_gauss <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 100)
  mod <- update(mod_gauss, data = new.data)
  confint.myboot2(mod)
  })
```


## 95% CI by parametric bootstrap

### Coverage N = 100
```{r coverage boot2 summary}
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

This is very good and our best result so far!


## 95% CI by parametric bootstrap

### Coverage N = 20
```{r  coverage small boot3, message = FALSE, warning = FALSE}
set.seed(1L)
test_poiss <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 20)
  mod <- update(mod_poiss, data = new.data)
  confint.myboot2(mod)
  })
test_binar <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 40) ## 20 fails
  mod <- update(mod_binar, data = new.data)
  confint.myboot2(mod)
  })
test_binom <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 20)
  mod <- update(mod_binom, data = new.data)
  confint.myboot2(mod)
  })
test_gauss <- replicate(1000, {
  new.data <- simulate_Aliens_GLM(N = 20)
  mod <- update(mod_gauss, data = new.data)
  confint.myboot2(mod)
  })
```


## 95% CI by parametric bootstrap

### Coverage N = 20
```{r coverage boot3 summary}
rbind(poiss = apply(test_poiss, 1, mean),
      binar = apply(test_binar, 1, mean),
      binom = apply(test_binom, 1, mean),
      gauss = apply(test_gauss, 1, mean))
```

Not so great; likelihood profiling was much better!


# Confidence intervals on the fitted values

## 95% CI on fitted values: gaussian

### Using predict.lm from ```stats```
```{r conf fitted}
mod_lm <- lm(size ~ humans_eaten, data = Aliens)
newdata <- data.frame(humans_eaten = c(5, 15))
predict(mod_lm, newdata = newdata, interval = "confidence")

pred <- predict(mod_gauss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qt(0.025, df = mod_lm$df.residual) * pred$se.fit
upr <- pred$fit + qt(0.975, df = mod_lm$df.residual) * pred$se.fit
pred.table <- cbind(fit = pred$fit, lwr, upr)
pred.table
```

## 95% CI on fitted values: gaussian

### Using predict.glm from ```stats```
```{r conf fitted gauss}
pred <- predict(mod_gauss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qt(0.025, df = mod_lm$df.residual) * pred$se.fit
upr <- pred$fit + qt(0.975, df = mod_lm$df.residual) * pred$se.fit
pred.table <- cbind(fit = pred$fit, lwr, upr)
pred.table
```

Same with gaussian approximation:
```{r pred gauss}
pred <- predict(mod_gauss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qnorm(0.025) * pred$se.fit
upr <- pred$fit + qnorm(0.975) * pred$se.fit
pred.table <- cbind(fit = pred$fit, lwr, upr)
pred.table
```


## 95% CI on fitted values: gaussian

### Using predict.HLfit from ```spaMM```

```{r predict.HKfit, message = FALSE}
library(spaMM)
mod_gauss_spaMM <- fitme(size ~ humans_eaten, family = gaussian(), data = Aliens, method = "REML")
p <- predict(mod_gauss_spaMM, newdata = newdata, intervals = "predVar")
attr(p, "intervals")
get_intervals(mod_gauss_spaMM, newdata = newdata, intervals = "predVar")## spaMM uses here the Student
```


## 95% CI on fitted values: Poisson

### On the scale of the linear predictor
```{r conf fitted poiss}
pred <- predict(mod_poiss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qnorm(0.025) * pred$se.fit
upr <- pred$fit + qnorm(0.975) * pred$se.fit
cbind(lwr, upr)

mod_poiss_spaMM <- fitme(eggs ~ humans_eaten, family = poisson(), data = Aliens, method = "REML")
log(get_intervals(mod_poiss_spaMM, newdata = newdata, intervals = "predVar"))  ## log to get eta!
```


## 95% CI on fitted values: Poisson

### On the scale of the response
```{r pred fitted poiss}
pred <- predict(mod_poiss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qnorm(0.025) * pred$se.fit
upr <- pred$fit + qnorm(0.975) * pred$se.fit
exp(cbind(lwr, upr))
get_intervals(mod_poiss_spaMM, newdata = newdata, intervals = "predVar")  ## spaMM uses here the gaussian

```


## 95% CI on fitted values: Binomial

### Practice

Plot the predictions and confidence intervals for ```mod_binar``` and ```mod_binom``` for a number of humans eaten varying between 0 and 15.


## 95% CI on fitted values

### Conclusion

You have many options to get there, but if you do it manually never forget to first build the CI on the scale of the linear predictor and then transform all the values using the inverse link function.

### Never do the opposite!!!!! (SD should never be transformed)


# Prediction intervals

## 95% PI: gaussian

```{r pred int}
pred <- predict(mod_gauss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qt(0.025, df = mod_lm$df.residual) * sqrt(pred$se.fit^2 + pred$residual.scale^2)
upr <- pred$fit + qt(0.975, df = mod_lm$df.residual) * sqrt(pred$se.fit^2 + pred$residual.scale^2)
pred.table <- cbind(fit = pred$fit, lwr, upr)
pred.table
predict(mod_lm, newdata = newdata, interval = "prediction") ## uses here the Student
```

## 95% PI: gaussian

```{r pred int gauss}
lwr <- pred$fit + qnorm(0.025) * sqrt(pred$se.fit^2 + pred$residual.scale^2)
upr <- pred$fit + qnorm(0.975) * sqrt(pred$se.fit^2 + pred$residual.scale^2)
cbind(lwr, upr)
## spaMM works well in this case (and uses Student) as announced in ?predict.HLfit:
get_intervals(mod_gauss_spaMM, newdata = newdata, intervals = "respVar")
```


## 95% PI: Poisson

### The residual variance is no longer constant!
```{r pred int poiss}
pred <- predict(mod_poiss, newdata = newdata, se.fit = TRUE)
lwr <- pred$fit + qnorm(0.025) * sqrt(pred$se.fit^2 + poisson()$variance(exp(pred$fit)))
upr <- pred$fit + qnorm(0.975) * sqrt(pred$se.fit^2 + poisson()$variance(exp(pred$fit)))
exp(cbind(lwr, upr))  ## don't forget the inverse link!
## spaMM does not work well in this case as announced in ?predict.HLfit:
get_intervals(mod_poiss_spaMM, newdata = newdata, intervals = "respVar")
```


## 95% PI: Binomial

### Practice

Plot the predictions and prediction intervals for ```mod_binar``` and ```mod_binom``` for a number of humans eaten varying between 0 and 15.


# Summary tables

## New models

Let us refit models with less data to see differences in p-values more easily
```{r pv}
set.seed(1L)
Aliens2 <- simulate_Aliens_GLM(N = 20)
mod_lm2    <- lm(size  ~ humans_eaten, data = Aliens2)
mod_gauss2 <- glm(size  ~ humans_eaten, family = gaussian(), data = Aliens2)
mod_poiss2 <- glm(eggs  ~ humans_eaten, family = poisson(),  data = Aliens2)
mod_binar2 <- glm(happy ~ humans_eaten, family = binomial(), data = Aliens2)
mod_gauss2_spaMM  <- fitme(size  ~ humans_eaten, family = gaussian(), data = Aliens2) ## not REML!
mod_gauss2_spaMM0 <- fitme(size  ~ 1, family = gaussian(), data = Aliens2)
mod_poiss2_spaMM  <- fitme(eggs  ~ humans_eaten, family = poisson(),  data = Aliens2)
mod_poiss2_spaMM0 <- fitme(eggs  ~ 1, family = poisson(),  data = Aliens2)
mod_binar2_spaMM <- fitme(happy ~ humans_eaten, family = binomial(), data = Aliens2)
```


## Summary: gaussian

```{r summary lm2}
summary(mod_lm2)
```


## Summary: gaussian

```{r summary gauss2}
summary(mod_gauss2)
```


## Summary: gaussian

### Just like LM!
```{r  summary lm2 gauss2}
summary(mod_gauss2)$coef
summary(mod_lm2)$coef
```

## Summary: Poisson

```{r summary poiss}
summary(mod_poiss2)$coef
z <- (mod_poiss2$coef - 0) / sqrt(diag(vcov(mod_poiss2)))
pvalues <- 2 * (1 - pnorm(abs(z)))
cbind(z, pvalues)
```

<br>

When the dispersion parameter is not estimated, we use $z$, not $t$!

## Summary: Binomial

```{r summary binom}
summary(mod_binar2)$coef
z <- (mod_binar2$coef - 0) / sqrt(diag(vcov(mod_binar2)))
pvalues <- 2 * (1 - pnorm(abs(z)))
cbind(z, pvalues)
```


# Anova

## Anova: gaussian

### For gaussian() (and quasiXXX()) the F test is the one you should use!
```{r Anova gauss}
library(car)
Anova(mod_gauss2, test = "F")
```


## Anova: gaussian

```{r LR gauss}
Anova(mod_gauss2, test = "LR")  ## Does not do true LRT here!!
anova(mod_gauss2_spaMM, mod_gauss2_spaMM0)  ## True LRT, "same" with spaMM
c(-2 * (logLik(update(mod_gauss2, . ~ 1)) - logLik(mod_gauss2)), 
  (deviance(update(mod_gauss2, . ~ 1)) - deviance(mod_gauss2)) / summary(mod_gauss2)$dispersion)
```

Note: the computation will differ between methods when dispersion is not equals to 1...


## Anova: gaussian

```{r t gauss}
summary(mod_gauss2)$coef["humans_eaten", "t value"]
summary(mod_gauss2)$coef["humans_eaten", "t value"]^2
Anova(mod_gauss2, test = "F")
```


## $t^2$, $F$, and $\chi^2$

### Under certain conditions, these 3 distributions are the same:
* when $F_{df1 = 1}$ & when ```residuals.df``` $\rightarrow \infty$

```{r ecdf}
plot(ecdf(rt(10000, df = 5)^2), col = "blue", lwd = 3, main = "")
plot(ecdf(rf(10000, df1 = 1, df2 = 5)), add = TRUE, col = "red", lwd = 2)
plot(ecdf(rchisq(10000, df = 1)), add = TRUE, col = "green", lwd = 1)
```

## $t^2$, $F$, and $\chi^2$

### Under certain conditions, these 3 distributions are the same:
* when $F_{df1 = 1}$ & when ```residuals.df``` $\rightarrow \infty$

```{r ecdf 2}
plot(ecdf(rt(10000, df = 500)^2), col = "blue", lwd = 3, main = "")
plot(ecdf(rf(10000, df1 = 1, df2 = 500)), add = TRUE, col = "red", lwd = 2)
plot(ecdf(rchisq(10000, df = 1)), add = TRUE, col = "green", lwd = 1)
```


## Anova: Poisson and Binomial

### For poisson() and binomial(), better use the likelihood ratio test

```{r Anova Poiss}
Anova(mod_poiss2, test = "LR")
LR <- -2 * (logLik(update(mod_poiss2, . ~ 1)) - logLik(mod_poiss2))
pvalue <- 1 - pchisq(LR, 1)
cbind(LR, pvalue)
```


## Comparing the robustness of tests using simulations

```{r comp 1, warning = FALSE}
set.seed(1L)
N <- 20
pv <- replicate(1000, {
  d <- data.frame(eggs = rpois(N, lambda = 0.5),
                  humans_eaten = round(runif(N, min = 0, max = 50)))
  mod_poiss_1      <- glm(eggs ~ humans_eaten, family = poisson(), data = d)
  mod_poiss_1_null <- glm(eggs ~ 1, family = poisson(), data = d)
  c(LR = anova(mod_poiss_1, mod_poiss_1_null, test = "LR")[2, "Pr(>Chi)"],
    z = summary(mod_poiss_1)$coefficients["humans_eaten", "Pr(>|z|)" ])
})
rbind(mean(pv["LR", ] <= 0.05, na.rm = TRUE), mean(pv["z", ] <= 0.05, na.rm = TRUE))
```

## Comparing the robustness of tests using simulations

<div class="columns-2">

```{r comp plot, echo = FALSE}
plot(ecdf(pv["LR", ]), col = "green", cex = 0.3,  asp = 1, xlim = c(0, 0.05))
plot(ecdf(pv["z", ]), col = "blue", cex = 0.3, add = TRUE)
abline(0, 1, col = "red", lty = 2, lwd = 2)
```

```{r comp plot 2, echo = FALSE}
plot(pv["LR", ] ~ pv["z", ], xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red", lty = 2, lwd = 2)
```

</div>


<br>

But the performances depend on the predictor(s), lambda and sample size...


## Comparing the robustness of tests using simulations

```{r comp 2, warning = FALSE}
set.seed(1L)
N <- 20
pv <- replicate(1000, {
  d <- data.frame(eggs = rpois(N, lambda = 0.5),
                  mood = factor(sample(c("depr.", "happy", "upset", "tired", "anxious"), size = N, replace = TRUE)))
  mod_poiss_1      <- glm(eggs ~ mood, family = poisson(), data = d)
  mod_poiss_1_null <- glm(eggs ~ 1, family = poisson(), data = d)
  c(LR = anova(mod_poiss_1, mod_poiss_1_null, test = "LR")[2, "Pr(>Chi)"],
    F = Anova(mod_poiss_1, test = "F")["mood", "Pr(>F)"])
})
rbind(mean(pv["LR", ] <= 0.05, na.rm = TRUE), mean(pv["F", ] <= 0.05, na.rm = TRUE))
```

## Comparing the robustness of tests using simulations

<div class="columns-2">

```{r comp 2 plot, echo = FALSE}
plot(ecdf(pv["LR", ]), col = "green", cex = 0.3,  asp = 1, xlim = c(0, 0.05))
plot(ecdf(pv["F", ]), col = "blue", cex = 0.3, add = TRUE)
abline(0, 1, col = "red", lty = 2, lwd = 2)
```

```{r comp 2 plot 2, echo = FALSE}
plot(pv["LR", ] ~ pv["F", ], xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red", lty = 2, lwd = 2)
```

</div>


<br>

But the performances depend on the predictor(s), lambda and sample size...


## Anova: parametric bootstrap with ```spaMM```

### It works for all!
```{r lrt boot 1, message=FALSE}
anova(mod_gauss2_spaMM, mod_gauss2_spaMM0, boot.repl = 200)
```


## Anova: parametric bootstrap with ```spaMM```

### It works for all!
```{r lrt boot 2, message=FALSE}
anova(mod_poiss2_spaMM, mod_poiss2_spaMM0, boot.repl = 200)
```


## Comparing tests using spaMM

```{r comp 3, warning = FALSE, eval = FALSE}
set.seed(1L)
N <- 50
pv <- replicate(1000, {
  d <<- data.frame(eggs = rpois(N, lambda = 0.5),
                  humans_eaten = round(runif(N, min = 0, max = 50)))
  mod_poiss_1      <- glm(eggs ~ humans_eaten, family = poisson(), data = d)
  mod_poiss_1_null <- glm(eggs ~ 1, family = poisson(), data = d)
  mod_poiss_1_spaMM      <- fitme(eggs ~ humans_eaten, family = poisson(), data = d)
  mod_poiss_1_null_spaMM <- fitme(eggs ~ 1, family = poisson(), data = d)
  
  boot <- anova(mod_poiss_1_spaMM, mod_poiss_1_null_spaMM, boot.repl = 100, nb_cores = 1)
  c(z = summary(mod_poiss_1)$coefficients["humans_eaten", "Pr(>|z|)" ],
    LR = boot$basicLRT$p_value,
    Bartlett = boot$BartBootLRT$p_value,
    Raw = boot$rawBootLRT$p_value)
})
```

```{r outcome comp 3 new trial, eval = FALSE}
rbind(mean(pv["z", ] <= 0.05, na.rm = TRUE),
      mean(pv["LR", ] <= 0.05, na.rm = TRUE),
      mean(pv["Bartlett", ] <= 0.05, na.rm = TRUE),
      mean(pv["Raw", ] <= 0.05, na.rm = TRUE))
```

Results for later...



# Non-nested model comparison

## Comparing different links or different families

Since the response variable does not change but models are not nested, you can use:

* likelihood (larger = better)
* deviance (smaller = better)
* AIC (smaller = better)

For true test, the only method available is the parametric bootstrap!


## Comparing LM vs GLM

### Responses differ, so only parametric bootstrap is possible
```{r LM vs GLM}
set.seed(1L)
x <- seq(1, 2, length = 100)
y <- exp(rnorm(n = length(x), mean = 2 + 1 * x, sd = 0.5))

mod_lm   <- lm(log(y) ~ x)
mod_glm  <- glm(y ~ x, family = gaussian(link = "log"))
logLikH0 <- replicate(1000, {
  new.y <- exp(as.matrix(simulate(mod_lm)))
  logLik(lm(log(new.y) ~ x)) - logLik(glm(new.y ~ x, family = gaussian(link = "log")))
  })
(sum(logLikH0 < (logLik(mod_lm) - logLik(mod_glm))) + 1) / (length(logLikH0) + 1)
```

Here we used the GLM as the null model so we simulate the data using the LM.

In principle, we could have do the opposite, but in practice the function ```simulate()``` bugs for GLM with ```gaussian()$link != "identity"```.

## What you need to remember

* that estimates are still roughly normally distributed (but only roughly)
* how to compute intervals for GLM
* that intervals based on likelihood are best for family with known dispersion
* that established methods don't always work well, so you should always run tests
* that parametric bootstrap can outperform other methods when done properly
* that intervals on scale other than $\eta$ must be done by hand or using ```spaMM```
* that tests from summary tables are only reliable for the gaussian family
* that LR tests are best for family with known dispersion
* that LR tests by parametric bootstrap are best in all situations and easy to do with ```spaMM```


# Table of contents

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Let's practice more](./GLM_practice.html)

