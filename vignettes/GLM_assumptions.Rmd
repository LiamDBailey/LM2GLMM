---
title: "GLM: Residuals & Assumptions"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{3.2 Residuals & Assumptions}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(spaMM)
library(car)
library(DHARMa)
library(pscl)
spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/GLM_resid/", fig.path = "./fig_knitr/GLM_resid/", fig.width = 5, fig.height = 5, fig.align = "center", error = TRUE)
```

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Let's practice more](./GLM_practice.html)

## You will learn in this session

* that many residuals can be computed for GLM and that they are often useless
* that computing residuals by parametric bootstraps in the way out
* that traditional goodness of fit tests are bad
* that most assumptions behing GLM are similar to those for LM
* that General Additive Models (GAM) can be useful to improve linearity
* how to diagnose and tackle overdispersion, zero-augmentation and separation


## Our data and toy models

```{r fit all models}
set.seed(1L)
Aliens <- simulate_Aliens_GLM()
mod_gauss <- glm(size  ~ humans_eaten, family = gaussian(), data = Aliens)
mod_poiss <- glm(eggs  ~ humans_eaten, family = poisson(),  data = Aliens)
mod_binar <- glm(happy ~ humans_eaten, family = binomial(), data = Aliens)
mod_binom <- glm(cbind(blue_eyes, pink_eyes) ~ humans_eaten, family = binomial(), data = Aliens)
```

# Residuals

## Can we still use ```plot(model)``` ?

```{r plot poiss}
par(mfrow = c(2, 2))
plot(mod_poiss)
```


## Can we still use ```plot(model)``` ?

```{r plot binar}
par(mfrow = c(2, 2))
plot(mod_binar)
```


## Can we still use ```plot(model)``` ?

```{r plot binom}
par(mfrow = c(2, 2))
plot(mod_binom)
```

## Many types of residuals can be computed

<div class="columns-2">

```{r residuals poiss}
rbind(
  residuals(mod_poiss, type = "deviance")[1:2],
  residuals(mod_poiss, type = "pearson")[1:2],
  residuals(mod_poiss, type = "working")[1:2],
  residuals(mod_poiss, type = "response")[1:2])
```

```{r residuals binom}
rbind(
  residuals(mod_binom, type = "deviance")[1:2],
  residuals(mod_binom, type = "pearson")[1:2],
  residuals(mod_binom, type = "working")[1:2],
  residuals(mod_binom, type = "response")[1:2])
```

</div>

<br>

Note 1: we can also compute partial residuals, which we shall see later.

Note 2: this is also true for LM and ```gaussian(link = "identity")``` GLM, but it makes no difference in such case (except for partial residuals).

## Distribution

### They are not necessarily normaly distributed!

<div class="columns-2">

```{r ecdf resid poiss, echo = FALSE}
par(las = 1)
plot(ecdf(residuals(mod_poiss, type = "working")), col = 3, main = "mod_poiss")
plot(ecdf(residuals(mod_poiss, type = "deviance")), col = 1, add = TRUE)
plot(ecdf(residuals(mod_poiss, type = "pearson")), col = 2, add = TRUE)
plot(ecdf(residuals(mod_poiss, type = "response")), col = 4, add = TRUE)
legend("bottomright", fill = 1:4, bty = "n", legend = c("deviance", "pearson", "working", "response"), title = "Type of residuals:")
```

```{r ecdf resid binom, echo = FALSE}
par(las = 1)
plot(ecdf(residuals(mod_binom, type = "working")), col = 3, main = "mod_binom")
plot(ecdf(residuals(mod_binom, type = "deviance")), col = 1, add = TRUE)
plot(ecdf(residuals(mod_binom, type = "pearson")), col = 2, add = TRUE)
plot(ecdf(residuals(mod_binom, type = "response")), col = 4, add = TRUE)
legend("topleft", fill = 1:4, bty = "n", legend = c("deviance", "pearson", "working", "response"), title = "Type of residuals:")
```

</div>


## Response residuals

These are the residuals that we saw in LM: observed - predicted. Simple but pretty much useless.

```{r response res}
rbind(residuals(mod_gauss, type = "response")[1:2],
      (mod_gauss$y - mod_gauss$fitted.values)[1:2])
rbind(residuals(mod_poiss, type = "response")[1:2],
      (mod_poiss$y - mod_poiss$fitted.values)[1:2])
rbind(residuals(mod_binom, type = "response")[1:2],
      (mod_binom$y - mod_binom$fitted.values)[1:2])
```


## Pearson residuals

These are response residuals scaled by the square root of the variance function and accounting for their prior weights.

```{r pearson res}
variances <- poisson()$variance(mod_poiss$fitted.values)
rbind(residuals(mod_poiss, type = "pearson")[1:2],
      (residuals(mod_poiss, type = "response") * sqrt(mod_poiss$prior.weights / variances))[1:2])

variances <- binomial()$variance(mod_binom$fitted.values)
rbind(residuals(mod_binom, type = "pearson")[1:2],
      (residuals(mod_binom, type = "response") * sqrt(mod_binom$prior.weights / variances))[1:2])
```

These residuals are often use to check the quality of the fit.


## Pearson residuals

The sum of the squared Pearson residuals gives the Pearson $\chi^2$ statistic of goodness of fit:

```{r pearson dev binom}
(X <- sum(residuals(mod_binom, type = "pearson")^2))
```

This statistic is sometimes used to test the goodness of fit of the model (= assess the ability of a model to fit the data with no alternative in mind) or to test for overdispersion (= is the residual variance compatible with the variance function), but I would not recommend you to use such test because its usage is often not optimal (it requires a lot of replications within each grouping, the power is poor, and the coverage is often poor).

```{r pearson dev pois}
pchisq(X, mod_poiss$df.residual, lower.tail = FALSE)  ## Pearson goodness of fit test
X / mod_poiss$df.residual  ## measure of overdispersion
```


## Deviance residuals

These are the residuals that are minimized after the fit.

Contrary to those computed by ```family()$dev.resids``` they are here not squared.

<br>

```{r dev resid}
residuals(mod_poiss, type = "deviance")[1:2]
rbind((residuals(mod_poiss, type = "deviance")[1:2])^2,
      with(mod_poiss, poisson()$dev.resids(y = y, mu = fitted.values, wt = prior.weights))[1:2])
```


## Deviance residuals

The sum of the squared deviance residuals gives the residual (unscaled) deviance of the model fit:

```{r deviance}
c(sum(residuals(mod_gauss, type = "deviance")^2), deviance(mod_gauss))
c(sum(residuals(mod_poiss, type = "deviance")^2), deviance(mod_poiss))
c(sum(residuals(mod_binom, type = "deviance")^2), deviance(mod_binom))
```

<br>

These residuals are sometimes also used to check the quality of the fit, in the exact same fashion as the Pearson's residuals, but again (and for the same reasons), I would not recommend you to do that.


## Testing false positives in the goodness of fit tests

For ```mod_poiss```

<div class = "columns-2">

```{r gof 1, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_poiss, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  pchisq(X, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 2, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_poiss, data = d)
  dev <- deviance(m)
  pchisq(dev, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Testing false positives in the goodness of fit tests

For ```mod_binar```

<div class = "columns-2">

```{r gof 3, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binar, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  pchisq(X, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 4, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binar, data = d)
  dev <- deviance(m)
  pchisq(dev, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Testing false positives in the goodness of fit tests

For ```mod_binom```

<div class = "columns-2">

```{r gof 5, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binom, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  pchisq(X, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 6, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binom, data = d)
  dev <- deviance(m)
  pchisq(dev, m$df.residual, lower.tail = FALSE)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Working residuals

These are the residuals used during the last iteration of the iterative fitting procedure. Useless otherwise.

```{r work res}
rbind(mod_poiss$residuals[1:2],
      ((mod_poiss$y - mod_poiss$fitted.values)/poisson()$mu.eta(mod_poiss$linear.predictors))[1:2])

rbind(mod_binom$residuals[1:2],
      ((mod_binom$y - mod_binom$fitted.values)/binomial()$mu.eta(mod_binom$linear.predictors))[1:2])
```


## Partial residuals

These are residuals expressed at the level of each predictor.

```{r partial res}
mod_UK_small <- glm(milk ~ sex + mother_weight + cigarettes, data = UK[1:10, ], family = poisson())
residuals(mod_UK_small, type = "partial")
```


## Partial residuals

These are residuals expressed at the level of each predictor.

Computation:
```{r compute partial res}
(p <- predict(mod_UK_small, type = "terms")) ## prediction expressed per predictor
c(sum(p[1, ]) + attr(p, "constant"), predict(mod_UK_small, type = "link")[1])
```


## Partial residuals

These are residuals expressed at the level of each predictor.

Computation:
```{r compute partial res 2}
rbind((p + residuals(mod_UK_small, type = "working"))[1, ],
      residuals(mod_UK_small, type = "partial")[1, ])
```

<br>

Partial residuals can be use to check for departure from linearity (concerns quantitative predictors).


## Partial residuals

```{r plot partial res, fig.width = 10, fig.height = 4}
library(car)
par(mfrow = c(1, 3))
crPlots(mod_UK_small, terms = ~ sex)
crPlots(mod_UK_small, terms = ~ mother_weight)
crPlots(mod_UK_small, terms = ~ cigarettes)
```


## Partial residuals

```{r plot partial res 2, fig.width = 10, fig.height = 4}
mod_UK <- glm(milk ~ sex + mother_weight + cigarettes, data = UK, family = poisson())
par(mfrow = c(1, 3))
crPlots(mod_UK, terms = ~ sex)
crPlots(mod_UK, terms = ~ mother_weight)
crPlots(mod_UK, terms = ~ cigarettes)
```


# Residuals by parametric bootstrap: the most useful ones!

## Computing residuals by parametric bootstrap

```{r bootstrap, fig.height = 4, fig.width = 4}
set.seed(1L)
s <- simulate(mod_poiss, 1000)
r <- sapply(s, function(i) i + runif(nrow(mod_poiss$model), min = -0.5, max = 0.5))
hist(r[1, ], main = "distrib of first fitted value", nclass = 30)
abline(v = mod_poiss$y[1] + runif(1, min = -0.5, max = 0.5), col = "red", lwd = 2, lty = 2)
```


## Computing residuals by parametric bootstrap

```{r bootstrap 2, fig.height = 4, fig.width = 4}
plot(ecdf1 <- ecdf(r[1, ]), main = "cdf of first fitted value")
noise <- runif(1, min = -0.5, max = 0.5)
simulated_residual_1 <-  ecdf1(mod_poiss$y[1] + noise)
segments(x0 = mod_poiss$y[1] + noise, y0 = 0, y1 =  simulated_residual_1, col = "red", lwd = 2)
arrows(x0 = mod_poiss$y[1] + noise, x1 = -1, y0 = simulated_residual_1, col = "red", lwd = 2)
```


## Computing residuals by parametric bootstrap

```{r bootstrap 3, fig.height = 4, fig.width = 4}
plot(ecdf2 <- ecdf(r[2, ]), main = "cdf of second fitted value")
noise <- runif(1, min = -0.5, max = 0.5)
simulated_residual_2 <-  ecdf2(mod_poiss$y[2] + noise)
segments(x0 = mod_poiss$y[2] + noise, y0 = 0, y1 =  simulated_residual_2, col = "red", lwd = 2)
arrows(x0 = mod_poiss$y[2] + noise, x1 = -1, y0 = simulated_residual_2, col = "red", lwd = 2)
```


## Computing residuals by parametric bootstrap

```{r simu resid, fig.height = 4, fig.width = 4}
simulated_residuals <- rep(NA, nrow(mod_poiss$model))
for (i in 1:nrow(mod_poiss$model)) {
  ecdf_fn <- ecdf(r[i, ])
  simulated_residuals[i] <- ecdf_fn(mod_poiss$y[i] + runif(1, min = -0.5, max = 0.5))
}
```

<div class="columns-2">

```{r plot sim res, fig.height = 4, fig.width = 4}
plot(simulated_residuals)
```

```{r plot ecdf sim res, fig.height = 4, fig.width = 4}
plot(ecdf(simulated_residuals))
```

</div>


## Simulating residuals with ```DHARMa```

```{r simres poiss, fig.width = 7, fig.height = 4}
library(DHARMa)
mod_poiss_simres <- simulateResiduals(mod_poiss)
plot(mod_poiss_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres binar, fig.width = 7, fig.height = 4}
mod_binar_simres <- simulateResiduals(mod_binar)
plot(mod_binar_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres binom, fig.width = 7, fig.height = 4}
mod_binom_simres <- simulateResiduals(mod_binom)
plot(mod_binom_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres UK, fig.width = 7, fig.height = 4}
mod_UK_simres <- simulateResiduals(mod_UK)
plot(mod_UK_simres)
```


# Assumptions

## The main assumptions

### Model structure

* linearity (for the linear predictor)
* lack of perfect multicollinearity (design matrix of full rank)
* predictor variables have fixed values

### Errors

* independence (no serial autocorrelation)
* lack of overdispersion and underdispersion

# Assumptions about the model structure

## How to test for linearity?

### It is very difficult...

but we may try to

* plot partial residuals
* plot simulated residuals & run an uniformity test
* plot the predictions from a GAM model


## Example of non-linearity

```{r Aliens2}
set.seed(1L)
Aliens2 <- data.frame(humans_eaten = runif(100, min = 0, max = 15))
Aliens2$eggs <- rpois( n = 100, lambda = exp(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2))
mod_poiss2bad <- glm(eggs ~ humans_eaten, data = Aliens2, family = poisson()) ## mispecified model
(mod_poiss2good <- glm(eggs ~ poly(humans_eaten, 2, raw = TRUE), data = Aliens2, family = poisson())) ## good model
```


## Example of non-linearity
```{r plot Aliens2,  fig.width = 4, fig.height = 4}
plot(I(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2) ~ Aliens2$humans_eaten,
     ylab = "eta", ylim = c(-1, 2))
data.for.pred <- data.frame(humans_eaten = 0:15)
points(predict(mod_poiss2good, newdata = data.for.pred) ~ I(0:15), col = "blue")
points(predict(mod_poiss2bad, newdata = data.for.pred) ~ I(0:15), col = "red")
```


## Example of non-linearity
```{r plot Aliens2 2,  fig.width = 4, fig.height = 4}
plot(exp(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2) ~ Aliens2$humans_eaten,
     ylab = "predicted number of eggs", ylim = c(0, 6))
points(predict(mod_poiss2good, newdata = data.for.pred, type = "response") ~ I(0:15), col = "blue")
points(predict(mod_poiss2bad, newdata = data.for.pred, type = "response") ~ I(0:15), col = "red")
```


## Example of non-linearity

```{r plot partial Aliens2}
crPlots(mod_poiss2bad, terms = ~ humans_eaten)
```


## Example of non-linearity

```{r plot partial Aliens2 2}
crPlots(mod_poiss2good)
```


## Example of non-linearity

```{r plot res Aliens2, fig.width = 7, fig.height = 4}
plot(s_bad <- simulateResiduals(mod_poiss2bad))
```


## Example of non-linearity

```{r plot res Aliens2 2, fig.width = 7, fig.height = 4}
plot(s_good <- simulateResiduals(mod_poiss2good))
```


## Example of non-linearity

### The lack of linearity is unfortunatly not detected here:
```{r test unif}
testUniformity(s_bad)
testUniformity(s_good)
```


## Example of non-linearity

```{r GAM, fig.width = 4, fig.height = 4, message = FALSE}
library(mgcv)
mod_poiss2GAM <- gam(eggs ~ s(humans_eaten), data = Aliens2, family = poisson())
plot(mod_poiss2GAM, shade = TRUE, residuals = FALSE, shade.col = "red")  ## on the scale of the linear predictor!
```

## Multicollinearity and fixed-values

### Same as for LM!

# Assumptions about the errors

## How to test for independence?

You can run the Durbin Watson test on the simulated residuals:

```{r autocorr}
testTemporalAutocorrelation(s_good, time = mod_poiss2good$fitted.values, plot = FALSE)
```


## How to test for independence?

You can run the Durbin Watson test on the simulated residuals:

```{r autocorr 2}
testTemporalAutocorrelation(s_good, time = Aliens2$humans_eaten, plot = FALSE)
```

<br>

Note: As for LM it is good practice to test for the lack of serial autocorrelation along all your predictors and not just along the fitted values.


## A particular legitimate case of dependence

Survival times series that are discrete and complete can be analysed using a binomial (binary!) GLM.

Example: the study of the influence of rainfall on mortality (here A dies at 5 yrs old and B at 3 yrs old).

```{r eles}
data.frame(age = c(1:5, 1:5),
      id = c(rep("A", 5), rep("B", 5)),
      death = c(0, 0, 0, 0, 1, 0, 0, 1, NA, NA),
      annual_rain = c(100, 120, 310, 50, 200, 45, 100, 320, 100, 120))
```

In such a case, a random effect for the identity of the individual is not needed! So there is no need for mixed model if individuals are all equally independent from each other.



## Overdispersion / Underdispersion

### A GLM assumes a particular relationship between mu and var(mu)

```{r varmu, fig.width = 10, fig.height = 3}
p <- seq(0, 1, 0.1)
lambda <- 0:10
theta <- 0:10
v_b <- binomial()$variance(p)
v_p <- poisson()$variance(lambda)
v_G <- Gamma()$variance(theta)
par(mfrow = c(1, 3), las = 2)
plot(v_b ~ p); plot(v_p ~ lambda); plot(v_G ~ theta)
```


## Overdispersion / Underdispersion

### Overdispersion = more variance than expected

* very common $\rightarrow$ increases false positive
* specially relevant for Poisson and Binomial
* irrelevant for the binary case! (don't look for it)

Usual suspects:

* lack of linearity
* unobserved heterogeneity
* zero-augmentation

<br>

### Underdispersion = less variance than expected

* rather rare  $\rightarrow$ increases false negative


## Overdispersion / Underdispersion

### A toy example

```{r overdisp}
set.seed(1L)
popA <- data.frame(humans_eaten = runif(50, min = 0, max = 15))
popA$eggs <- rpois(n = 50, lambda = exp(-1 + 0.05 * popA$humans_eaten))
popA$pop <- "A"
popB <- data.frame(humans_eaten = runif(50, min = 0, max = 15))
popB$eggs <- rpois(n = 50, lambda = exp(-3 + 0.4 * popB$humans_eaten))
popB$pop <- "B"
AliensMix <- rbind(popA, popB)
(mod_poissMix <- glm(eggs ~ humans_eaten, family = poisson(), data = AliensMix))
```

## Overdispersion / Underdispersion

### How to test it?

A widely used not so good way:

```{r deviance overdisp}
cbind(disp = mod_poissMix$deviance / mod_poissMix$df.residual,
      pv = pchisq(mod_poissMix$deviance, mod_poissMix$df.residual, lower.tail = FALSE))
```

A slightly better way:

```{r pearson overdisp}
cbind(disp = sum(residuals(mod_poissMix, type = "pearson")^2) / mod_poissMix$df.residual,
      pv = pchisq(sum(residuals(mod_poissMix, type = "pearson")^2), mod_poissMix$df.residual, lower.tail = FALSE))
```


## Overdispersion / Underdispersion

### How to test it?

A better way (?)

```{r sim overdisp}1
r <- simulateResiduals(mod_poissMix, refit = TRUE)
testOverdispersion(r) # stat = squared pearson residuals compared to that obtained by simulation
```


# Solving dispersion problems

## Potential solutions

* fix linearity issues
* fix heterogeneity issues (if you have the data)

```{r inter}
mod_poissMix2 <- glm(eggs ~ pop*humans_eaten, family = poisson(), data = AliensMix)
r2 <- simulateResiduals(mod_poissMix2, refit = TRUE)
testOverdispersion(r2)  ## you can change options to test for underdispersion
```

## Potential solutions

* fix linearity issues
* fix heterogeneity issues (if you have the data)
* model the overdispersion
* try another probability distribution
* there are specific solutions if the origin is zero-augmentation


## Modeling simply overdispersion

### For Poisson

Variance = $k \times \mu$

```{r quasipoiss}
mod_poissMixQ <- glm(eggs ~ humans_eaten, family = quasipoisson(), data = AliensMix)
summary(mod_poissMixQ)$coef
Anova(mod_poissMixQ, test = "F")  ## 'F' not 'LR' because dispersion is estimated!
```


## Modeling simply overdispersion

### For Poisson

Variance = $k \times \mu$

It works, but we cannot do everything with these types of fit:
```{r confint quasipoiss}
confint(mod_poissMixQ)
c(logLik(mod_poissMixQ), AIC(mod_poissMixQ))
```


## Modeling simply overdispersion

### For binomial

* irrelevant for the binary case!
* there is also ```quasibinomial()```, with the same limit (no likelihood)
* can happen in the general case

<br>

### Solution

* add a random effect with one level per individual (see GLMM)


## Probability distributions for count data

* Poisson ($V(\mu) = \mu$)
* Negative binomial ($V(\mu) = \mu + \frac{\mu^2}{\theta}$, if $\theta = 1$ this is the geometric model)
* Conway-Maxwell-Poisson ($V(\mu) =$ something complex)


## Poisson

### For comparison

```{r pearson overdisp fit}
mod_poissMix <- glm(eggs ~ humans_eaten, family = poisson(), data = AliensMix)
sum(residuals(mod_poissMix, type = "pearson")^2) / mod_poissMix$df.residual
summary(mod_poissMix)$coefficients
```


## Negative binomial with ```MASS```

```{r negbin}
library(MASS)
mod_poissMixNB <- glm.nb(eggs ~ humans_eaten, data = AliensMix)
sum(residuals(mod_poissMixNB, type = "pearson")^2) / mod_poissMix$df.residual
summary(mod_poissMixNB)$coefficients
c(AIC(mod_poissMix), AIC(mod_poissMixNB), logLik(mod_poissMixNB))
```


## Negative binomial with ```spaMM```

```{r negbin spaMM}
mod_poissMixSpaMM   <- fitme(eggs ~ humans_eaten, family = poisson(), data = AliensMix)
mod_poissMixNBSpaMM <- fitme(eggs ~ humans_eaten, family = spaMM::negbin(), data = AliensMix) ## namespace needed!
summary(mod_poissMixNBSpaMM)
c(AIC(mod_poissMixSpaMM), AIC(mod_poissMixNBSpaMM), logLik(mod_poissMixNBSpaMM))
```


## Conway-Maxwell-Poisson (from ```spaMM```)

### It can fit both over- and under- dispersion!

* overdispersion: nu < 1 (nu = 0 corresponds to the geometric distribution)
* expected dispersion (i.e. Poisson): nu = 1
* underdispersion: nu > 1

Replicating Poisson:

```{r COMPoisson, warning = FALSE}
mod_poissMixCP       <- glm(eggs ~ humans_eaten, family = COMPoisson(nu = 1), data = AliensMix)
mod_poissMixCPSpaMM <- fitme(eggs ~ humans_eaten, family = COMPoisson(nu = 1), data = AliensMix)

rbind(logLik(mod_poissMix)[[1]], logLik(mod_poissMixCP)[[1]], logLik(mod_poissMixCPSpaMM)[[1]])
```

IT SHOULD ALL BE IDENTICAL (AND USED TO), BUT THERE SEEM TO BE A BUG!!

## Conway-Maxwell-Poisson

### It can fit both over- and under- dispersion!

```{r COMPoisson2, warning = FALSE}
mod_poissMixCPSpaMM <- fitme(eggs ~ humans_eaten, family = COMPoisson(), data = AliensMix)
summary(mod_poissMixCPSpaMM)
c(AIC(mod_poissMixSpaMM), AIC(mod_poissMixNBSpaMM), AIC(mod_poissMixCP), AIC(mod_poissMixCPSpaMM))
```

Note: here we estimate nu, so it can take (much) longer time to fit!


## Comparison

```{r comp COMPoiss, echo = FALSE, warning = FALSE, fig.height = 6, fig.width = 6}
d <- data.frame(humans_eaten = seq(0, 15, length = 1000))
p <- predict(mod_poissMixSpaMM, newdata = d, intervals = "predVar")
plot(p ~ d$humans_eaten, ylim = range(mod_poissMixSpaMM$data$eggs), type = "l", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l")
p <- predict(mod_poissMixNBSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "red", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
p <- predict(mod_poissMixCPSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "blue", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
legend("topleft", fill = c("black", "red", "blue"), legend = c("Poisson", "Negative Binomial", "Conway-Maxwell-Poisson "), bty = "n")
```


## Comparison

```{r comp COMPoiss same, eval = FALSE}
d <- data.frame(humans_eaten = seq(0, 15, length = 1000))
p <- predict(mod_poissMixSpaMM, newdata = d, intervals = "predVar")
plot(p ~ d$humans_eaten, ylim = range(mod_poissMixSpaMM$data$eggs), type = "l", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l")
p <- predict(mod_poissMixNBSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "red", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
p <- predict(mod_poissMixCPSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "blue", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "blue") ## BUG
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "blue") ## BUG
legend("topleft", fill = c("black", "red", "blue"),
       legend = c("Poisson", "Negative Binomial", "Conway-Maxwell-Poisson "), bty = "n")
```


# Zero-augmentation

## Zero-augmentation in brief

### What is it?

It occurs for binomial or Poisson events when too many zeros occur.

### Why does it occur?

It can occur when the response results from a 2 (or more) steps process

Examples:

* detection issue (low counts are less detected, e.g. counting cells on microscope)
* biological (e.g. infection, then spread of microbes)


## How to tackle zero-augmentation?

You may try again to use the negative binomial or the COM-Poisson distribution, but an alternative that is often best is to combine two models (during the fitting procedure, not sequentially)!

### We have 2 main options for this:

Fit an hurdle model

* binomial  (or truncated count distribution) + truncated Poisson or truncated negative binomial
* a single source of zeros
* e.g. number of offspring

<br>

Fit a zero-inflation model

* binomial (or truncated count distribution) + Poisson or negative binomial
* two sources of zeros
* e.g. number of viruses in individuals (0 for unexposed, 0 for exposed with strong immune system)

### If you are not sure where the zeros come from, try both!


## Zero-augmented data

```{r zeroinfl}
set.seed(1L)
AliensZ <- simulate_Aliens_GLM(N = 1000)
AliensZ$eggs <- AliensZ$eggs * AliensZ$happy ## unhappy Aliens loose their eggs :-(
barplot(table(AliensZ$eggs))
```


## Poisson fit of zero-augmented data

```{r zeroinfl poiss}
mod_Zpoiss <- glm(eggs ~ humans_eaten, data = AliensZ, family = poisson())
r <- simulateResiduals(mod_Zpoiss)
testZeroInflation(r, plot = FALSE)
mean(sum(AliensZ$eggs == 0) / sum(dpois(0, fitted(mod_Zpoiss))))
```


## Poisson fit of zero-augmented data

```{r test zeroinfl}
testZeroInflation(r, plot = TRUE)
```


## Negative binomial fit of zero-augmented data

```{r test zeroinfl negbin}
mod_Znb <- glm.nb(eggs ~ humans_eaten, data = AliensZ)
r <- simulateResiduals(mod_Znb)
testZeroInflation(r, plot = FALSE)
```


## Negative binomial fit of zero-augmented data

```{r plot test zeroinfl}
testZeroInflation(r, plot = TRUE)
```


## Hurdle fit of zero-augmented data

```{r hurdle, message = FALSE}
library(pscl)
mod_Zhurd1 <- hurdle(eggs ~ humans_eaten | humans_eaten, dist = "poisson", zero.dist = "binomial",
                     data = AliensZ)
mod_Zhurd2 <- hurdle(eggs ~ humans_eaten | 1, dist = "poisson", zero.dist = "binomial", data = AliensZ)
lmtest::lrtest(mod_Zhurd1, mod_Zhurd2)
mean(sum(AliensZ$eggs == 0) / sum(predict(mod_Zhurd1, type = "prob")[, 1]))
```


## Zero-inflation fit of zero-augmented data

```{r zeroinfl fit, message = FALSE}
mod_Zzi1 <- zeroinfl(eggs ~ humans_eaten | humans_eaten, dist = "poisson", data = AliensZ)
mod_Zzi2 <- zeroinfl(eggs ~ humans_eaten | 1, dist = "poisson", data = AliensZ)
lmtest::lrtest(mod_Zzi1, mod_Zzi2)
mean(sum(AliensZ$eggs == 0) / sum(predict(mod_Zzi1, type = "prob")[, 1]))
```


## Comparison

```{r comp zeroinfl}
cbind(Poisson = AIC(mod_Zpoiss),
      NegBin  = AIC(mod_Znb),
      Hurdle  = AIC(mod_Zhurd1),
      ZeroInf = AIC(mod_Zzi1))
tab <- rbind(Poisson = c(mod_Zpoiss$coefficients, NA, NA),
             NegBin = c(mod_Znb$coefficients, NA, NA),
             Hurdle = unlist(mod_Zhurd1$coefficients),  ## NOTE: the hurdle part predicts positive counts, not zeros!!
             ZeroInfl = unlist(mod_Zzi1$coefficients),  ## NOTE: the binary part predicts zeros, not counts!!
             Truth = c(attr(AliensZ, "param.eta")$eggs, attr(AliensZ, "param.eta")$happy))
colnames(tab) <- c("Int.count", "Slope.count", "Int.bin", "Slope.bin")
tab
```


# One last trap: separation

## The problem of separation in Binomial

### What is it?

Separation occurs when a level or combination of levels for categorical predictor, or when a particular threshold along a continuous predictor, predicts the outcomes perfectly.

<br>

### Complete or quasi separation?

From Wikipedia:

*For example, if the predictor X is continuous, and the outcome y = 1 for all observed x > 2. If the outcome values are perfectly determined by the predictor (e.g., y = 0 when x ≤ 2) then the condition "complete separation" is said to occur. If instead there is some overlap (e.g., y = 0 when x < 2, but y has observed values of 0 and 1 when x = 2) then "quasi-complete separation" occurs. A 2 × 2 table with an empty cell is an example of quasi-complete separation.*

### Consequences

* sometimes the model cannot be fitted
* when it does fit, the estimates and standard errors are usually wrong!


## The problem of separation in Binomial

```{r separ}
set.seed(1L)
n <- 50
test <- data.frame(happy = rbinom(2*n, prob = c(rep(0, n), rep(0.75, n)), size = 1), 
                   sp = factor(c(rep("sp1", n), rep("sp2", n))))
table(test$happy, test$sp)
```

```{r fit separ}
mod <- glm(happy ~ sp, data = test, family = binomial())
exp(coef(mod))
```


## The problem of separation in Binomial

### Detection

```{r safebin, error = TRUE, message = FALSE}
library(safeBinaryRegression)  ## overload the glm function
mod2 <- glm(happy ~ sp, data = test, family = binomial())
detach(package:safeBinaryRegression)
```


## The problem of separation in Binomial

### Detection

```{r spaMM separ, error = TRUE}
library(spaMM)  ## with package e1071 installed! (but not working at the moment...)
mod2 <- fitme(happy ~ sp, data = test, family = binomial())
```

## The problem of separation in Binomial

### Let's tweak the data in a conservative way
```{r tweak separ}
test$happy[1] <- 1
table(test$happy, test$sp)
mod2 <- glm(happy ~ sp, data = test, family = binomial())
exp(coef(mod2))
```


## The problem of separation in Binomial

### A solution for the binary case (as well as for binomial in general!)

```{r brglm2, message = FALSE}
test$happy[1] <- 0  ## restore the original data
library(brglm2)
mod3 <- glm(happy ~ sp, data = test, family = binomial(), method = "brglmFit")
exp(coef(mod3))
```

## What you need to remember

* that many residuals can be computed for GLM and that they are often useless
* that computing residuals by parametric bootstraps in the way out
* that traditional goodness of fit tests are bad
* that most assumptions behing GLM are similar to those for LM
* that General Additive Models (GAM) can be useful to improve linearity
* how to diagnose and tackle overdispersion, zero-augmentation and separation


# Table of contents

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Let's practice more](./GLM_practice.html)

