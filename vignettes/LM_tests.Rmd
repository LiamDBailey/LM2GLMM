---
title: "LM: Tests"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.3 Tests}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
options(width = 120)
library(LM2GLMM)
library(spaMM)
library(lattice)
library(mvtnorm)
library(rgl)
library(car)
spaMM.options(nb_cores = 4L)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LM_tests/", fig.path = "./fig_knitr/LM_tests/")
```

## You will learn in this session

* how to use the functions ```summary()```, ```anova()``` and ```Anova()```
* that parameters are tested using t-tests
* that you can use LM to do usual Student's t-tests and Pearson correlation tests
* that variables are tested using F-tests or Likelihood Ratio Tests
* how to check the statistical robustness of a test
* how to do a LRT by parametric bootstrap
* that type II ANOVA is the way to go for testing variables
* that doing many tests is dangerous


# The fitted model

## The UK model

```{r UK data}
(mod_UK1 <- lm(height ~ drink + sex*weight, data = UK))
```

# Testing parameter estimates

## Using the function ```summary()```

```{r summary UK}
summary(mod_UK1)
```

## t-test on parameter estimates

```{r summary random line}
summary(mod_UK1)$coefficient["drinkMost days", ]
```

### Nothing really new: we have seen all this before! (but the p-value)

```{r summary by hand}
estimate <- coef(mod_UK1)["drinkMost days"]
std.error <- sqrt(diag(vcov(mod_UK1))["drinkMost days"])
t.value <- (estimate - 0)/std.error
p.value <- 2*pt(abs(t.value), df = mod_UK1$df.residual, lower.tail = FALSE) ## 2x as bilateral test
rbind(estimate, std.error, t.value, p.value)
```

## Comparison to the Student's t-test

```{r compare t-tests}
t.test(extra ~ group, data = sleep, var.equal = TRUE)
summary(lm(extra ~ group, data = sleep))$coef
```


## Comparing an estimate to a specific value

### Ex: Could the parameter ```drinkMost days``` be equal to 0.8799548?

```{r compare to 0.5}
new.t.value <- (estimate - 0.8799548)/std.error
2*pt(abs(new.t.value), df = mod_UK1$df.residual, lower.tail = FALSE)
library(car)
## this uses a F-test, but similar (see later)
linearHypothesis(mod_UK1, c("drinkMost days = 0.8799548"))$"Pr(>F)"[2]
```

Notice:

```{r confint Most days}
confint(mod_UK1, parm = "drinkMost days")
```

## Comparison to the one sample Student's t-test

```{r compare t-tests one sample}
set.seed(1L)
y <- rnorm(10, mean = 2)
t.test(y, mu = 2)$p.value
summary(lm(y ~ 1 + offset(rep(2, 10))))$coef
```

## Comparison to Pearson's correlation test

```{r correlation}
x <- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1) ## from ?cor.test
y <- c( 2.6,  3.1,  2.5,  5.0,  3.6,  4.0,  5.2,  2.8,  3.8)
c(rho = cor(y, x), pv = cor.test(y, x)$p.value)
summary(lm(y ~ x))$coef["x", ]
cov(x,y)/(sd(x)*sd(y)) # definition of rho
coef(lm(y~x))["x"] * sd(x)/sd(y)  ## because slope = cov(x, y)/var(x)
```

# Comparing the fitted model to the fit of a null model

## Using the function ```anova()```

```{r null fit}
mod_UK0 <- lm(height ~ 1, data = mod_UK1$model) ## we use model frame to keep same dataset
anova(mod_UK0, mod_UK1)  ## same line as the last one from summary(mod_UK1)
```

<br>

### This test should always be performed first before testing estimates individually!

## The F-test step by step

```{r anova null 2}
anova(mod_UK0, mod_UK1)
c(df = df <- 11231 - 11225, SS = SS <- 443561 - 256402, F = F <- (SS/df) / (256402/11225))
SS/df  ## mean square treatment = var between = loss in estimated error variance
```

## The F-test step by step

```{r anova null}
anova(mod_UK0, mod_UK1)
c(df = df <- 11231 - 11225, SS = SS <- 443561 - 256402, F = F <- (SS/df) / (256402/11225))
c(256402/11225, summary(mod_UK1)$sigma^2)  ## mean square error = var within = estimate of error variance
```


## The F-test step by step

```{r anova null 3}
anova(mod_UK0, mod_UK1)
c(df = df <- 11231 - 11225, SS = SS <- 443561 - 256402, F = F <- (SS/df) / (256402/11225))
(p.value <- 1 - pf(F, df1 = 6, df2 = 11225))
```

## The likelihood ratio test

```{r}
mod_UK2 <- lm(height ~ milk, data = UK)
mod_UK0 <- lm(height ~ 1, data = mod_UK2$model)
anova(mod_UK0, mod_UK2, test = "Chisq")  ## does not show the statistic :-(
stat <- 2*(c(logLik(mod_UK2)) - c(logLik(mod_UK0)))
1 - pchisq(stat, df = 1)  ## df = K model 1 - K model 2
anova(mod_UK0, mod_UK2, test = "F")[2, "Pr(>F)"]  ## for LM F-test and LRT are equivalent!
```


# Comparing two nested models

## t-test and F-test are equivalent for quantit var

```{r F test milk}
mod_UK3 <- lm(height ~ milk + sex*weight, data = UK)
summary(mod_UK3)$coefficient["milk", ]
mod_UK3_no_milk <- lm(height ~ sex*weight, data = mod_UK3$model)
anova(mod_UK3_no_milk, mod_UK3)
round(summary(mod_UK3)$coefficient["milk", "t value"]^2-anova(mod_UK3_no_milk, mod_UK3)$F[2], 2) ## t^2 = F
```

## Nested model comparison allows for testing the overall effect of several parameters

```{r F test weight}
mod_UK1 <- lm(height ~ drink + sex*weight, data = UK)
mod_UK1_no_weight <- lm(height ~ drink + sex, data = mod_UK1$model)
anova(mod_UK1_no_weight, mod_UK1)
```

## Nested model comparison thus allows for testing the overall effect of a qualitative variable

```{r F test alcool}
mod_UK1 <- lm(height ~ drink + sex*weight, data = UK)
mod_UK1_no_alcool <- lm(height ~ sex*weight, data = mod_UK1$model)
anova(mod_UK1_no_alcool, mod_UK1)
```

## Testing all variables in one go with ```anova()```

### Sequential test: Type 1 ANOVA (dangerous)

```{r anova 1}
anova(mod_UK1)
```

## Testing all variables in one go with ```Anova()```

### Marginal test: Type 2 ANOVA (recommended)

```{r anova 2}
library(car)
Anova(mod_UK1)
```

<br>

Note : test of simple terms consider other simple terms to be there, but not interactions.

Tip: never use type 3 ANOVA!


## Robustness of the F test

```{r F test robustness}
set.seed(1L)
pv.F <- replicate(1000, {
  mod_UK1$model$height_no_alcool <- as.matrix(simulate(mod_UK1_no_alcool))
  mod_alcool_fooled <- lm(height_no_alcool ~ drink + sex*weight, data = mod_UK1$model)
  mod_alcool_not_fooled <- lm(height_no_alcool ~ sex*weight, data = mod_UK1$model)
  anova(mod_alcool_not_fooled, mod_alcool_fooled)[2, "Pr(>F)"]
})
mean(pv.F <= 0.05)
```

## Robustness of the F test

```{r plot robustness F, fig.align = "center", fig.width = 5, fig.height = 5}
plot(ecdf(pv.F))
abline(0, 1, col = "red")  ## uniform reference
```


## LRT by parametric boostrap

```{r LRT param boot}
mod_UK_milk <- lm(height ~ milk + sex*weight, data = UK)
mod_UK_no_milk <- lm(height ~ sex*weight, data = mod_UK_milk$model)
anova(mod_UK_no_milk, mod_UK_milk, test = "Chisq")[2, "Pr(>Chi)"]
(stat.obs <- 2*(c(logLik(mod_UK_milk)) - c(logLik(mod_UK_no_milk))))

stat.rand <- replicate(200, {
  newY <- as.matrix(simulate(mod_UK_no_milk))
  mod_milk <- lm(newY ~ milk + sex*weight, data = mod_UK_milk$model)
  mod_no_milk <- lm(newY ~ sex*weight, data = mod_UK_milk$model)
  c(2*(logLik(mod_milk) - logLik(mod_no_milk)))
  })

(sum(stat.rand >= stat.obs) + 1) / (length(stat.rand) + 1)
```

## LRT by parametric boostrap

```{r plot LRT param boot, fig.align = "center", fig.width = 5, fig.height = 5}
hist(stat.rand)
abline(v = stat.obs, lwd = 2, lty = 2, col = "red")
```

## LRT by parametric boostrap using spaMM
```{r LRT param boot spaMM}
mod_UK_milk_spaMM <- fitme(height ~ milk + sex*weight, data = UK)
mod_UK_no_milk_spaMM <- fitme(height ~ sex*weight, data = mod_UK_milk$model)
LRT(mod_UK_milk_spaMM, mod_UK_no_milk_spaMM, boot.repl = 200)
```


# Comparing two non-nested models (with same response!)

## Comparing two non-nested models using deviance

You may use the deviance (smaller is better) or (log)likelihood  (bigger is better),
but only if the number of df is the same

```{r non nested deviance}
UK_for_both <- na.omit(UK[, c("height", "cigarettes", "sex", "weight", "milk")])
mod_UK1 <- lm(height ~ cigarettes + sex*weight, data = UK_for_both)
mod_UK5 <- lm(height ~ milk + sex*weight, data = UK_for_both)

deviance(mod_UK1)
deviance(mod_UK5)
```


## Comparing two non-nested models using AIC

You may use the AIC (AIC = -2 * (logLik - K)) (smaller is better) even if df differ

```{r non nested AIC}
AIC(mod_UK1)
AIC(mod_UK5)
exp((AIC(mod_UK5) - AIC(mod_UK1)) / 2) ## evidence ratio
```

The model ```mod_UK1``` is ```r round(exp((AIC(mod_UK5) - AIC(mod_UK1)) / 2), 1)``` times more
likely to minimizes the (estimated) information loss than ```mod_UK5```.


## Comparing two non-nested models using adjusted R-squared

You may use ajusted R-squared (higher is better) even if df differ

<br>

```{r}
summary(mod_UK1)$adj.r.squared

SST <- sum((fitted(mod_UK1) - mean(fitted(mod_UK1)))^2)
SSR <- sum((residuals(mod_UK1) - mean(residuals(mod_UK1)))^2)
(r.squared <- SST/(SST + SSR))  ## Note: SST+SSR = sum((mod_UK1$model$height - mean(mod_UK1$model$height))^2)
1 - (1 - r.squared) * ((nrow(mod_UK1$model) - 1)/mod_UK1$df.residual)  ## adjusted r-squared

summary(mod_UK5)$adj.r.squared
```

## Comparing two non-nested models using parametric bootstrap

### This is the only general method to get a p-value

We consider one model as the null model of the other (we choose here the worse model)

```{r non-nested param boot}
set.seed(1L)
(LRTobs <- 2*(logLik(mod_UK1)[[1]] - logLik(mod_UK5)[[1]]))
LRTH0 <- replicate(200, {
                    UK_for_both$height <- simulate(mod_UK5)[, 1]
                    2*(logLik(update(mod_UK1))[[1]] -
                      logLik(update(mod_UK5))[[1]])
                    })
c(summary(LRTH0), "p-value" = mean(LRTH0 >= LRTobs)) ## or (sum + 1) / (R + 1), see earlier
```

## Comparing two non-nested models using parametric bootstrap using spaMM

```{r non-nested param boot spaMM}
mod_UK1_spaMM <- fitme(height ~ cigarettes + sex*weight, data = UK_for_both)
mod_UK5_spaMM <- fitme(height ~ milk + sex*weight, data = UK_for_both)
LRTUK5null_obs <- 2*(logLik(mod_UK1_spaMM) - logLik(mod_UK5_spaMM))
refit_mod <- function(y, data, mod_null, mod_full, ...) {
  data$height <- y  ## replace original response by response simulated under mod_null
  mod_full_under_null <- update(mod_full, data = data)
  mod_null_under_null <- update(mod_null, data = data)
  2*(logLik(mod_full_under_null) - logLik(mod_null_under_null))
}
LRTUK5null_boot <- spaMM_boot(mod_UK5_spaMM, simuland = refit_mod, nsim = 200,
                              data = UK_for_both, mod_null = mod_UK5_spaMM,
                              mod_full = mod_UK1_spaMM)[["bootreps"]]
(sum(LRTUK5null_boot >= LRTUK5null_obs) + 1) / (length(LRTUK5null_boot) + 1)
```


# Comparing too many things

## Simulate nothingness

```{r simu_null}
simu_null <- function(N, K) {
  d <- as.data.frame(replicate(K + 1, rnorm(N)))
  colnames(d)[1] <- "y"
  colnames(d)[2:(K + 1)] <- paste0("x", 1:K)
  return(d)
}

set.seed(1L)
simu_null(N = 5, K = 3)
```

## Testing nothingness

```{r robustness}
K <- 3
set.seed(1L)
lm(y ~ ., data = simu_null(N = 5, K = K))
set.seed(1L)
(p.values <- Anova(lm(y ~ ., data = simu_null(N = 5, K = K)))["Pr(>F)"][1:K, 1])
min(p.values) <= 0.05
```

## Comparing many estimates/models decreases the robustness!!!

```{r robustness 2}
set.seed(1L)
mean(replicate(1000, min(Anova(lm(y ~ ., data = simu_null(N = 100, K = 1)))["Pr(>F)"][1, 1]) <= 0.05))

mean(replicate(1000, min(Anova(lm(y ~ ., data = simu_null(N = 100, K = 3)))["Pr(>F)"][1:3, 1]) <= 0.05))

mean(replicate(1000, min(Anova(lm(y ~ ., data = simu_null(N = 100, K = 10)))["Pr(>F)"][1:10, 1]) <= 0.05))

mean(replicate(1000, min(Anova(lm(y ~ ., data = simu_null(N = 1000, K = 50)))["Pr(>F)"][1:50, 1]) <= 0.05))
```

## Why not do backward selection?

```{r backward}
backward <- function(k, nsimu=100) {
  pv.signif.rec <- matrix(rep(NA, 2*nsimu), nrow = 2)
  row.names(pv.signif.rec) <- c("full", "backward")
  for (i in 1:nsimu) {
    d <- replicate(k, rnorm(500))
    formul <- as.formula(paste("d[,1] ~", paste("d[,", 2:k,"]", collapse = "+")))
    mod.full <- lm(formul)
    mod.null <- lm(d[ ,1] ~ 1)
    mod.backward <- step(mod.full, direction = "backward", trace = FALSE)
    pv.full <- anova(mod.null, mod.full)$Pr[2]
    pv.backward <- anova(mod.null, mod.backward)$Pr[2]
    pv.signif.rec["full", i] <- ifelse(is.na(pv.full), FALSE, pv.full <= 0.05)
    pv.signif.rec["backward", i] <- ifelse(is.na(pv.backward), FALSE, pv.backward <= 0.05)
  }
  return(apply(pv.signif.rec, 1, mean))
}

result_backward <- sapply(c(3, 5, 10, 15, 20, 50), function(i) {backward(k = i)})
```

## Why not do backward selection?

```{r backward plot, fig.align = "center", fig.width = 4, fig.height = 4}
plot(result_backward["full", ] ~ c(3, 5, 10, 15, 20, 50),
  type = "b", col = "green", ylim = c(0, 1), xlab = "Initial number of variables",
  ylab = "Proportion of significant models | H0", log = "x")
points(result_backward["backward", ] ~ c(3, 5, 10, 15, 20, 50),
  type = "b", col = "red")
abline(h = 0.05, lty = 2)
legend("topleft", fill = c("red", "green"),
       legend = c("step selection (backward AIC)", "full model"), bty = "n")
```



## What shall I do?

* Don't go fishing (forget backward selection!!!)
* Always compare the fit to null model fit and do not do more tests if not signif

```{r null fit test}
mean(replicate(1000, {
  d <- simu_null(N = 1000, K = 50)
  mod <- lm(y ~ ., data = d)
  mod0 <- lm(y ~ 1, data = d)
  anova(mod, mod0)[2, "Pr(>F)"] <= 0.05
}))

```

* Always test the qualitative variable as a whole before comparing levels


## What shall I do?

* Correct for multiple comparisons

```{r ghlt, eval = FALSE}
mod_drink <- lm(height ~ drink, data = UK)
library(multcomp)
summary(glht(mod_drink, linfct = mcp(drink = "Tukey")))
```

```{r ghlt echo, echo = FALSE, message = FALSE}
options(width = 200)
mod_drink <- lm(height ~ drink, data = UK)
library(multcomp)
summary(glht(mod_drink, linfct = mcp(drink = "Tukey")))
```

## What shall I do?

* Correct for multiple comparisons

```{r ghlt plot, fig.align = "center", fig.width = 8, fig.height = 4}
par(oma = c(0, 15, 0, 0))  ## increase left outer margin
plot(glht(mod_drink, linfct = mcp(drink = "Tukey")))
```

## What shall I do?

* Use at least well choosen contrasts

```{r drink}
UK$drink2 <- relevel(UK$drink, ref = "Not at all")
summary(mod_drink2 <- lm(height ~ drink2, data = UK))$coef
## compare to summary(glht(mod_drink2, linfct = mcp(drink2 = "Dunnett")))
```


## What you need to remember

* how to use the functions ```summary()```, ```anova()``` and ```Anova()```
* that parameters are tested using t-tests
* that you can use LM to do usual Student's t-tests and Pearson correlation tests
* that variables are tested using F-tests or Likelihood Ratio Tests
* how to check the statistical robustness of a test
* how to do a LRT by parametric bootstrap
* that type II ANOVA is the way to go for testing variables
* that doing many tests is dangerous


# Table of contents

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Point estimates](./LM_point_estimates.html)
* 2.2 [Uncertainty in point estimates](./LM_uncertainty.html)
* 2.3 [Tests](./LM_tests.html)
* 2.4 [Assumptions and Outliers](./LM_assumptions.html)
* 2.5 [Let's practice more](./LM_practice.html)

