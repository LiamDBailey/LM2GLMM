---
title: "LMM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{4.0 Linear Mixed-effects Models}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(spaMM)
library(lme4)
library(car)
spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LMM_intro/", fig.path = "./fig_knitr/LMM_intro/", fig.width = 5, fig.height = 5, fig.align = "center")
```


## Mixed-effects models

* 4.0 [Introduction](./LMM_intro.html)
* 4.1 [More than one random effect](./LMM_multi.html)
* 4.2 [More extensions from simple LMM](./LMM_more.html)


## You will learn in this session

* why you may need linear mixed-effects models
* what mixed models are
* how to use ```lme4``` and ```spaMM``` to fit mixed models
* how to fit a simple mixed models by hand in a few lines of code
* what BLUPs are
* how to make predictions at two different levels
* how to test effects using parametric bootstraps
* how to decide between considering a factor as fixed or random
* how to check assumptions for a linear mixed-effects model


# Introduction

## Why Linear Mixed-effects Models?


### Goal:

To study, or to account for, unobservable sources of heterogeneity between observations.

<br>

Mixed-effects models allow for:

* the study of other questions than LM (e.g. heritability)
* the fixing of assumption violations in LM (lack of dependence, some cases of overdispersion)
* the reduction of the uncertainty in estimates and predictions in cases where many parameters would have to be estimated $\underline{\textrm{at the cost of an additional hypothesis}}$ (the distribution of the random effects)

<br>

The main sources of heterogeneity considered by mixed-effects models are:

* origin (in its widest sense)
* time
* space


## The linear mixed-effects model

<br>

### Definition

* a LMM is a specific linear model for which, given the design matrix $\mathbf{X}$, the responses ($\mathbf{Y}$) are no longer independent, but where the correlations can be described in terms of a random effect, i.e. a random variable that is not included in the predictor variables


## Mathematical notation of LM

LM, which we have seen before:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \epsilon$ </font></center>

<center><font size = 4>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\sigma^2 & 0 & 0 & \dots & 0 \\
0 & \sigma^2 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \sigma^2
\end{bmatrix}\right)
$$
</font></center>

## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
c_{1,1} & c_{1,2} & c_{1,3} & \dots & c_{1,q} \\
c_{2,1} & c_{2,2} & c_{2,3} & \dots & c_{2,q} \\
c_{3,1} & c_{3,2} & c_{3,3} & \dots & c_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_{q,1} & c_{q,2} & c_{q,3} & \dots & c_{q,q} \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>

<br>

with $\text{E}(b) = 0$, $\text{Cov}(b) = \mathbf{C}$, which is symmetrical ($c_{i, j} = c_{j, i}$). Also, $\text{Cov}(b, \epsilon) = 0$.


## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ and often, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\lambda & 0 & 0 & \dots & 0 \\
0 & \lambda & 0 & \dots & 0 \\
0 & 0 & \lambda & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \lambda \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>


# Fitting procedure

## A simple simulation function

```{r}
simulate_Mix <- function(intercept, slope, n, group_nb, var.rand, var.error){
  data <- data.frame(intercept = intercept, slope = slope, x = runif(n)) 
  group_compo <- rmultinom(n = 1, size = n, prob = c(rep(1/group_nb, group_nb)))
  data$group <- factor(rep(paste("group", 1:group_nb, sep = "_"), group_compo))
  data$b <- rep(rnorm(group_nb, mean = 0, sd = sqrt(var.rand)), group_compo)
  data$error <- rnorm(n, mean = 0, sd = sqrt(var.error))
  data$y <- data$intercept + data$slope*data$x + data$b + data$error
  return(data)
}

set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```


## Our toy dataset

```{r}
Aliens
```


## Fitting the model with ```lme4```

```{r}
library(lme4)
(mod_lme4 <- lmer(y ~ x + (1|group), data = Aliens, REML = FALSE))
c(var.group = as.numeric(attr(VarCorr(mod_lme4)$group, "stddev")^2),
  var.error = as.numeric(attr(VarCorr(mod_lme4), "sc")^2))  ## note: those are biased estimates (ML not REML)
```


## Fitting the model with ```spaMM```

```{r}
library(spaMM)
(mod_spaMM <- fitme(y ~ x + (1|group), data = Aliens, method = "ML"))
```


## Functions for fitting the mixed model numerically

```{r}
lik_b <- function(b.vec, level, intercept, slope, var.rand, var.error, data, scale = 1){
  lik <- sapply(b.vec, function(b){
    sub_data <- data[which(data$group == level), ]
    sub_data$pred <- intercept + slope*sub_data$x + b
    sub_data$conditional.density <- dnorm(sub_data$y, mean = sub_data$pred, sd = sqrt(var.error))
    return(dnorm(b, mean = 0, sd = sqrt(var.rand)) * prod(sub_data$conditional.density))
  })
  return(scale * lik)
}
```

<br>

```{r}
log_lik_b_prod <- function(param, data, scale = 1){
  log_lik_vec <- sapply(levels(data$group), function(level) {
    log(integrate(lik_b, -Inf, Inf, level = level, intercept = param[1], slope = param[2],
                  var.rand = param[3], var.error = param[4], data = data)$value)})
  return(scale * sum(log_lik_vec))
}
```


## Testing the functions


Let's test the function under fixed parameter values:
```{r}
log_lik_b_prod(param = c(50, 1.5, 2, 0.5), data = Aliens) ## test functions above
mod_constr <- fitme(y ~ 0 + (1|group) + offset(50 + 1.5*x), data = Aliens, fixed = list(lambda = 2, phi = 0.5))
logLik(mod_constr)
```


## Fitting the mixed model numerically

```{r}
bad_mod <- lm(y ~ x + group, data = Aliens)
```

```{r}
(init_values <- c(bad_mod$coefficients[1], bad_mod$coefficients[2],
                  var.group = var(bad_mod$coefficients[-c(1:2)]),
                  var.error = deviance(bad_mod) / bad_mod$df.residual))
```

```{r numerical fit}
system.time(
  opt <-  nloptr::nloptr(x0 = init_values, eval_f = log_lik_b_prod, data = Aliens, scale = -1,
                         lb = 0.5*init_values, ub = 2*init_values,
                         opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))
)
```


## Fitting the mixed model numerically

```{r}
estimates <- rbind(opt$solution, as.numeric(c(mod_spaMM$fixef, mod_spaMM$lambda, mod_spaMM$phi)))
colnames(estimates) <- c("intercept", "slope", "var.group", "var.error")
rownames(estimates) <- c("numeric", "spaMM")
estimates

c(logLik.num = -1 * opt$objective, logLik.spaMM = logLik(mod_spaMM)[[1]])
```


## The estimation of random effects

### We estimate the realized values of the random variable:

```{r fit b1}
ranef_alex <- sapply(levels(Aliens$group), function(group) {
  nloptr::nloptr(x0 = 0, lik_b, level = group, intercept = opt$solution[1],
                 slope = opt$solution[2], var.rand = opt$solution[3],
                 var.error = opt$solution[4], data = Aliens, scale = -1,
                 lb = -4*sqrt(opt$solution[3]), ub = 4*sqrt(opt$solution[3]),
                 opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))$solution})

rbind(ranef_alex,
      ranef_lme4 = as.numeric(t(lme4::ranef(mod_lme4)[[1]])),
      ranef_spaMM = as.numeric(unlist(ranef(mod_spaMM))))
```


## Best Linear Unbiased Predictions

* The numbers we obtained with ```ranef``` are BLUPs of the random effects.
* Even if BLUPs can be estimated, they are not parameters of the statistical model!!!
* BLUPs can be computed using ML or REML but REML is recommended (I did ML here for simplicity).

```{r, fig.height = 3.5, fig.width = 4}
curve(dnorm(x, mean = 0, sd = sqrt(estimates[1, "var.group"])), -5, 5, ylab = "density", xlab = "b")
points(dnorm(ranef_alex, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_alex, col = "blue", type = "h")
points(dnorm(ranef_alex, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_alex, col = "blue")
```


# A simple example of LMM

## The ```nlme::Oats``` dataset

```{r}
data("Oats", package = "nlme")
Oats2 <- as.data.frame(Oats)
Oats2$Block <- factor(Oats2$Block, ordered = FALSE, levels = sort(levels(Oats2$Block)))
head(Oats2)
str(Oats2)
```


## The ```nlme::Oats``` dataset

```{r}
coplot(yield ~ nitro | Variety + Block, data = Oats2, type = "b")
```

## LMM using ```lme4```

```{r}
(mod_lmm_lme4 <- lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE))
```


## LMM using ```lme4```

```{r}
head(mod_lmm_lme4@pp$X)  ## X
mod_lmm_lme4@beta  ## beta estimates
c(var.group = as.numeric(attr(VarCorr(mod_lmm_lme4)$Block, "stddev")^2),
  var.error = as.numeric(attr(VarCorr(mod_lmm_lme4), "sc")^2))  ## lambda and phi estimates
```


## LMM using ```lme4```

```{r}
head(t(mod_lmm_lme4@pp$Zt))  ## Z
as.matrix(mod_lmm_lme4@pp$Zt)
```


## LMM using ```lme4```

```{r}
head(t(mod_lmm_lme4@pp$Zt))  ## Z
crossprod(t(as.matrix(mod_lmm_lme4@pp$Zt)))
```


## LMM using ```lme4```

```{r}
lme4::ranef(mod_lmm_lme4)  ## b estimates
```


## LMM using ```spaMM```

```{r}
(mod_lmm_spaMM <- fitme(yield ~ nitro + Variety + (1|Block), data = Oats2))
```


## LMM using ```spaMM```

```{r}
head(mod_lmm_spaMM$X.pv)  ## X
mod_lmm_spaMM$fixef  ## beta estimates
as.numeric(c(mod_lmm_spaMM$lambda, mod_lmm_spaMM$phi))  ## lambda and phi estimates
```


## LMM using ```spaMM```

```{r}
head(mod_lmm_spaMM$ZAlist[[1]])  ## Z
crossprod(mod_lmm_spaMM$ZAlist[[1]])
```


## LMM using ```spaMM```

```{r}
ranef(mod_lmm_spaMM)  ## b estimates
```

# Predictions

## Predictions with LM

```{r}
mod_lm <- lm(yield ~ nitro + Variety + Block, data = Oats2)
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
(p <- predict(mod_lm, newdata = data.for.pred, interval = "confidence", se.fit = TRUE))
```

## Predictions with LM

```{r, fig.width = 4, fig.height = 4}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block)))) 
arrows(x0 = 1:length(levels(Oats2$Block)), y0 = p$fit[, "lwr"], y1 = p$fit[, "upr"],
       code = 3, angle = 90, length = 0.05)
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block)); axis(2, las = 1); box()
```

## Marginal predictions using ```lme4```

### Prediction averaged over the random variable:
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = "new")
p1 <- predict(mod_lmm_lme4, newdata = data.for.pred, re.form = NA)
X <- matrix(c(1, 0.3, 0, 1), nrow = 1)
se.fit <- sqrt(X %*% vcov(mod_lmm_lme4) %*% t(X))
se.rand <- attr(VarCorr(mod_lmm_lme4)$Block, "stddev")
(se.predVar <- as.numeric(sqrt(se.fit^2 + se.rand^2)))
lwr <- as.numeric(p1 + qnorm(0.025) * se.predVar)
upr <- as.numeric(p1 + qnorm(0.975) * se.predVar)
c(fit = as.numeric(p1), lwr = lwr, upr = upr)  ## Wald CI for prediction
```


## Marginal predictions using ```spaMM```

### Prediction averaged over the random variable:
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = "new")
p2 <- predict(mod_lmm_spaMM, newdata = data.for.pred, intervals = "predVar")
sqrt(attr(p2, "predVar"))  ## se.predVar
c(fit = p2, attr(p2, "intervals"))
```


## Conditional predictions using ```spaMM```

### Prediction conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
p3 <- predict(mod_lmm_spaMM, newdata = data.for.pred, intervals = "predVar")
sqrt(attr(p3, "predVar"))  ## se.predVar
cbind(p3, attr(p3, "intervals"))
```


## Conditional predictions using ```lme4 + merTools```

### Prediction conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):
```{r, message = FALSE}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
library(merTools)
(p4 <- predictInterval(mod_lmm_lme4, newdata = data.for.pred, include.resid.var = FALSE))
```

<br>

Note 1: ```lme4``` does not compute assymptotic confidence interval, which is why we use ```merTools```.

Note 2: ```merTools``` uses simulation so results differ slightly each time you run the function.

## Comparing predictions from LM and LMM

```{r, fig.width = 5, fig.height = 5, echo = FALSE}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
abline(h = mean(tapply(Oats2$yield, Oats2$Block, mean)), lty = 2, lwd = 2, col = "blue")
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block))-0.1))
arrows(x0 = (1:length(levels(Oats2$Block))) - 0.1, y0 = p$fit[, "lwr"],
       y1 = p$fit[, "upr"], code = 3, angle = 90, length = 0.05)
points(p3 ~ I(1:length(levels(Oats2$Block)) + 0.1), col = "red")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.1, y0 = attr(p3, "intervals")[, 1],
       y1 = attr(p3, "intervals")[, 2], code = 3, angle = 90, length = 0.05, col = "red")
points(p4$fit ~ I(1:length(levels(Oats2$Block)) + 0.2), col = "orange")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.2, y0 = p4$upr,
       y1 = p4$lwr, code = 3, angle = 90, length = 0.05, col = "orange")
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block))
axis(2, las = 1)
box()
legend("top", fill = c("black", "red", "orange"), legend = c("fix", "spaMM", "merTools"),
       bty = "n", horiz = TRUE)
```

Note: in LMM predictions are attracted toward the mean.

## Comparing predictions from LM and LMM

```{r, eval = FALSE}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
abline(h = mean(tapply(Oats2$yield, Oats2$Block, mean)), lty = 2, lwd = 2, col = "blue")
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block))-0.1))
arrows(x0 = (1:length(levels(Oats2$Block))) - 0.1, y0 = p$fit[, "lwr"],
       y1 = p$fit[, "upr"], code = 3, angle = 90, length = 0.05)
points(p3 ~ I(1:length(levels(Oats2$Block)) + 0.1), col = "red")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.1, y0 = attr(p3, "intervals")[, 1],
       y1 = attr(p3, "intervals")[, 2], code = 3, angle = 90, length = 0.05, col = "red")
points(p4$fit ~ I(1:length(levels(Oats2$Block)) + 0.2), col = "orange")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.2, y0 = p4$upr,
       y1 = p4$lwr, code = 3, angle = 90, length = 0.05, col = "orange")
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block))
axis(2, las = 1)
box()
legend("top", fill = c("black", "red", "orange"), legend = c("fix", "spaMM", "merTools"),
       bty = "n", horiz = TRUE)
```

# Tests

## Testing the effect of ```nitro```

### LM

```{r}
mod_lm_nonitro <- lm(yield ~ Variety + Block, data = Oats2)
anova(mod_lm, mod_lm_nonitro)
```


## Testing the effect of ```nitro```

### with ```lme4``` using an asymptotic LRT

```{r}
mod_lmm_lme4_nitro <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE)
mod_lmm_lme4_nonitro <-  lmer(yield ~ Variety + (1|Block), data = Oats2, REML = FALSE)
anova(mod_lmm_lme4_nitro, mod_lmm_lme4_nonitro)
```

<br>

Note: always use ML fit for testing fixed effects!


## Testing the reliability of the test of the LRT

```{r simu lme4, fig.height = 4, fig.width = 4, warning = FALSE}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$yield <- simulate(mod_lmm_lme4_nonitro)[, 1]
  mod_lmm_lme4_new <- lmer(yield ~ nitro + Variety + (1|Block), data = Oats3, REML = FALSE)
  mod_lmm_lme4_new_nonitro <-  lmer(yield ~ Variety + (1|Block), data = Oats3, REML = FALSE)
  anova(mod_lmm_lme4_new, mod_lmm_lme4_new_nonitro)$"Pr(>Chisq)"[2]})
plot(ecdf(pvalues)); abline(0, 1, col = 2)
```


## Testing the effect of the effect of ```nitro```

### with ```lme4``` + ```pbkrtest``` using parametric bootstrap

```{r yield lme4 param boot}
library(pbkrtest)
PBmodcomp(mod_lmm_lme4_nitro, mod_lmm_lme4_nonitro, nsim = 999)
```


## Testing the effect of the effect of ```nitro```

### with ```spaMM``` using an asymptotic LRT

```{r}
mod_lmm_spaMM_nitro <-  fitme(yield ~ nitro + Variety + (1|Block), data = Oats2, method = "ML")
mod_lmm_spaMM_nonitro <-  fitme(yield ~ Variety + (1|Block), data = Oats2, method = "ML")
anova(mod_lmm_spaMM_nitro, mod_lmm_spaMM_nonitro)
```


## Testing the effect of the effect of ```nitro```

### with ```spaMM``` using parametric bootstrap

```{r yield spaMM param boot, message = FALSE}
anova(mod_lmm_spaMM_nitro, mod_lmm_spaMM_nonitro, boot.repl = 999)
```


## The old-fashion ```aov()``` alternative?

```{r}
mod_aov <- aov(yield ~ nitro + Variety + Error(Block), data = Oats2)
coef(mod_aov)
```

The coefficient are the same (except the intercept) because the experimental design is well balanced.

Never use ```aov``` if this is not the case!


## The old-fashion ```aov()``` alternative?

```{r}
summary(mod_aov)
```

<br>

This test is "not particularly sensible statistically" (as the authors of ```?aov``` put it)...


## Testing the reliability of the test from ```aov()```

```{r simu aov, fig.height = 4, fig.width = 4}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$nitro <- runif(1:nrow(Oats3))  ## simulate H0 for nitro effect
  mod_aov_sim <- aov(yield ~ nitro + Variety + Error(Block), data = Oats3)
  summary(mod_aov_sim)[[2]][[1]][2, "Pr(>F)"]
})
plot(ecdf(pvalues))
abline(0, 1, col = 2)
```


## A summary table for mixed models?

Testing estimates of the different levels is not implemented in ```lme4``` or ```spaMM``` because the t-test have bad properties for LMM. Some alternatives have been proposed:

```{r, message = FALSE}
library(lmerTest)
mod_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE)
summary(mod_lmm_lme4_nitro_bis)$coefficients
```

## Testing ```lme4 + lmerTest``` summary table
```{r simu lmerTest, fig.height = 4, fig.width = 4}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$nitro <- runif(1:nrow(Oats3))  ## simulate H0 for nitro effect
  mod_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats3, REML = FALSE)
  summary(mod_lmm_lme4_nitro_bis)$coefficients[2, "Pr(>|t|)"]
})
plot(ecdf(pvalues))
abline(0, 1, col = 2)
```


# Information Criteria

## Information Criteria

```{r}
AIC(mod_lm)
AIC(mod_lmm_lme4)
print(AIC(mod_lmm_spaMM))
```

* rely on the marginal AIC if you are interested in marginal predictions.
* rely on the conditional AIC if you are interested in conditional predictions.


# To mix or not to mix?


## Choosing between fixed and random effects

The choice between considering a predictor has having fixed or random effects can be difficult; it depends on the trade-off between the pros and cons of both approaches.

### Fixed effects

* no assumption about the distribution of the effects associated with each level of a predictor
* require many new datapoints for each additional levels to get reliable results
* allow for the prediction of the effect of observed levels only (for factors)
* simple to study

### Random effects

* the effects associated with each level of a predictor follow a given probability distribution
* require at least one new datapoint for each additional levels to get reliable results (more is better)
* allow for the prediction of the effect of both observed and unobserved levels (for factors)
* more difficult to study


# Assumptions

## What are the assumptions in LMM?

* same as LM (except for independence)
* given the fixed effects and the realized values of the random effects, the residuals must be independent 
* the random effects must follow the assumed distribution
* the levels of the factorial variable for which random effects are estimated must be representative from the whole population

<br>

Note:

* companion packages exist for checking outliers on ```lme4``` fits: ```HLMdiag``` and ```influence.ME```
* the package ```DHARMa``` works for ```lme4```!
* for now ```spaMM``` is quite limited in terms of tools to check assumptions


## Plotting residuals

```{r}
plot(mod_lmm_lme4, type = c("p", "smooth"))  ## see ?lme4:::plot.merMod for details
```


## Plotting residuals

```{r}
plot(mod_lmm_lme4, resid(., scaled=TRUE) ~ fitted(.) | Block, abline = 0)
```


## Plotting residuals

```{r}
lattice::qqmath(mod_lmm_lme4, id = 0.05) ## id allows to see outliers
```


## Plotting BLUPs

```{r, fig.width = 4, fig.height = 4}
lattice::qqmath(lme4::ranef(mod_lmm_lme4, condVar = TRUE))
```


## Using simulated residuals

```{r, fig.width = 8, fig.height = 4}
library(DHARMa)
r <- simulateResiduals(mod_lmm_lme4, n = 1000)
plot(r)
```


## Using simulated residuals

### How to deal with random effects matters!
```{r}
r_unconditional <- simulateResiduals(mod_lmm_lme4, n = 1000)  ## resimulate BLUPs
testTemporalAutocorrelation(r_unconditional, time = 1:nrow(Oats2), plot = FALSE)
r_conditional <- simulateResiduals(mod_lmm_lme4, re.form = NULL, n = 1000)  ## conditional to fitted BLUPs
testTemporalAutocorrelation(r_conditional, time = 1:nrow(Oats2), plot = FALSE)
```

## What you need to remember

* why you may need linear mixed-effects models
* what mixed models are
* how to use ```lme4``` and/or ```spaMM``` to fit mixed models
* what BLUPs are
* how to make predictions at two different levels
* how to test effects using parametric bootstraps
* how to decide between considering a factor as fixed or random
* how to check assumptions for a linear mixed-effects model


# Table of contents

## Mixed-effects models

* 4.0 [Introduction](./LMM_intro.html)
* 4.1 [More than one random effect](./LMM_multi.html)
* 4.2 [More extensions from simple LMM](./LMM_more.html)
