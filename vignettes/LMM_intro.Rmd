---
title: "LMM & GLMM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{4.0 Linear Mixed-effects Models & Generalized Linear Mixed-effects Models }
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(doSNOW)
spaMM::spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LMM_intro/", fig.path = "./fig_knitr/LMM_intro/", fig.width = 5, fig.height = 5, fig.align = "center")
```


## Mixed-effects models

* 4.0 [Introduction to LMM & GLMM](./LMM_intro.html)
* 4.1 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.2 [A showcase of some useful applications](./LMM_showcase.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>


## You will learn in this session

* what mixed models (LMM & GLMM) are
* how to use **{lme4}**, **{spaMM}** and **{glmmTMB}** to fit mixed models
* how to make predictions, tests, CI...
* how to check assumptions
* when to consider effects as fixed or random
* how to implement different random effects structure


# Linear Mixed-effects Models


## Why mixed models?


### Goal:

To study, or to account for, unobservable sources of heterogeneity between observations.

<br>

Mixed-effects models allow for:

* the study of other questions than LM/GLM (e.g. heritability)
* the fixing of assumption violations in LM/GLM (lack of dependence, some cases of overdispersion)
* the reduction of the uncertainty in estimates and predictions in cases where many parameters would have to be estimated at the cost of an additional hypothesis (the distribution of the random effects)

<br>

The main sources of heterogeneity considered by mixed-effects models are:

* origin (in its widest sense)
* time
* space


## The linear mixed-effects model

<br>

### Definition

* a LMM is a specific linear model for which, given the design matrix $\mathbf{X}$, the responses ($\mathbf{Y}$) are no longer independent, but where the correlations can be described in terms of a random effect, i.e. a random variable that is not included in the predictor variables


## Mathematical notation of LM

LM, which we have seen before:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \epsilon$ </font></center>

<center><font size = 4>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\sigma^2 & 0 & 0 & \dots & 0 \\
0 & \sigma^2 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \sigma^2
\end{bmatrix}\right)
$$
</font></center>

## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
c_{1,1} & c_{1,2} & c_{1,3} & \dots & c_{1,q} \\
c_{2,1} & c_{2,2} & c_{2,3} & \dots & c_{2,q} \\
c_{3,1} & c_{3,2} & c_{3,3} & \dots & c_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_{q,1} & c_{q,2} & c_{q,3} & \dots & c_{q,q} \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>

<br>

with $\text{E}(b) = 0$, $\text{Cov}(b) = \mathbf{C}$, which is symmetrical ($c_{i, j} = c_{j, i}$). Also, $\text{Cov}(b, \epsilon) = 0$.


## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ and often, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\lambda & 0 & 0 & \dots & 0 \\
0 & \lambda & 0 & \dots & 0 \\
0 & 0 & \lambda & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \lambda \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>

<br>

Note: the dimensions of the matrices differ: $n \times n$ for $\mathbf{X}$ vs $q \times q$ for $\mathbf{Z}$

# Fitting procedure

## A simple simulation function

```{r}
simulate_Mix <- function(intercept, slope, n, group_nb, var.rand, var.error){
  data <- data.frame(intercept = intercept, slope = slope, x = runif(n)) 
  group_compo <- rmultinom(n = 1, size = n, prob = c(rep(1/group_nb, group_nb)))
  data$group <- factor(rep(paste("group", 1:group_nb, sep = "_"), group_compo))
  data$b <- rep(rnorm(group_nb, mean = 0, sd = sqrt(var.rand)), group_compo)
  data$error <- rnorm(n, mean = 0, sd = sqrt(var.error))
  data$y <- data$intercept + data$slope*data$x + data$b + data$error
  return(data)
}

set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```

<br>

Note: look carefully at the next slide to better understand.


## Our toy dataset

```{r}
Aliens
```


## Fitting the model with **{lme4}**

```{r, message=FALSE}
library(lme4)
(fit_lme4 <- lmer(y ~ x + (1|group), data = Aliens)) ## REML fit by default!
print(VarCorr(fit_lme4), comp = "Variance") ## extract REML variance estimates
```


## Fitting the model with **{spaMM}**

```{r, message=FALSE}
library(spaMM)
(fit_spaMM <- fitme(y ~ x + (1|group), data = Aliens, method = "REML"))
```


## Functions for fitting the mixed model numerically

```{r}
lik_b <- function(b.vec, level, intercept, slope, var.rand, var.error, data, scale = 1) {
  lik <- sapply(b.vec, function(b){
    sub_data <- data[which(data$group == level), ]
    sub_data$pred <- intercept + slope*sub_data$x + b
    sub_data$conditional.density <- dnorm(sub_data$y, mean = sub_data$pred, sd = sqrt(var.error))
    return(dnorm(b, mean = 0, sd = sqrt(var.rand)) * prod(sub_data$conditional.density))
  })
  return(scale * lik)
}
```

<br>

```{r}
log_lik_b_prod <- function(param, data, scale = 1){
  log_lik_vec <- sapply(levels(data$group), function(level) {
    log(integrate(lik_b, -Inf, Inf, level = level, intercept = param[1], slope = param[2],
                  var.rand = param[3], var.error = param[4], data = data)$value)})
  return(scale * sum(log_lik_vec))
}
```

Note: this is a ML fit for simplicity.


## Testing the functions

Let's test the function under fixed parameter values:
```{r}
log_lik_b_prod(param = c(50, 1.5, 2, 0.5), data = Aliens) ## test functions above
fit_constr <- fitme(y ~ 0 + (1|group) + offset(50 + 1.5*x), data = Aliens,
                    fixed = list(lambda = 2, phi = 0.5))
logLik(fit_constr)
```


## Fitting the mixed model numerically

```{r}
bad_mod <- lm(y ~ x + group, data = Aliens)
```

```{r}
(init_values <- c(bad_mod$coefficients[1], bad_mod$coefficients[2],
                  var.group = var(bad_mod$coefficients[-c(1:2)]),
                  var.error = deviance(bad_mod) / bad_mod$df.residual))
```

```{r numerical fit}
system.time(
  opt <-  nloptr::nloptr(x0 = init_values, eval_f = log_lik_b_prod, data = Aliens, scale = -1,
                         lb = 0.5*init_values, ub = 2*init_values,
                         opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))
)
```


## Fitting the mixed model numerically

```{r}
fit_spaMM_ML <- fitme(y ~ x + (1|group), data = Aliens, method = "ML")
estimates <- rbind(opt$solution, as.numeric(c(fit_spaMM_ML$fixef, fit_spaMM_ML$lambda, fit_spaMM_ML$phi)))
colnames(estimates) <- c("intercept", "slope", "var.group", "var.error")
rownames(estimates) <- c("numeric", "spaMM")
estimates

c(logLik.num = -1 * opt$objective, logLik.spaMM = logLik(fit_spaMM_ML)[[1]])
```


## The estimation of random effects

We estimate the realized values of the random variable:

```{r fit b1}
ranef_num <- sapply(levels(Aliens$group), function(group) {
  nloptr::nloptr(x0 = 0, lik_b, level = group, intercept = opt$solution[1],
                 slope = opt$solution[2], var.rand = opt$solution[3],
                 var.error = opt$solution[4], data = Aliens, scale = -1,
                 lb = -4*sqrt(opt$solution[3]), ub = 4*sqrt(opt$solution[3]),
                 opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))$solution})

rbind(ranef_num,
      ranef_spaMM = as.numeric(unlist(ranef(fit_spaMM_ML))))
```


## Best Linear Unbiased Predictions

* The numbers we obtained with `ranef` are BLUPs of the random effects.
* Even if BLUPs can be estimated, they are not parameters of the statistical model!!!
* BLUPs can be computed using ML or REML (REML is best when BLUPs are not added to fitted values, unclear otherwise)

```{r, fig.height = 3.5, fig.width = 4}
curve(dnorm(x, mean = 0, sd = sqrt(estimates[1, "var.group"])), -5, 5, ylab = "density", xlab = "b")
points(dnorm(ranef_num, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_num, col = "blue", type = "h")
points(dnorm(ranef_num, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_num, col = "blue")
```


# A simple example of LMM

## The `oatsyield` dataset

```{r}
head(oatsyield)
str(oatsyield)
```


## The `oatsyield` dataset

```{r}
coplot(yield ~ nitro | Variety + Block, data = oatsyield, type = "b")
```

## LMM using **{lme4}**

```{r}
(fit_lmm_lme4 <- lmer(yield ~ nitro + Variety + (1|Block), data = oatsyield))
```


## LMM using **{lme4}**

```{r}
head(model.matrix(fit_lmm_lme4))  ## X
fixef(fit_lmm_lme4)  ## beta estimates
print(VarCorr(fit_lmm_lme4), comp = "Variance") ## extract REML variance estimates
```


## LMM using **{lme4}**

```{r}
head(getME(fit_lmm_lme4, "Z"))  ## Z
getME(fit_lmm_lme4, "Zt") ## Z transposed
```


## LMM using **{lme4}**

```{r}
ranef(fit_lmm_lme4)  ## b estimates
```


## LMM using **{spaMM}**

```{r}
(fit_lmm_spaMM <- fitme(yield ~ nitro + Variety + (1|Block), data = oatsyield, method = "REML"))
```


## LMM using **{spaMM}**

```{r}
head(model.matrix(fit_lmm_spaMM))  ## X
fixef(fit_lmm_spaMM)  ## beta estimates
as.numeric(c(fit_lmm_spaMM$lambda, fit_lmm_spaMM$phi))  ## lambda and phi estimates
```


## LMM using **{spaMM}**

```{r}
head(get_ZALMatrix(fit_lmm_spaMM))  ## Z
```


## LMM using **{spaMM}**

```{r}
ranef(fit_lmm_spaMM)  ## b estimates
```

# Predictions

## Predictions with LM

```{r}
fit_lm <- lm(yield ~ nitro + Variety + Block, data = oatsyield)
data.for.pred <- data.frame(nitro = 0.3, Variety = "Victory", Block = levels(oatsyield$Block))
(p <- predict(fit_lm, newdata = data.for.pred, interval = "confidence", se.fit = TRUE))
```

## Predictions with LM

```{r, fig.width = 4, fig.height = 4}
plot(oatsyield$yield ~ unclass(oatsyield$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(oatsyield$Block)) + 0.5), col = "blue", cex = 0.3)
points(p$fit[, "fit"] ~ I(1:length(levels(oatsyield$Block)))) 
arrows(x0 = 1:length(levels(oatsyield$Block)), y0 = p$fit[, "lwr"], y1 = p$fit[, "upr"],
       code = 3, angle = 90, length = 0.05)
axis(1, at = 1:length(levels(oatsyield$Block)), labels = levels(oatsyield$Block)); axis(2, las = 1); box()
```

## Marginal predictions for LMM using **{lme4}**

In LMM (but not in GLMM), you can obtain predictions averaged over the random variable (i.e. marginal prediction), by considering a realisation of the random effects equals to 0:

```{r}
data.for.pred <- data.frame(nitro = 0.3, Variety = "Victory", Block = "new")
p <- predict(fit_lmm_lme4, newdata = data.for.pred, re.form = NA)
X <- matrix(c(1, 0.3, 0, 1), nrow = 1)
se.fit <- sqrt(X %*% vcov(fit_lmm_lme4) %*% t(X))
se.rand <- attr(VarCorr(fit_lmm_lme4)$Block, "stddev")
(se.predVar <- as.numeric(sqrt(se.fit^2 + se.rand^2)))
lwr <- as.numeric(p + qnorm(0.025) * se.predVar)
upr <- as.numeric(p + qnorm(0.975) * se.predVar)
c(fit = as.numeric(p), lwr = lwr, upr = upr)
```
Notes:

- unlike in **{spaMM}**, you have to do it by hand here...
- I don't know why people don't use the Student's here, but it would not make a big difference.


## Marginal predictions for LMM using **{spaMM}**

In LMM (but not in GLMM), you can obtain predictions averaged over the random variable (i.e. marginal prediction), by considering a realisation of the random effects equals to 0:

```{r}
data.for.pred <- data.frame(nitro = 0.3, Variety = "Victory", Block = "new")
predict(fit_lmm_spaMM, newdata = data.for.pred)
get_intervals(fit_lmm_spaMM, newdata = data.for.pred, intervals = "predVar")
```

Note: we use `intervals = "predVar"` and not `intervals = "fixefVar"` to account for the uncertainty in both fixed and random effects.


## Conditional predictions for LMM using **{spaMM}**

Predictions conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):

```{r}
data.for.pred.with.b <- data.frame(nitro = 0.3, Variety = "Victory", Block = levels(oatsyield$Block))
predict(fit_lmm_spaMM, newdata = data.for.pred.with.b)
get_intervals(fit_lmm_spaMM, newdata = data.for.pred.with.b, intervals = "predVar")
```


## Conditional predictions using **{lme4}** + **{merTools}**

Predictions conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):

```{r, message = FALSE}
data.for.pred <- data.frame(nitro = 0.3, Variety = "Victory", Block = levels(oatsyield$Block))
library(merTools)
predictInterval(fit_lmm_lme4, newdata = data.for.pred, include.resid.var = FALSE)
```

<br>

Note 1: **{lme4}** does not compute asymptotic confidence intervals, which is why we use **{merTools}**.

Note 2: **{merTools}** uses simulation so results differ slightly each time you run the function.


## Conditional predictions from LM and LMM

```{r, fig.width = 5, fig.height = 5, echo = FALSE}
p <- predict(fit_lm, newdata = data.for.pred, interval = "confidence", se.fit = TRUE)
p3 <- predict(fit_lmm_spaMM, newdata = data.for.pred.with.b, intervals = "predVar")
p4 <- predictInterval(fit_lmm_lme4, newdata = data.for.pred, include.resid.var = FALSE)
plot(yield ~ as.numeric(Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(oatsyield$Block)) + 0.5), col = "blue", cex = 0.3, data = oatsyield)
abline(h = mean(tapply(oatsyield$yield, oatsyield$Block, mean)), lty = 2, lwd = 2, col = "blue")
points(p$fit[, "fit"] ~ I(1:length(levels(oatsyield$Block))-0.1))
arrows(x0 = (1:length(levels(oatsyield$Block))) - 0.1, y0 = p$fit[, "lwr"],
       y1 = p$fit[, "upr"], code = 3, angle = 90, length = 0.05)
points(p3 ~ I(1:length(levels(oatsyield$Block)) + 0.1), col = "red")
arrows(x0 = (1:length(levels(oatsyield$Block))) + 0.1, y0 = attr(p3, "intervals")[, 1],
       y1 = attr(p3, "intervals")[, 2], code = 3, angle = 90, length = 0.05, col = "red")
points(p4$fit ~ I(1:length(levels(oatsyield$Block)) + 0.2), col = "orange")
arrows(x0 = (1:length(levels(oatsyield$Block))) + 0.2, y0 = p4$upr,
       y1 = p4$lwr, code = 3, angle = 90, length = 0.05, col = "orange")
axis(1, at = 1:length(levels(oatsyield$Block)), labels = levels(oatsyield$Block))
axis(2, las = 1)
box()
legend("top", fill = c("black", "red", "orange"), legend = c("fix", "spaMM", "merTools"),
       bty = "n", horiz = TRUE)
```

Note: in LMM predictions (with gaussian random effects) are always attracted toward the mean.



# Random or fixed?

## Choosing between fixed and random effects

The choice between considering a predictor has having fixed or random effect can be difficult; it depends on the trade-off between the pros and cons of both approaches.

### Fixed

* no assumption about the distribution of the parameter values associated with each level of a predictor
* requires many datapoints for each additional levels to get reliable results
* allows for the prediction of the effect of observed levels only (for factors)
* simple to study

### Random

* the values associated with the levels of a predictor follow a probability distribution (usually gaussian)
* requires at least one datapoint for each additional levels to get reliable results (more is better)
* requires many levels for the variance estimates to be reliable (5 being strict minimum)
* allow for the prediction of the effect of both observed and unobserved levels (for factors)
* more difficult to study


# Tests

## Testing the effect of `nitro`

With **{lme4}** using an asymptotic LRT:

```{r}
fit_lmm_lme4_nonitro <-  lmer(yield ~ Variety + (1|Block), data = oatsyield)
anova(fit_lmm_lme4, fit_lmm_lme4_nonitro)
```

<br>

Note: always use ML fits for testing fixed effects (but `anova()` from **{lme4}** does that automatically!)


## Testing the effect of the effect of `nitro`

With **{spaMM}** using an asymptotic LRT:

```{r}
fit_lmm_spaMM_ML <-  fitme(yield ~ nitro + Variety + (1|Block), data = oatsyield, method = "ML")
fit_lmm_spaMM_nonitro <-  fitme(yield ~ Variety + (1|Block), data = oatsyield, method = "ML")
anova(fit_lmm_spaMM_ML, fit_lmm_spaMM_nonitro)
```

<br>

Note: always use ML fits for testing fixed effects (`anova()` from **{spaMM}** does NOT do that automatically)


## Reliability of the test of the asymptotic LRT

```{r simu lme4, fig.height = 3.5, fig.width = 3.5, warning = FALSE, message = FALSE}
pvalues <- replicate(1000, {
  oatsyield$yield <- simulate(fit_lmm_lme4_nonitro)[, 1]
  fit_lmm_lme4_new <- lmer(yield ~ nitro + Variety + (1|Block), data = oatsyield, REML = FALSE) 
  ## We use ML to save time because anova won't trigger refit
  fit_lmm_lme4_new_nonitro <- lmer(yield ~ Variety + (1|Block), data = oatsyield, REML = FALSE)
  anova(fit_lmm_lme4_new, fit_lmm_lme4_new_nonitro)$"Pr(>Chisq)"[2]})
plot(ecdf(pvalues), xlim = c(0, 1), ylim = c(0, 1))
abline(0, 1, col = 2, lwd = 2, lty = 2)
```


## Testing the effect of the effect of `nitro`

With **{lme4}** + **{pbkrtest}** using parametric bootstrap:

```{r yield lme4 param boot}
pbkrtest::PBmodcomp(fit_lmm_lme4, fit_lmm_lme4_nonitro, nsim = 999)
```


## Testing the effect of the effect of `nitro`

With **{spaMM}** using parametric bootstrap:

```{r yield spaMM param boot, eval = FALSE}
anova(fit_lmm_spaMM_ML, fit_lmm_spaMM_nonitro, boot.repl = 999)
```
```{r yield spaMM param boot run, echo = FALSE, results = "hide"}
res <- anova(fit_lmm_spaMM_ML, fit_lmm_spaMM_nonitro, boot.repl = 999)
```
```{r yield spaMM param boot show, echo = FALSE}
print(res)
```


## Never use the outdated `aov()`!!!

```{r simu aov, fig.height = 3, fig.width = 3}
pvalues <- replicate(1000, {
  oatsyield$nitro <- runif(1:nrow(oatsyield))  ## simulate H0 for nitro effect
  fit_aov_sim <- aov(yield ~ nitro + Variety + Error(Block), data = oatsyield)
  summary(fit_aov_sim)[[2]][[1]][2, "Pr(>F)"]})
plot(ecdf(pvalues), xlim = c(0, 1), ylim = c(0, 1))
abline(0, 1, col = 2, lwd = 2, lty = 2)
```
Note: even the help file tells you that tests using `aov()` are *"not particularly sensible statistically"*...


## A summary table for mixed models?

Testing estimates of the different levels is not implemented in **{lme4}** or **{spaMM}** because the t-test have bad properties for LMM. Some alternatives have been proposed:

```{r, message = FALSE}
library(lmerTest)  ## overwrite lmer!
fit_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = oatsyield, REML = FALSE)
summary(fit_lmm_lme4_nitro_bis)$coefficients
detach(package:lmerTest)  ## to restore original lmer
```

<br>

Note: here you must use a ML fit (not automatic here, and the results would change if you don't!).

The method relies on the so-called Satterthwaite's degrees of freedom method, which correct the degrees of freedom to account for the facts that the design is not always ballanced.

<!--
## Testing ```lme4 + lmerTest``` summary table
```{r simu lmerTest, fig.height = 4, fig.width = 4, eval = FALSE}
oatsyield3 <- oatsyield
pvalues <- replicate(1000, {
  oatsyield3$nitro <- runif(1:nrow(oatsyield3))  ## simulate H0 for nitro effect
  fit_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = oatsyield3, REML = FALSE)
  summary(fit_lmm_lme4_nitro_bis)$coefficients[2, "Pr(>|t|)"]
})
plot(ecdf(pvalues))
abline(0, 1, col = 2)
```
-->


# Information Criteria

## Information Criteria

As for predictions, you can compute a marginal or a conditional AIC depending on whether you are interested in the fit under the average realisations of the random effect or conditional on the estimates for such random values under the observed levels in particular.

```{r}
AIC(fit_lmm_spaMM)
```

```{r, echo = FALSE}
print(AIC(fit_lmm_spaMM))
```

* rely on the marginal AIC if you are interested in marginal predictions.
* rely on the conditional AIC if you are interested in conditional predictions.

<br>

Note: **{lme4}** does not offer this possibility and only returns the marginal AIC.


# Assumptions

## What are the assumptions in LMM?

Same as LM (except for independence) +

* given the fixed effects and the realized values of the random effects, the errors must be independent 
* the random effects must follow the assumed distribution
* the levels of the factorial variable for which random effects are estimated must be representative from the whole population


## Plotting residuals with **{lme4}**

```{r}
plot(fit_lmm_lme4, type = c("p", "smooth"))  ## see ?lme4:::plot.merMod for details
```


## Plotting residuals with **{lme4}**

```{r}
plot(fit_lmm_lme4, resid(., scaled=TRUE) ~ fitted(.) | Block, abline = 0)
```


## Plotting residuals with **{lme4}**

```{r}
lattice::qqmath(fit_lmm_lme4, id = 0.05) ## id allows to see outliers
```


## Plotting BLUPs with **{lme4}**

```{r, fig.width = 4, fig.height = 4}
lattice::qqmath(lme4::ranef(fit_lmm_lme4))
```


## Plotting residuals with **{spaMM}**

```{r, fig.width = 4, fig.height = 4}
#plot(fit_lmm_spaMM)  ## broken at the moment... fix in progress!
```


## Using simulated residuals

You can also rely on **{DHARMa}** as we have seen it for GLM:

```{r, fig.width = 6, fig.height = 3}
library(DHARMa)
plot(simulateResiduals(fit_lmm_spaMM, n = 1000))
```

Note: **{DHARMa}** works for both **{spaMM}** and **{lme4}**!

<!--
## Using simulated residuals

### How to deal with random effects matters!
```{r}
r_unconditional <- simulateResiduals(fit_lmm_lme4, n = 1000)  ## resimulate BLUPs
testTemporalAutocorrelation(r_unconditional, time = 1:nrow(oatsyield), plot = FALSE)
r_conditional <- simulateResiduals(fit_lmm_lme4, re.form = NULL, n = 1000)  ## conditional to fitted BLUPs
testTemporalAutocorrelation(r_conditional, time = 1:nrow(oatsyield), plot = FALSE)
```

# Studying variation using LMM

## Estimating a variance

### Let's simulate a dataset under the assumptions of LMM

```{r}
set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```


## Estimating a variance

* Estimating variance components and estimating BLUPS are the only situation in which parameters must be fitted to the data by REstricted (or REsidual) Maximum Likelihood instead of Maximum Likelihood.
* A ML fit would lead to underestimate the variances.

<br>

Note:

* different packages and different functions within the same package may have ML or REML as a default fitting method, so always double check!
* unlike ML, REML is sensitive to changes in contrasts.


## Estimating a variance

### Model fit with ```lmer``` (```REML = TRUE``` by default)

```{r, message = FALSE}
library(lme4)
(mod <- lmer(y ~ x + (1|group), data = Aliens))
```


## Estimating a variance

### Model fit with ```fitme```

```{r, message = FALSE}
library(spaMM)
(mod2 <- fitme(y ~ x + (1|group), data = Aliens, method = "REML"))
```


## Distribution of the variance estimate

```{r distrib lambda large, warning = FALSE}
lambdas_large <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 10, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda, warning = FALSE}
lambdas <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda small, warning = FALSE}
lambdas_small <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 10
hist(lambdas_large[lambdas_large < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 30, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 2
hist(lambdas[lambdas < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 7, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group2 <- 0.1
hist(lambdas_small[lambdas_small < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group2)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 1, add = TRUE, lwd = 2, col = "red")
```


## Testing the variance

### Model fit with ```fitme```

```{r}
mod2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML")
mod2_H0_REML <- fitme(y ~ x, data = Aliens, method = "REML")
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]), df = 1, lower.tail = FALSE)
mod2_H2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML", fixed = list(lambda = 2))
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H2_REML)[[1]]), df = 1, lower.tail = FALSE)
```

Note 1: this asymptotic test is poor when the variance is low.

Note 2: **{spaMM}** allows for testing difference with a specific value.

Note 3: ```anova()``` from **{spaMM}** will not run in this case. 

## Testing the variance

### Model fit with ```lmer```

```{r}
fit_REML <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
fit_H0 <- lm(y ~ x, data = Aliens)
pchisq(2*(logLik(fit_REML)[[1]] - logLik(fit_H0)[[1]]), df = 1, lower.tail = FALSE)
anova(fit_REML, fit_H0)
```

Note: **{lme4}** does not allow for testing difference with a specific value.


## Reliability of the test

```{r test spaMM, warning=FALSE}
test <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 2))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability of the test

```{r}
plot(ecdf(test), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```


## Reliability 2 (small variance)

```{r test spaMM 2, warning=FALSE}
test2 <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 0.1))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability 2 (small variance)

```{r, fig.width=4, fig.height=4}
plot(ecdf(test2), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```

Likelihood ratio tests never work well close to parameter boundaries... (better use parametric bootstrap!)


## Testing variance using parametric bootsrap

### Based on the models fitted with **{lme4}**

```{r param boot lme4 by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(fit_REML)[[1]] - logLik(fit_H0)[[1]]))
LRTH0 <- replicate(200, {
                    Aliens$y <- simulate(fit_H0)[, 1]
                    2*(logLik(lmer(y ~ x + (1|group), data = Aliens, REML = TRUE))[[1]] -
                      logLik(lm(y ~ x, data = Aliens))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

Note using update creates problems here...


## Testing variance using parametric bootsrap

### Based on the models fitted with **{spaMM}**

```{r param boot spaMM by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]))
LRTH0_bis <- replicate(200, {
                    Aliens$y <- simulate(mod2_H0_REML)
                    2*(logLik(fitme(y ~ x + (1|group), data = Aliens, method = "REML"))[[1]] -
                      logLik(fitme(y ~ x, data = Aliens, method = "REML"))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

## Confidence interval for the variance with **{lme4}**

```{r CI lambda lme4}
fit_lmer <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
round(confint(fit_lmer, method = "profile")[1, ]^2, 2)  ## more at ?lme4:::confint.merMod
round(confint(fit_lmer, method = "boot", nsim = 1000)[1, ]^2, 2)
```

<br>

Note: there is not yet an alternative for **{spaMM}** but you can use ```boot``` as we did for LM!


## Confidence interval for the variance using ```spaMM + boot```

```{r spaMM and boot, message = FALSE, warnings = FALSE}
library(boot)
n_boot <- 1000
newYs <- simulate(mod2_REML, type = "marginal", nsim = n_boot)
res_sim <- sapply(1:n_boot, function(i) {
  Aliens$y <- newYs[, i]
  log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]])}) ## lambda on new data
exp(boot.ci(boot.out = list(R = n_boot),
        t0 = log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]]), ## original lambda
        t  = matrix(res_sim, ncol = 1), type = "basic")$basic[, 4:5])
```
-->


# Specifying multiple random effects

## The `lme4::Penicillin` dataset

```{r}
str(Penicillin)
table(Penicillin$sample, Penicillin$plate)
```

## The `lme4::Penicillin` dataset

The random effects are "crossed", i.e. we have now 2 independent random effects:

```{r}
fit_peni <- fitme(diameter ~ 1 + (1|plate) + (1|sample), data = Penicillin)
```

```{r}
fit_peni$lambda
```


## The `lme4::cake` dataset

```{r}
head(cake)
str(cake)
```


## The `lme4::cake` dataset

```{r}
table(cake$recipe, cake$replicate, cake$temperature)
```


## The `lme4::cake` dataset

The random effect is nested within a fixed effect, so we must account for that:

```{r}
fit_cake <- fitme(angle ~ recipe + temperature + (1|recipe:replicate), data = cake)
fit_cake$lambda
```

<br>

Note: considering random effects as crossed here would be wrong. For example, it would imply that the random realization of the effect associated with replicate 1 from recipe A would be the same as that for the replicate 1 from recipe B.


## The `lme4::cake` dataset

The random effect is nested within a fixed effect, so we must account for that, (alternative specification):

```{r}
cake$replicate_tot <- factor(paste(cake$recipe, cake$replicate, sep = "_"))
levels(cake$replicate_tot)
fit_cake <- fitme(angle ~ recipe + temperature + (1|replicate_tot), data = cake)
fit_cake$lambda
```

<br>

Whatever you do, the important thing is to make sure that replicates across recipes do not share the same names.


## The `carnivora` dataset

```{r}
carnivora$log_brain <- log(carnivora$Brain)
carnivora$log_body <- log(carnivora$Weight)
str(carnivora)
```


## The `carnivora` dataset

Genus names are unique across families:
```{r}
length(unique(paste(carnivora$Genus, carnivora$Family))) == length(unique(carnivora$Genus))
```

```{r, fig.height=3.5, fig.width=3.5}
coplot(log_brain ~ log_body | Family, data = carnivora)
```


## The `carnivora` dataset

Here we could consider two nested random effects:

```{r}
fit_carnivora <- fitme(log_brain ~ log_body + (1|Family/Genus), data = carnivora, method = "REML")
fit_carnivora
```


## The `carnivora` dataset

Here we could consider two nested random effects (alternative specification):

```{r}
fit_carnivora_bis <- fitme(log_brain ~ log_body + (1|Family) + (1|Family:Genus), data = carnivora, method = "REML")
fit_carnivora_bis
```


## The `carnivora` dataset

But since genus names are not (here) replicated across families, nested random effects are equivalent to crossed random effects:

```{r}
fit_carnivora_ter <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
fit_carnivora_ter
```



<!--
## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(fit_carnivora$ZAlist[[1]]))
```


## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(fit_carnivora$ZAlist[[2]]))
```


## Checking the random structure

### You can also use the ```model.matrix``` clone from **{lme4}**:

```{r}
lF <- lFormula(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora)
lF$reTrms$flist  ## list of grouping factors used in the random-effects terms; see ?mkReTrms
```
-->

## Checking the random structure

Checking the names of the BLUPs structure is an easy solution to check that you did specify the random structure correctly:

```{r}
lapply(ranef(fit_carnivora), function(x) head(names(x)))  ## or just ranef(fit_carnivora)
```

<!--
## Estimating the variances of two subgroups

### Two variances between genus

```{r}
carnivora$Canidae  <- as.numeric(carnivora$Family == "Canidae")
carnivora$Others   <- as.numeric(carnivora$Family != "Canidae")

mod2 <- fitme(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora, method = "REML")
```

<br>

Note: it does not seem to work with more than 2 variances, which I don't understand...


## Estimating the variances of two subgroups

```{r}
mod2
```


## Estimating the variances of two subgroups

```{r}
as.data.frame(ranef(mod2))
```


## Estimating the variances of two subgroups

### Same using **{lme4}**

```{r}
mod2_lme4 <- lmer(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora)
lapply(VarCorr(mod2_lme4), function(r) attr(r, "stddev")^2)
head(ranef(mod2_lme4))
```
-->

# Random slopes


## Fitting a random slope model

```{r}
(fit_rand_slope_spaMM <- fitme(log_brain ~ log_body + (log_body|Family) + (1|Genus),
                                   data = carnivora, method = "REML"))
```


## The BLUPs for the slopes

A random slope model is a model in which one slope is a random variable.

Here the effect of `log_body` is considered to vary across families:

```{r}
ranef(fit_rand_slope_spaMM)$`( log_body | Family )`
```


## Predictions

```{r, fig.width = 5.5, fig.height = 3.5}
data_pred <- predict(fit_rand_slope_spaMM,
                     newdata = expand.grid(log_body = range(carnivora$log_body),
                                           Family = levels(carnivora$Family), Genus = "new"),
                     binding = "log_brain_pred")  ## binding adds predictions to newdata!
library(ggplot2)
ggplot() +
  geom_line(aes(y = log_brain_pred, x = log_body, colour = Family), data = data_pred) +
  geom_point(aes(y = log_brain, x = log_body, colour = Family), shape = 1, data = carnivora) +
  theme_minimal()
```


## Testing the random slope using **{lme4}**

Asymptotic LRT are really poor for that, so better use parametric bootstrap:

```{r, warning=FALSE}
fit_rand_slope_lme4    <- lmer(log_brain ~ log_body + (log_body|Family) + (1|Genus), data = carnivora)
fit_no_rand_slope_lme4 <- lmer(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora)
pbkrtest::PBmodcomp(fit_rand_slope_lme4, fit_no_rand_slope_lme4)
```

<br>

Notes:

- fits differ by 2 degrees of freedom (1 for the fit of the variance of the random slope + 1 for its covariance with the random intercept).
- `pbkrtest::PBmodcomp()` uses ML logLik.


## Testing the random slope using **{spaMM}**

```{r test slope spaMM boot, results="hide"}
fit_no_rand_slope_spaMM <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus),
                                     data = carnivora, method = "REML")
LRT_obs <- -2 * (logLik(fit_no_rand_slope_spaMM) - logLik(fit_rand_slope_spaMM))  ## LRT observed
LRT_slope <- function(y) {
  carnivora$new.y <- y
  fit_slope <- fitme(new.y ~ log_body + (log_body|Family) + (1|Genus), data = carnivora, method = "REML")
  fit_no_slope <- fitme(new.y ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
  -2 * (logLik(fit_no_slope) - logLik(fit_slope))} ## LRT simulated under H0
LRT_H0 <- spaMM_boot(fit_rand_slope_spaMM, simuland = LRT_slope, type = "marginal",
                     nsim = 999, nb_cores = 1)$bootreps  ## more complex with more cores...
p_value <- sum(LRT_obs >= LRT_H0) / (length(LRT_H0) + 1)
```
```{r, cache = FALSE}
c(LRT_obs = LRT_obs, p_value = p_value)
```

<br>

Note: `anova()` from **{spaMM}** does not let you compare random structures, so we do it by hand...

<!--
## Testing if slopes differ between families

### LM
```{r}
mod5 <- lm(log_brain ~ log_body * Family + Genus, data = carnivora)
mod5noIS <- lm(log_brain ~ log_body + Family + Genus, data = carnivora)
anova(mod5, mod5noIS)
```


## Fitting uncorrelated random intercept and slope

```{r}
(mod3_alt <- fitme(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora))
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt <- lmer(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt), function(r) attr(r, "stddev")^2)
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt_bis <- lmer(log_brain ~ log_body + (log_body||Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt_bis), function(r) attr(r, "stddev")^2)
```

<br>

Note 1: the syntax ```||``` does not work in **{spaMM}** but it is just a shortcut, so it is not really needed.

Note 2: unless you have a very good reason not to, you should consider the correlations between random effects!
-->


# Generalized Linear Mixed-effects Models

## GLM + LMM = GLMM

$$\begin{array}{lcl}
\mu &=& g^{-1}(\eta)\\
\mu &=& g^{-1}(\mathbf{X}\beta + \mathbf{Z}b)\\
\end{array}
$$

with (as for GLM):

* $\text{E}(\text{Y}) = \mu = g^{-1}(\eta)$
* $\text{Var}(\text{Y}) = \phi\text{V}(\mu)$ 

<br>

Note:

* If $g^{-1}$ is the identity function, $\phi = \sigma^2$ and $\text{V}(\mu) = 1$, we have the LMM.
* If $\mathbf{Z}b = 0$, we have the GLM.
* If $g^{-1}$ is the identity function, $\phi = \sigma^2$, $\text{V}(\mu) = 1$, and $\mathbf{Z}b = 0$, we have the LM.


## The `Flatwork` dataset

```{r Flatwork}
Flatwork
```


## The `Flatwork` dataset

```{r Flatwork str}
str(Flatwork)
```


## GLMM with **{lme4}**

```{r Flatwork lme4}
(fit_glmm_lme4 <- glmer(shopping ~ gender + (1|individual) + (1|month), family = poisson(),
                        data = Flatwork))
```


## GLMM with **{spaMM}**

```{r Flatwork spaMM 2}
(fit_glmm_spaMM <- fitme(shopping ~ gender + (1|individual) + (1|month), family = poisson(),
                         data = Flatwork))
```


## Checking residuals

You can test the assumptions of (simple) GLMM with **{DHARMa}**:

```{r Flatwork res, fig.width = 9}
library(DHARMa)
resid <- simulateResiduals(fit_glmm_spaMM)
plot(resid)
```

## Overdispersion?

```{r Flatwork disp}
testDispersion(resid)
```



## Extra 0s?

```{r Flatwork shopping}
barplot(table(Flatwork$shopping))
```


## Extra 0s!

```{r Flatwork zeros}
testZeroInflation(resid)
```


## Attempt 1: negative binomial

```{r}
(fit_glmm_spaMM_NB <- fitme(shopping ~ gender + (1|individual) + (1|month), family = spaMM::negbin(),
                           data = Flatwork))
```


## Attempt 1: negative binomial

```{r}
resid2 <- simulateResiduals(fit_glmm_spaMM_NB)
plot(resid2)
```


## Attempt 1: negative binomial

```{r}
testDispersion(resid2)
```


## Attempt 1: negative binomial

```{r}
testZeroInflation(resid2)
```


## Attempt 1: negative binomial

```{r Flatwork gender param boot, results='hide'}
fit_glmm_spaMM_NB_no_gender <- fitme(shopping ~ 1 + (1|individual) + (1|month),
                                     family = spaMM::negbin(), data = Flatwork)

result <- anova(fit_glmm_spaMM_NB, fit_glmm_spaMM_NB_no_gender, boot.repl = 999)
```

```{r}
result
```


## Attempt 2: zero-augmented negative binomial

```{r TMB 1}
library(glmmTMB)
fit_TMB_gender <- glmmTMB(shopping ~ gender + (1|individual) + (1|month), family = nbinom1(), data = Flatwork,
        ziformula = ~ 1)
```

<br>

**{glmmTMB}** is extremelly flexible package that is still relativelly young but that look very promising.

It allows for both zero-inflation and hurdle models (using truncated distributions), as well as many covariance functions, modeling the residual variance...

There is no full-fletched parametric bootstrap methods implemented as far as I know, nor conditional AIC, but this could change in the near future.

The package is also a wrapper around general numerical methods, so it may not be as robust as packages build with mixed models in mind (e.g. **{spaMM}** and **{lme4}**) in some cases (i.e., convergence issues could happen more often).


## Attempt 2: zero-augmented negative binomial

As for GLM, do not trust the tests in the summary table:

```{r TMB 2}
summary(fit_TMB_gender)
```


## Attempt 2: zero-inflated GLMM

**{glmmTMB}** is also compatible with **{DHARMa}**:

```{r fit_TMB_gender, fig.width = 9}
resid3 <- simulateResiduals(fit_TMB_gender)
plot(resid3)
```


## Attempt 2: zero-inflated GLMM

**{glmmTMB}** is also compatible with **{DHARMa}**:

```{r fit_TMB_gender 2, fig.width = 9}
testDispersion(resid3)
```


## Attempt 2: zero-inflated GLMM

**{glmmTMB}** is also compatible with **{DHARMa}**:

```{r fit_TMB_gender 3, fig.width = 9}
testZeroInflation(resid3)
```

This is better!


## Attempt 2: zero-augmented negative binomial

```{r TMB3}
fit_TMB_nogender <- glmmTMB(shopping ~ 1 + (1|individual) + (1|month), family = nbinom1(), data = Flatwork,
        ziformula = ~ 1)
anova(fit_TMB_gender, fit_TMB_nogender)
```


## Predictions for GLMM?

- building conditional predictions is similar to what we have seen in LMM: just use  `predict()` for that.

- building marginal predictions is more complex because you need to integrate the predictions over the distribution of the random effects... There is no function doing that automatically and this is a little too advanced for this course.



## What you need to remember

* what mixed models (LMM & GLMM) are
* how to use **{lme4}**, **{spaMM}** and **{glmmTMB}** to fit mixed models
* how to make predictions, tests, CI...
* how to check assumptions
* when to consider effects as fixed or random
* how to implement different random effects structure


# Table of contents

## Mixed-effects models

* 4.0 [Introduction to LMM & GLMM](./LMM_intro.html)
* 4.1 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.2 [A showcase of some useful applications](./LMM_showcase.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>

