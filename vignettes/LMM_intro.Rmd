---
title: "LMM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{4.0 Linear Mixed-effects Models}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(spaMM)
library(lme4)
library(car)
library(doSNOW)
spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LMM_intro/", fig.path = "./fig_knitr/LMM_intro/", fig.width = 5, fig.height = 5, fig.align = "center")
```


## Mixed-effects models

* 4.0 [Introduction to LMM & GLMM](./LMM_intro.html)
* 4.1 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.2 [A showcase of some useful applications](./LMM_showcase.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>


## You will learn in this session

* why you may need linear mixed-effects models
* what mixed models are
* how to use ```lme4``` and ```spaMM``` to fit mixed models
* how to fit a simple mixed models by hand in a few lines of code
* what BLUPs are
* how to make predictions at two different levels
* how to test effects using parametric bootstraps
* how to check assumptions for a linear mixed-effects model
* when to consider effects as fixed or random
* that REML fits are best to study variances
* how to implement different random effects structure
* what random slopes are

# Introduction


## Why Linear Mixed-effects Models?


### Goal:

To study, or to account for, unobservable sources of heterogeneity between observations.

<br>

Mixed-effects models allow for:

* the study of other questions than LM (e.g. heritability)
* the fixing of assumption violations in LM (lack of dependence, some cases of overdispersion)
* the reduction of the uncertainty in estimates and predictions in cases where many parameters would have to be estimated at the cost of an additional hypothesis (the distribution of the random effects)

<br>

The main sources of heterogeneity considered by mixed-effects models are:

* origin (in its widest sense)
* time
* space


## The linear mixed-effects model

<br>

### Definition

* a LMM is a specific linear model for which, given the design matrix $\mathbf{X}$, the responses ($\mathbf{Y}$) are no longer independent, but where the correlations can be described in terms of a random effect, i.e. a random variable that is not included in the predictor variables


## Mathematical notation of LM

LM, which we have seen before:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \epsilon$ </font></center>

<center><font size = 4>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\sigma^2 & 0 & 0 & \dots & 0 \\
0 & \sigma^2 & 0 & \dots & 0 \\
0 & 0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \sigma^2
\end{bmatrix}\right)
$$
</font></center>

## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ with, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
c_{1,1} & c_{1,2} & c_{1,3} & \dots & c_{1,q} \\
c_{2,1} & c_{2,2} & c_{2,3} & \dots & c_{2,q} \\
c_{3,1} & c_{3,2} & c_{3,3} & \dots & c_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_{q,1} & c_{q,2} & c_{q,3} & \dots & c_{q,q} \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>

<br>

with $\text{E}(b) = 0$, $\text{Cov}(b) = \mathbf{C}$, which is symmetrical ($c_{i, j} = c_{j, i}$). Also, $\text{Cov}(b, \epsilon) = 0$.


## Mathematical notation of LMM

LMM with one random factor with $q$ levels:

<center><font size = 6> $\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z}b + \epsilon$ </font></center>

<center><font size = 3>
$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p
\end{bmatrix}+
\begin{bmatrix}
z_{1,1} & z_{1,2} & z_{1,3} & \dots & z_{1,q} \\
z_{2,1} & z_{2,2} & z_{2,3} & \dots & z_{2,q} \\
z_{3,1} & z_{3,2} & z_{3,3} & \dots & z_{3,q} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
z_{n,1} & z_{n,2} & z_{n,3} & \dots & z_{n,q} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_q
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n
\end{bmatrix}
$$ and often, 
$$
b \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\lambda & 0 & 0 & \dots & 0 \\
0 & \lambda & 0 & \dots & 0 \\
0 & 0 & \lambda & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \lambda \\
\end{bmatrix}\right)
\text{&  }
\epsilon \sim
\mathcal{N}\left(0,
\begin{bmatrix}
\phi & 0 & 0 & \dots & 0 \\
0 & \phi & 0 & \dots & 0 \\
0 & 0 & \phi & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & \phi
\end{bmatrix}\right)
$$
</font></center>

<br>

Note: the dimensions of the matrices differ: $q \times q$ vs $n \times n$

# Fitting procedure

## A simple simulation function

```{r}
simulate_Mix <- function(intercept, slope, n, group_nb, var.rand, var.error){
  data <- data.frame(intercept = intercept, slope = slope, x = runif(n)) 
  group_compo <- rmultinom(n = 1, size = n, prob = c(rep(1/group_nb, group_nb)))
  data$group <- factor(rep(paste("group", 1:group_nb, sep = "_"), group_compo))
  data$b <- rep(rnorm(group_nb, mean = 0, sd = sqrt(var.rand)), group_compo)
  data$error <- rnorm(n, mean = 0, sd = sqrt(var.error))
  data$y <- data$intercept + data$slope*data$x + data$b + data$error
  return(data)
}

set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```


## Our toy dataset

```{r}
Aliens
```


## Fitting the model with ```lme4```

```{r}
library(lme4)
(fit_lme4 <- lmer(y ~ x + (1|group), data = Aliens, REML = FALSE))
c(var.group = as.numeric(attr(VarCorr(fit_lme4)$group, "stddev")^2),
  var.error = as.numeric(attr(VarCorr(fit_lme4), "sc")^2))  ## note: those are biased estimates (ML not REML)
```


## Fitting the model with ```spaMM```

```{r}
library(spaMM)
(fit_spaMM <- fitme(y ~ x + (1|group), data = Aliens, method = "ML"))
```


## Functions for fitting the mixed model numerically

```{r}
log_lik_b_prod <- function(param, data, scale = 1){
  log_lik_vec <- sapply(levels(data$group), function(level) {
    log(integrate(lik_b, -Inf, Inf, level = level, intercept = param[1], slope = param[2],
                  var.rand = param[3], var.error = param[4], data = data)$value)})
  return(scale * sum(log_lik_vec))
}
```

<br>

```{r}
lik_b <- function(b.vec, level, intercept, slope, var.rand, var.error, data, scale = 1){
  lik <- sapply(b.vec, function(b){
    sub_data <- data[which(data$group == level), ]
    sub_data$pred <- intercept + slope*sub_data$x + b
    sub_data$conditional.density <- dnorm(sub_data$y, mean = sub_data$pred, sd = sqrt(var.error))
    return(dnorm(b, mean = 0, sd = sqrt(var.rand)) * prod(sub_data$conditional.density))
  })
  return(scale * lik)
}
```


## Testing the functions


Let's test the function under fixed parameter values:
```{r}
log_lik_b_prod(param = c(50, 1.5, 2, 0.5), data = Aliens) ## test functions above
fit_constr <- fitme(y ~ 0 + (1|group) + offset(50 + 1.5*x), data = Aliens, fixed = list(lambda = 2, phi = 0.5))
logLik(fit_constr)
```


## Fitting the mixed model numerically

```{r}
bad_mod <- lm(y ~ x + group, data = Aliens)
```

```{r}
(init_values <- c(bad_mod$coefficients[1], bad_mod$coefficients[2],
                  var.group = var(bad_mod$coefficients[-c(1:2)]),
                  var.error = deviance(bad_mod) / bad_mod$df.residual))
```

```{r numerical fit}
system.time(
  opt <-  nloptr::nloptr(x0 = init_values, eval_f = log_lik_b_prod, data = Aliens, scale = -1,
                         lb = 0.5*init_values, ub = 2*init_values,
                         opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))
)
```


## Fitting the mixed model numerically

```{r}
estimates <- rbind(opt$solution, as.numeric(c(fit_spaMM$fixef, fit_spaMM$lambda, fit_spaMM$phi)))
colnames(estimates) <- c("intercept", "slope", "var.group", "var.error")
rownames(estimates) <- c("numeric", "spaMM")
estimates

c(logLik.num = -1 * opt$objective, logLik.spaMM = logLik(fit_spaMM)[[1]])
```


## The estimation of random effects

### We estimate the realized values of the random variable:

```{r fit b1}
ranef_alex <- sapply(levels(Aliens$group), function(group) {
  nloptr::nloptr(x0 = 0, lik_b, level = group, intercept = opt$solution[1],
                 slope = opt$solution[2], var.rand = opt$solution[3],
                 var.error = opt$solution[4], data = Aliens, scale = -1,
                 lb = -4*sqrt(opt$solution[3]), ub = 4*sqrt(opt$solution[3]),
                 opts = list(algorithm = "NLOPT_LN_BOBYQA", xtol_rel = 1.0e-4, maxeval = -1))$solution})

rbind(ranef_alex,
      ranef_lme4 = as.numeric(t(lme4::ranef(fit_lme4)[[1]])),
      ranef_spaMM = as.numeric(unlist(ranef(fit_spaMM))))
```


## Best Linear Unbiased Predictions

* The numbers we obtained with ```ranef``` are BLUPs of the random effects.
* Even if BLUPs can be estimated, they are not parameters of the statistical model!!!
* BLUPs can be computed using ML or REML (REML is best when BLUPs are not added to fitted values, unclear otherwise)

```{r, fig.height = 3.5, fig.width = 4}
curve(dnorm(x, mean = 0, sd = sqrt(estimates[1, "var.group"])), -5, 5, ylab = "density", xlab = "b")
points(dnorm(ranef_alex, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_alex, col = "blue", type = "h")
points(dnorm(ranef_alex, mean = 0, sd = sqrt(estimates[1, "var.group"])) ~ ranef_alex, col = "blue")
```


# A simple example of LMM

## The ```nlme::Oats``` dataset

```{r}
data("Oats", package = "nlme")
Oats2 <- as.data.frame(Oats)
Oats2$Block <- factor(Oats2$Block, ordered = FALSE, levels = sort(levels(Oats2$Block)))
head(Oats2)
str(Oats2)
```


## The ```nlme::Oats``` dataset

```{r}
coplot(yield ~ nitro | Variety + Block, data = Oats2, type = "b")
```

## LMM using ```lme4```

```{r}
(fit_lmm_lme4 <- lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE))
```


## LMM using ```lme4```

```{r}
head(fit_lmm_lme4@pp$X)  ## X
fit_lmm_lme4@beta  ## beta estimates
c(var.group = as.numeric(attr(VarCorr(fit_lmm_lme4)$Block, "stddev")^2),
  var.error = as.numeric(attr(VarCorr(fit_lmm_lme4), "sc")^2))  ## lambda and phi estimates
```


## LMM using ```lme4```

```{r}
head(t(fit_lmm_lme4@pp$Zt))  ## Z
as.matrix(fit_lmm_lme4@pp$Zt)
```


## LMM using ```lme4```

```{r}
head(t(fit_lmm_lme4@pp$Zt))  ## Z
crossprod(t(as.matrix(fit_lmm_lme4@pp$Zt)))
```


## LMM using ```lme4```

```{r}
lme4::ranef(fit_lmm_lme4)  ## b estimates
```


## LMM using ```spaMM```

```{r}
(fit_lmm_spaMM <- fitme(yield ~ nitro + Variety + (1|Block), data = Oats2))
```


## LMM using ```spaMM```

```{r}
head(fit_lmm_spaMM$X.pv)  ## X
fit_lmm_spaMM$fixef  ## beta estimates
as.numeric(c(fit_lmm_spaMM$lambda, fit_lmm_spaMM$phi))  ## lambda and phi estimates
```


## LMM using ```spaMM```

```{r}
head(fit_lmm_spaMM$ZAlist[[1]])  ## Z
crossprod(fit_lmm_spaMM$ZAlist[[1]])
```


## LMM using ```spaMM```

```{r}
ranef(fit_lmm_spaMM)  ## b estimates
```

# Predictions

## Predictions with LM

```{r}
fit_lm <- lm(yield ~ nitro + Variety + Block, data = Oats2)
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
(p <- predict(fit_lm, newdata = data.for.pred, interval = "confidence", se.fit = TRUE))
```

## Predictions with LM

```{r, fig.width = 4, fig.height = 4}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block)))) 
arrows(x0 = 1:length(levels(Oats2$Block)), y0 = p$fit[, "lwr"], y1 = p$fit[, "upr"],
       code = 3, angle = 90, length = 0.05)
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block)); axis(2, las = 1); box()
```

## Marginal predictions using ```lme4```

### Predictions averaged over the random variable:
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = "new")
p1 <- predict(fit_lmm_lme4, newdata = data.for.pred, re.form = NA)
X <- matrix(c(1, 0.3, 0, 1), nrow = 1)
se.fit <- sqrt(X %*% vcov(fit_lmm_lme4) %*% t(X))
se.rand <- attr(VarCorr(fit_lmm_lme4)$Block, "stddev")
(se.predVar <- as.numeric(sqrt(se.fit^2 + se.rand^2)))
lwr <- as.numeric(p1 + qnorm(0.025) * se.predVar)
upr <- as.numeric(p1 + qnorm(0.975) * se.predVar)
c(fit = as.numeric(p1), lwr = lwr, upr = upr)  ## Wald CI for prediction
```


## Marginal predictions using ```spaMM```

### Predictions averaged over the random variable:
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = "new")
p2 <- predict(fit_lmm_spaMM, newdata = data.for.pred, intervals = "predVar")
sqrt(attr(p2, "predVar"))  ## se.predVar
c(fit = p2, attr(p2, "intervals"))
```


## Conditional predictions using ```spaMM```

### Predictions conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):
```{r}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
p3 <- predict(fit_lmm_spaMM, newdata = data.for.pred, intervals = "predVar")
sqrt(attr(p3, "predVar"))  ## se.predVar
cbind(p3, attr(p3, "intervals"))
```


## Conditional predictions using ```lme4 + merTools```

### Predictions conditional on the random variable ($\mathbf{X}\widehat{\beta}+\mathbf{Z}\widehat{b}$):
```{r, message = FALSE}
data.for.pred <- expand.grid(nitro = 0.3, Variety = "Victory", Block = levels(Oats2$Block))
library(merTools)
(p4 <- predictInterval(fit_lmm_lme4, newdata = data.for.pred, include.resid.var = FALSE))
```

<br>

Note 1: ```lme4``` does not compute assymptotic confidence intervals, which is why we use ```merTools```.

Note 2: ```merTools``` uses simulation so results differ slightly each time you run the function.


## Comparing predictions from LM and LMM

```{r, fig.width = 5, fig.height = 5, echo = FALSE}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
abline(h = mean(tapply(Oats2$yield, Oats2$Block, mean)), lty = 2, lwd = 2, col = "blue")
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block))-0.1))
arrows(x0 = (1:length(levels(Oats2$Block))) - 0.1, y0 = p$fit[, "lwr"],
       y1 = p$fit[, "upr"], code = 3, angle = 90, length = 0.05)
points(p3 ~ I(1:length(levels(Oats2$Block)) + 0.1), col = "red")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.1, y0 = attr(p3, "intervals")[, 1],
       y1 = attr(p3, "intervals")[, 2], code = 3, angle = 90, length = 0.05, col = "red")
points(p4$fit ~ I(1:length(levels(Oats2$Block)) + 0.2), col = "orange")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.2, y0 = p4$upr,
       y1 = p4$lwr, code = 3, angle = 90, length = 0.05, col = "orange")
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block))
axis(2, las = 1)
box()
legend("top", fill = c("black", "red", "orange"), legend = c("fix", "spaMM", "merTools"),
       bty = "n", horiz = TRUE)
```

Note: in LMM predictions are attracted toward the mean.


## Comparing predictions from LM and LMM

```{r, eval = FALSE}
plot(Oats2$yield ~ unclass(Oats2$Block), axes = FALSE, ylab = "Yield", xlab = "Block",
     xlim = c(0.5, length(levels(Oats2$Block)) + 0.5), col = "blue", cex = 0.3)
abline(h = mean(tapply(Oats2$yield, Oats2$Block, mean)), lty = 2, lwd = 2, col = "blue")
points(p$fit[, "fit"] ~ I(1:length(levels(Oats2$Block))-0.1))
arrows(x0 = (1:length(levels(Oats2$Block))) - 0.1, y0 = p$fit[, "lwr"],
       y1 = p$fit[, "upr"], code = 3, angle = 90, length = 0.05)
points(p3 ~ I(1:length(levels(Oats2$Block)) + 0.1), col = "red")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.1, y0 = attr(p3, "intervals")[, 1],
       y1 = attr(p3, "intervals")[, 2], code = 3, angle = 90, length = 0.05, col = "red")
points(p4$fit ~ I(1:length(levels(Oats2$Block)) + 0.2), col = "orange")
arrows(x0 = (1:length(levels(Oats2$Block))) + 0.2, y0 = p4$upr,
       y1 = p4$lwr, code = 3, angle = 90, length = 0.05, col = "orange")
axis(1, at = 1:length(levels(Oats2$Block)), labels = levels(Oats2$Block))
axis(2, las = 1)
box()
legend("top", fill = c("black", "red", "orange"), legend = c("fix", "spaMM", "merTools"),
       bty = "n", horiz = TRUE)
```

# Tests

## Testing the effect of ```nitro```

### LM

```{r}
fit_lm_nonitro <- lm(yield ~ Variety + Block, data = Oats2)
anova(fit_lm, fit_lm_nonitro)
```


## Testing the effect of ```nitro```

### with ```lme4``` using an asymptotic LRT

```{r}
fit_lmm_lme4_nitro <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE)
fit_lmm_lme4_nonitro <-  lmer(yield ~ Variety + (1|Block), data = Oats2, REML = FALSE)
anova(fit_lmm_lme4_nitro, fit_lmm_lme4_nonitro)
```

<br>

Note: always use ML fits for testing fixed effects!


## Testing the reliability of the test of the LRT

```{r simu lme4, fig.height = 4, fig.width = 4, warning = FALSE}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$yield <- simulate(fit_lmm_lme4_nonitro)[, 1]
  fit_lmm_lme4_new <- lmer(yield ~ nitro + Variety + (1|Block), data = Oats3, REML = FALSE)
  fit_lmm_lme4_new_nonitro <-  lmer(yield ~ Variety + (1|Block), data = Oats3, REML = FALSE)
  anova(fit_lmm_lme4_new, fit_lmm_lme4_new_nonitro)$"Pr(>Chisq)"[2]})
plot(ecdf(pvalues)); abline(0, 1, col = 2)
```


## Testing the effect of the effect of ```nitro```

### with ```lme4``` + ```pbkrtest``` using parametric bootstrap

```{r yield lme4 param boot}
library(pbkrtest)
PBmodcomp(fit_lmm_lme4_nitro, fit_lmm_lme4_nonitro, nsim = 999)
```


## Testing the effect of the effect of ```nitro```

### with ```spaMM``` using an asymptotic LRT

```{r}
fit_lmm_spaMM_nitro <-  fitme(yield ~ nitro + Variety + (1|Block), data = Oats2, method = "ML")
fit_lmm_spaMM_nonitro <-  fitme(yield ~ Variety + (1|Block), data = Oats2, method = "ML")
anova(fit_lmm_spaMM_nitro, fit_lmm_spaMM_nonitro)
```


## Testing the effect of the effect of ```nitro```

### with ```spaMM``` using parametric bootstrap

```{r yield spaMM param boot, message = FALSE}
anova(fit_lmm_spaMM_nitro, fit_lmm_spaMM_nonitro, boot.repl = 100)
```


## The old-fashion ```aov()``` alternative?

```{r}
fit_aov <- aov(yield ~ nitro + Variety + Error(Block), data = Oats2)
coef(fit_aov)
```

The coefficient are the same (except the intercept) because the experimental design is well balanced.

Never use ```aov``` if this is not the case!


## The old-fashion ```aov()``` alternative?

```{r}
summary(fit_aov)
```

<br>

This test is "not particularly sensible statistically" (as the authors of ```?aov``` put it)...


## Testing the reliability of the test from ```aov()```

```{r simu aov, fig.height = 4, fig.width = 4}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$nitro <- runif(1:nrow(Oats3))  ## simulate H0 for nitro effect
  fit_aov_sim <- aov(yield ~ nitro + Variety + Error(Block), data = Oats3)
  summary(fit_aov_sim)[[2]][[1]][2, "Pr(>F)"]
})
plot(ecdf(pvalues))
abline(0, 1, col = 2)
```


## A summary table for mixed models?

Testing estimates of the different levels is not implemented in ```lme4``` or ```spaMM``` because the t-test have bad properties for LMM. Some alternatives have been proposed:

```{r, message = FALSE}
library(lmerTest)
fit_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats2, REML = FALSE)
summary(fit_lmm_lme4_nitro_bis)$coefficients
```

## Testing ```lme4 + lmerTest``` summary table
```{r simu lmerTest, fig.height = 4, fig.width = 4}
Oats3 <- Oats2
pvalues <- replicate(1000, {
  Oats3$nitro <- runif(1:nrow(Oats3))  ## simulate H0 for nitro effect
  fit_lmm_lme4_nitro_bis <-  lmer(yield ~ nitro + Variety + (1|Block), data = Oats3, REML = FALSE)
  summary(fit_lmm_lme4_nitro_bis)$coefficients[2, "Pr(>|t|)"]
})
plot(ecdf(pvalues))
abline(0, 1, col = 2)
```


# Information Criteria

## Information Criteria

```{r}
AIC(fit_lm)
AIC(fit_lmm_lme4)
print(AIC(fit_lmm_spaMM))
```

* rely on the marginal AIC if you are interested in marginal predictions.
* rely on the conditional AIC if you are interested in conditional predictions.


# Assumptions

## What are the assumptions in LMM?

* same as LM (except for independence)
* given the fixed effects and the realized values of the random effects, the residuals must be independent 
* the random effects must follow the assumed distribution
* the levels of the factorial variable for which random effects are estimated must be representative from the whole population

<br>

Note:

* companion packages exist for checking outliers on ```lme4``` fits: ```HLMdiag``` and ```influence.ME```
* the package ```DHARMa``` works for ```lme4```!
* for now ```spaMM``` is quite limited in terms of tools to check assumptions


## Plotting residuals

```{r}
plot(fit_lmm_lme4, type = c("p", "smooth"))  ## see ?lme4:::plot.merMod for details
```


## Plotting residuals

```{r}
plot(fit_lmm_lme4, resid(., scaled=TRUE) ~ fitted(.) | Block, abline = 0)
```


## Plotting residuals

```{r}
lattice::qqmath(fit_lmm_lme4, id = 0.05) ## id allows to see outliers
```


## Plotting BLUPs

```{r, fig.width = 4, fig.height = 4}
lattice::qqmath(lme4::ranef(fit_lmm_lme4, condVar = TRUE))
```


## Using simulated residuals

```{r, fig.width = 8, fig.height = 4}
library(DHARMa)
r <- simulateResiduals(fit_lmm_lme4, n = 1000)
plot(r)
```


## Using simulated residuals

### How to deal with random effects matters!
```{r}
r_unconditional <- simulateResiduals(fit_lmm_lme4, n = 1000)  ## resimulate BLUPs
testTemporalAutocorrelation(r_unconditional, time = 1:nrow(Oats2), plot = FALSE)
r_conditional <- simulateResiduals(fit_lmm_lme4, re.form = NULL, n = 1000)  ## conditional to fitted BLUPs
testTemporalAutocorrelation(r_conditional, time = 1:nrow(Oats2), plot = FALSE)
```

# To mix or not to mix?

## Choosing between fixed and random effects

The choice between considering a predictor has having fixed or random effects can be difficult; it depends on the trade-off between the pros and cons of both approaches.

### Fixed effects

* no assumption about the distribution of the effects associated with each level of a predictor
* requires many new datapoints for each additional levels to get reliable results
* allows for the prediction of the effect of observed levels only (for factors)
* simple to study

### Random effects

* the effects associated with each level of a predictor follow a given probability distribution
* requires at least one new datapoint for each additional levels to get reliable results (more is better)
* requires many levels for the variance estimates to be reliable (5 being strict minimum)
* allow for the prediction of the effect of both observed and unobserved levels (for factors)
* more difficult to study

<!--
# Studying variation using LMM

## Estimating a variance

### Let's simulate a dataset under the assumptions of LMM

```{r}
set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```


## Estimating a variance

* Estimating variance components and estimating BLUPS are the only situation in which parameters must be fitted to the data by REstricted (or REsidual) Maximum Likelihood instead of Maximum Likelihood.
* A ML fit would lead to underestimate the variances.

<br>

Note:

* different packages and different functions within the same package may have ML or REML as a default fitting method, so always double check!
* unlike ML, REML is sensitive to changes in contrasts.


## Estimating a variance

### Model fit with ```lmer``` (```REML = TRUE``` by default)

```{r, message = FALSE}
library(lme4)
(mod <- lmer(y ~ x + (1|group), data = Aliens))
```


## Estimating a variance

### Model fit with ```fitme```

```{r, message = FALSE}
library(spaMM)
(mod2 <- fitme(y ~ x + (1|group), data = Aliens, method = "REML"))
```


## Distribution of the variance estimate

```{r distrib lambda large, warning = FALSE}
lambdas_large <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 10, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda, warning = FALSE}
lambdas <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda small, warning = FALSE}
lambdas_small <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 10
hist(lambdas_large[lambdas_large < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 30, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 2
hist(lambdas[lambdas < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 7, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group2 <- 0.1
hist(lambdas_small[lambdas_small < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group2)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 1, add = TRUE, lwd = 2, col = "red")
```


## Testing the variance

### Model fit with ```fitme```

```{r}
mod2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML")
mod2_H0_REML <- fitme(y ~ x, data = Aliens, method = "REML")
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]), df = 1, lower.tail = FALSE)
mod2_H2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML", fixed = list(lambda = 2))
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H2_REML)[[1]]), df = 1, lower.tail = FALSE)
```

Note 1: this asymptotic test is poor when the variance is low.

Note 2: ```spaMM``` allows for testing difference with a specific value.

Note 3: ```anova()``` from ```spaMM``` will not run in this case. 

## Testing the variance

### Model fit with ```lmer```

```{r}
fit_REML <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
fit_H0 <- lm(y ~ x, data = Aliens)
pchisq(2*(logLik(fit_REML)[[1]] - logLik(fit_H0)[[1]]), df = 1, lower.tail = FALSE)
anova(fit_REML, fit_H0)
```

Note: ```lme4``` does not allow for testing difference with a specific value.


## Reliability of the test

```{r test spaMM, warning=FALSE}
test <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 2))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability of the test

```{r}
plot(ecdf(test), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```


## Reliability 2 (small variance)

```{r test spaMM 2, warning=FALSE}
test2 <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 0.1))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability 2 (small variance)

```{r, fig.width=4, fig.height=4}
plot(ecdf(test2), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```

Likelihood ratio tests never work well close to parameter boundaries... (better use parametric bootstrap!)


## Testing variance using parametric bootsrap

### Based on the models fitted with ```lme4```

```{r param boot lme4 by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(fit_REML)[[1]] - logLik(fit_H0)[[1]]))
LRTH0 <- replicate(200, {
                    Aliens$y <- simulate(fit_H0)[, 1]
                    2*(logLik(lmer(y ~ x + (1|group), data = Aliens, REML = TRUE))[[1]] -
                      logLik(lm(y ~ x, data = Aliens))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

Note using update creates problems here...


## Testing variance using parametric bootsrap

### Based on the models fitted with ```spaMM```

```{r param boot spaMM by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]))
LRTH0_bis <- replicate(200, {
                    Aliens$y <- simulate(mod2_H0_REML)
                    2*(logLik(fitme(y ~ x + (1|group), data = Aliens, method = "REML"))[[1]] -
                      logLik(fitme(y ~ x, data = Aliens, method = "REML"))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

## Confidence interval for the variance with ```lme4```

```{r CI lambda lme4}
fit_lmer <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
round(confint(fit_lmer, method = "profile")[1, ]^2, 2)  ## more at ?lme4:::confint.merMod
round(confint(fit_lmer, method = "boot", nsim = 1000)[1, ]^2, 2)
```

<br>

Note: there is not yet an alternative for ```spaMM``` but you can use ```boot``` as we did for LM!


## Confidence interval for the variance using ```spaMM + boot```

```{r spaMM and boot, message = FALSE, warnings = FALSE}
library(boot)
n_boot <- 1000
newYs <- simulate(mod2_REML, type = "marginal", nsim = n_boot)
res_sim <- sapply(1:n_boot, function(i) {
  Aliens$y <- newYs[, i]
  log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]])}) ## lambda on new data
exp(boot.ci(boot.out = list(R = n_boot),
        t0 = log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]]), ## original lambda
        t  = matrix(res_sim, ncol = 1), type = "basic")$basic[, 4:5])
```
-->


# Specifying multiple random effects

## The ```lme4::Penicillin``` dataset

```{r}
str(Penicillin)
table(Penicillin$sample, Penicillin$plate)
```

## The ```lme4::Penicillin``` dataset

### The random effects are "crossed"

```{r}
mod <- fitme(diameter ~ 1 + (1|plate) + (1|sample), data = Penicillin)
mod$lambda
```
<div class="columns-2">

```{r}
head(mod$ZAlist[[1]])
```

```{r}
head(mod$ZAlist[[2]], 10)
```
</div>


## The ```lme4::cake``` dataset

```{r}
head(cake)
str(cake)
```


## The ```lme4::cake``` dataset

```{r}
table(cake$recipe, cake$replicate, cake$temperature)
```


## The ```lme4::cake``` dataset

### The random effect is nested within a fixed effect:

```{r}
mod <- fitme(angle ~ recipe + temperature + (1|recipe:replicate), data = cake)
mod$lambda
```


## The ```lme4::cake``` dataset

### The random effect is nested within a fixed effect (alternative):

```{r}
cake$replicate_tot <- factor(paste(cake$recipe, cake$replicate, sep = "_"))
levels(cake$replicate_tot)
mod <- fitme(angle ~ recipe + temperature + (1|replicate_tot), data = cake)
mod$lambda
```


## The ``` carnivora``` dataset

```{r}
data("carnivora", package = "ape")
carnivora$log_brain <- log(carnivora$SB)
carnivora$log_body <- log(carnivora$SW)
str(carnivora)
```


## The ``` carnivora``` dataset

```{r}
tapply(carnivora$Genus, carnivora$Family, function(x) length(unique(x)))
```


## The ``` carnivora``` dataset

```{r}
coplot(log_brain ~ log_body | Family, data = carnivora)
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1 <- fitme(log_brain ~ log_body + (1|Family/Genus), data = carnivora, method = "REML")
mod1
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1bis <- fitme(log_brain ~ log_body + (1|Family) + (1|Family:Genus), data = carnivora, method = "REML")
mod1bis
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1ter <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
mod1ter
```


## The ``` ape::carnivora``` dataset

### Two nested random effects:

<br>

* the formula ```(1|Family/Genus)```
* the formula ```(1|Family) + (1|Family:Genus)```
* the formula ```(1|Family) + (1|Genus)```

are the same as long as genus are not being recycled between families!


## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(mod1$ZAlist[[1]]))
```

<!--
## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(mod1$ZAlist[[2]]))
```


## Checking the random structure

### You can also use the ```model.matrix``` clone from ```lme4```:

```{r}
lF <- lFormula(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora)
lF$reTrms$flist  ## list of grouping factors used in the random-effects terms; see ?mkReTrms
```
-->

## Checking the random structure

Checking the names of the BLUPs structure is an easy solution to check that you did specify the random structure correctly:

```{r}
lapply(ranef(mod1), function(x) head(names(x)))  ## or just ranef(mod1)
```

<!--
## Estimating the variances of two subgroups

### Two variances between genus

```{r}
carnivora$Canidae  <- as.numeric(carnivora$Family == "Canidae")
carnivora$Others   <- as.numeric(carnivora$Family != "Canidae")

mod2 <- fitme(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora, method = "REML")
```

<br>

Note: it does not seem to work with more than 2 variances, which I don't understand...


## Estimating the variances of two subgroups

```{r}
mod2
```


## Estimating the variances of two subgroups

```{r}
as.data.frame(ranef(mod2))
```


## Estimating the variances of two subgroups

### Same using ```lme4```

```{r}
mod2_lme4 <- lmer(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora)
lapply(VarCorr(mod2_lme4), function(r) attr(r, "stddev")^2)
head(ranef(mod2_lme4))
```
-->

# Random slopes


## Fitting a random slope model

```{r}
(mod3 <- fitme(log_brain ~ log_body + (log_body|Family) + (1|Genus), data = carnivora, method = "REML"))
```


## The BLUPs for the slopes

```{r}
ranef(mod3)$`( log_body | Family )`
```


## Predictions

```{r, fig.width = 3, fig.height = 3}
plot(log_brain ~ log_body, data = subset(carnivora, Family == "Felidae"), col = "red",
     ylim = range(carnivora$log_brain), xlim = range(carnivora$log_body))
points(log_brain ~ log_body, data = subset(carnivora, Family == "Mustelidae"), col = "blue")
points(log_brain ~ log_body, data = subset(carnivora, Family == "Viverridae"), col = "orange")
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Felidae", ], col = "red", lwd = 2, lty = 2)
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Mustelidae", ], col = "blue", lwd = 2, lty = 2)
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Viverridae", ], col = "orange", lwd = 2, lty = 2)
```
Note: you can also use `predict()` to do the same!


## Testing if slopes differ between families

### ```lme4```
```{r}
mod4 <- lmer(log_brain ~ log_body + (log_body|Family) + (1|Genus), data = carnivora, REML = TRUE)
mod4noRS <- lmer(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, REML = TRUE)
anova(mod4, mod4noRS) ## count 2 DF as covariance is included... correct?
```


## Testing if slopes differ between families

### ```spaMM```
```{r}
mod3noRS <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
pchisq(2*(logLik(mod3) - logLik(mod3noRS)), df = 2, lower.tail = FALSE) ## count 2 DF to include covariance as lme4
```

<!--
## Testing if slopes differ between families

### LM
```{r}
mod5 <- lm(log_brain ~ log_body * Family + Genus, data = carnivora)
mod5noIS <- lm(log_brain ~ log_body + Family + Genus, data = carnivora)
anova(mod5, mod5noIS)
```


## Fitting uncorrelated random intercept and slope

```{r}
(mod3_alt <- fitme(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora))
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt <- lmer(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt), function(r) attr(r, "stddev")^2)
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt_bis <- lmer(log_brain ~ log_body + (log_body||Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt_bis), function(r) attr(r, "stddev")^2)
```

<br>

Note 1: the syntax ```||``` does not work in ```spaMM``` but it is just a shortcut, so it is not really needed.

Note 2: unless you have a very good reason not to, you should consider the correlations between random effects!
-->


# GLMM

### GLM + LMM = GLMM

$$\begin{array}{lcl}
\mu &=& g^{-1}(\eta)\\
\mu &=& g^{-1}(\mathbf{X}\beta + \mathbf{Z}b)\\
\end{array}
$$

with (as for GLM):

* $\text{E}(\text{Y}) = \mu = g^{-1}(\eta)$
* $\text{Var}(\text{Y}) = \phi\text{V}(\mu)$ 

<br>

Note:

* If $g^{-1}$ is the identity function, $\phi = \sigma^2$ and $\text{V}(\mu) = 1$, we have the LMM.
* If $\mathbf{Z}b = 0$, we have the GLM.
* If $g^{-1}$ is the identity function, $\phi = \sigma^2$, $\text{V}(\mu) = 1$, and $\mathbf{Z}b = 0$, we have the LM.


## The ```LM2GLMM::Flatwork``` dataset

```{r Flatwork}
Flatwork
```


## The ```LM2GLMM::Flatwork``` dataset

```{r Flatwork str}
str(Flatwork)
```


## GLMM with ```lme4```

```{r Flatwork lme4}
(fit_glmm_lme4 <- glmer(shopping ~ gender + (1|individual) + (1|month), family = poisson(),
                        data = Flatwork))
```


## GLMM with ```spaMM```

```{r Flatwork spaMM 2}
(fit_glmm_spaMM <- fitme(shopping ~ gender + (1|individual) + (1|month), family = poisson(),
                        data = Flatwork, method = "ML"))
```


## Checking residuals

```{r Flatwork res, fig.width = 9}
library(DHARMa)
r <- simulateResiduals(fit_glmm_lme4)
plot(r)
```


## Extra 0s?

```{r Flatwork shopping}
barplot(table(Flatwork$shopping))
```


## Extra 0s?

```{r Flatwork zeros}
testZeroInflation(r)
```


## Binomial model (to avoid the problem)

```{r Flatwork binom}
Flatwork$shopping_bin <- Flatwork$shopping > 0
(fit_glmm_lme4bin <- glmer(shopping_bin ~ gender + (1|individual) + (1|month), family = binomial(),
                        data = Flatwork))
```


## Checking residuals

```{r Flatwork res 2, fig.width = 9}
r_bin <- simulateResiduals(fit_glmm_lme4bin)
plot(r_bin)
```


## Testing the gender effect

```{r Flatwork gender}
fit_glmm_lme4bin0 <- glmer(shopping_bin ~ 1 + (1|individual) + (1|month), family = binomial(),
                        data = Flatwork)

anova(fit_glmm_lme4bin, fit_glmm_lme4bin0)
```


## Same with ```spaMM```

```{r Flatwork spaMM}
fit_glmm_spaMMbin <- fitme(shopping_bin ~ gender + (1|individual) + (1|month), family = binomial(),
                        data = Flatwork)

fit_glmm_spaMMbin0 <- fitme(shopping_bin ~ 1 + (1|individual) + (1|month), family = binomial(),
                        data = Flatwork)

anova(fit_glmm_spaMMbin, fit_glmm_spaMMbin0)
```


## Is there an effect for the non-zeros?

```{r Flatwork shopping_bin}
Flatwork_pos <- subset(Flatwork, Flatwork$shopping_bin)
barplot(table(Flatwork_pos$shopping))
```


## Fitting models on truncated distributions

```{r truncated fit}
fit_glmm_spaMM_truncP <- fitme(shopping ~ gender + (1|individual) + (1|month), family = Tpoisson(),
                               data = Flatwork_pos)
fit_glmm_spaMM_truncNB <- fitme(shopping ~ gender + (1|individual) + (1|month), family = Tnegbin(),
                                data = Flatwork_pos)
print(AIC(fit_glmm_spaMM_truncP))
print(AIC(fit_glmm_spaMM_truncNB))
```

<br>

Note: this only work with the development version of ```spaMM``` at the moment...


## A better solution?

```{r TMB}
library(glmmTMB)
fit_TMB_gender <- glmmTMB(shopping ~ gender + (1|individual) + (1|month), family = poisson(), data = Flatwork,
        ziformula = ~ 1)
fit_TMB_nogender <- glmmTMB(shopping ~ 1 + (1|individual) + (1|month), family = poisson(), data = Flatwork,
        ziformula = ~ 1)
anova(fit_TMB_gender, fit_TMB_nogender)
```

<br>

```glmmTMB``` is extremelly flexible. It allows for both Zero Inflation and Hurdle models (using truncated distributions), as well as modeling the residual variance!

## Checking model assumption

```{r fit_TMB_gender, fig.height = 3, fig.width = 5}
plot(simulateResiduals(fit_TMB_gender))
```

Note: the implementation of ```DHARMa``` for ```glmmTMB``` is currently in progress!


## What you need to remember

* why you may need linear mixed-effects models
* what mixed models are
* how to use ```lme4``` and/or ```spaMM``` to fit mixed models
* what BLUPs are
* how to make predictions at two different levels
* how to test effects using parametric bootstraps
* how to check assumptions for a linear mixed-effects model
* when to consider effects as fixed or random
* that REML fits are best to study variances
* how to implement different random effects structure
* what random slopes are


# Table of contents

## Mixed-effects models

* 4.0 [Introduction to LMM & GLMM](./LMM_intro.html)
* 4.1 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.2 [A showcase of some useful applications](./LMM_showcase.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>

