---
title: "LM: Assumptions"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.4 Assumptions}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(lmtest)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LM_assumptions/", fig.path = "./fig_knitr/LM_assumptions/", fig.align = "center", fig.width = 4, fig.height = 4)
options(width = 200)
```

## You will learn in this session

# Introduction

## The main assumptions

### Model structure

* linearity
* lack of multicollinearity (design matrix of full rank)
* predictor variables have fixed values

### Errors

* independence (no serial autocorrelation)
* constant variance (homoscedasticity)
* normality

# Assumptions about the model structure

# Linearity

## Linearity in brief

### Assumption

The mean of the response variable must be a linear combination of the parameters and the predictor variables, with no systematic dependence on any omitted terms.


### Causes and consequences of violation

Departure from linearity can originate from a multitude of reasons and can create all kinds of problems.

### Solution

* transform one or several predictors (e.g. polynomials)
* transform the response (e.g. log and power transformation)

### Alternative

* non-linear models


## Quiz

### Can you express the following models as LM?

* <font size = 8>$y_i = \alpha + \epsilon_i$ </font>
* <font size = 8>$y_i = x_i^\beta + \epsilon_i$ </font>
* <font size = 8>$y_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$ </font>
* <font size = 8>$y_i = \frac{\beta x_i}{\alpha + x_i} + \epsilon_i$ </font> (Michaelis-Menten : V = Vmax[S]/(Km+[S]))

## Lineweaver Burk method for Michaelis-Menten

<center> ![](./LineweaverBurke.png) </center>

<br>

But using this method is not advised as results can be unreliable...

## A simple example of non-linearity

```{r simple non linear}
Alien0 <- data.frame(humans_eaten = sample(1:100)) 
Alien0$size <- 10 + 50 * Alien0$humans_eaten - 0.02 * (Alien0$humans_eaten^2) + rnorm(100, sd = 5)
mod0a <- lm(size ~ humans_eaten, data = Alien0)
coef(mod0a)
plot(size ~ humans_eaten, data = Alien0)
```

## A simple example of non-linearity

Solution for continuous design matrix column vectors: plot the residuals against them!
```{r simple non linear2}
plot(rstandard(mod0a) ~ model.matrix(mod0a)[,2])
```

## A simple example of non-linearity

```{r simple non linear3}
mod0b <- lm(size ~ poly(humans_eaten, 2, raw = TRUE), data = Alien0)
summary(mod0b)$coef
```

## Another example of non-linearity

```{r poison}
data(poison, package = "fastR")  ##  ?fastR::poison for description
poison$treat <- factor(poison$Treatment)
poison$poison <- factor(poison$Poison)
mod_poison <- lm(Time ~ poison + treat, data = poison)
plot(residuals(mod_poison) ~ fitted(mod_poison), xlab = "fitted values", ylab = "residuals")
abline(h = 0, col = "red", lty = 2)
```

## The Box-Cox transformation (Box & Cox 1964)

<center> ![](./BoxCox.png) </center>

## The Box-Cox transformation (Box & Cox 1964)

### It encompasses several classic transformations:

* log transformation ($\lambda = 0$)
* inverse transformation ($\lambda = -1$)
* square root transformation ($\lambda = 1/2$)
* square transformation ($\lambda = 2$)

<br>

(but it changes intercept and rescale the $\beta$)


## The Box-Cox transformation (Box & Cox 1964)

```{r boxcox}
library(MASS)
bc <- boxcox(mod_poison)  ## makes logLik profile!
```

We will consider -1 instead of ```r round(bc$x[which.max(bc$y)], 2)``` as it is close enough and easier to interpret!

## Poison example linearised

```{r poison bc}
mod_poison2 <- update(mod_poison, (Time^-1 - 1)/(-1) ~ .)
plot(residuals(mod_poison2) ~ fitted(mod_poison2), xlab = "fitted values", ylab = "residuals")
abline(h = 0, col = "red", lty = 2)
```

## Poison example linearised

### Predictions
```{r poison pred}
data.for.pred <- expand.grid(treat = levels(poison$treat), poison = levels(poison$poison))
(pred <- cbind(data.for.pred, predict(mod_poison2, newdata = data.for.pred, interval = "confidence")))
```

There are in the Box-Cox scales. In this case, it represents the survival rate, but you can always get back to the original scale:

```{r pred unbc}
lambda <- -1; (pred$fit * lambda + 1)^(1/lambda)
```

## Comparison of tests

```{r poison summary}
summary(mod_poison) ## the original model
```

## Comparison of tests

```{r poison bc summary}
summary(mod_poison2) ## the boxcoxed model
```

# Lack of multicollinearity

## Lack of multicollinearity in brief

### Assumption

The design matrix must have full rank. That means that the number of parameters to be estimated must be equal to the rank of the design matrix.

### Causes and consequences of violation

Caused by having less data than parameters or when there is linear dependence between the column vectors of the design matrix. In such case, some parameters cannot be computed.

### Solution

* change design matrix (change parameterization or drop redundant effects)
* change the experimental design
* collect more data

### Alternative

* none

## Degenerated design matrix: ```n < p```
```{r degenerated n too small, error = TRUE}
set.seed(1L)
N <- 3
Alien <- data.frame(humans_eaten = 1:N,
                     flowers_eaten = round(runif(N, min = 1, max = 15)),
                     cactus_eaten =  round(runif(N, min = 1, max = 10)))

Alien$size <- rnorm(n = nrow(Alien),
  mean = 50 + 0.2 * Alien$humans_eaten + 0.9 * Alien$flowers_eaten + 0.1 * Alien$cactus_eaten,
  sd = sqrt(25))

mod_alien1a <- lm(size ~  cactus_eaten + humans_eaten + flowers_eaten, data = Alien)
coef(mod_alien1a)
mod_alien1b <- lm(size ~  humans_eaten + flowers_eaten + cactus_eaten, data = Alien)
coef(mod_alien1b)
```


## Degenerated design matrix: trivial redundancy

```{r degenerated redundancy, error = TRUE}
set.seed(1L)
Alien2 <- simulate_Aliens()
Alien2$half_humans_eaten <-  0.5 * Alien2$humans_eaten
mod_alien2 <- lm(size ~ humans_eaten + half_humans_eaten, data = Alien2)
coef(mod_alien2)
det(crossprod(model.matrix(mod_alien2)))  ## when det(XTX) <= 0, XTX has no inverse!
mod_alien2$rank  == ncol(model.matrix(mod_alien2))
```

## Degenerated design matrix: miscellaneous

```{r degenerated redundancy subtle, error = TRUE}
set.seed(1L)
Alien3 <- data.frame(humans_eaten = 1:12,
                     flowers_eaten = round(runif(12, min = 1, max = 15)),
                     cactus_eaten = 0)
Alien3$food_units <- 1.2*Alien3$humans_eaten + 0.6*Alien3$flowers_eaten
Alien3$size <- rnorm(n = 12, mean = 50 + 1*Alien3$food_units, sd = sqrt(25))
mod_alien3 <- lm(size ~ food_units + humans_eaten + flowers_eaten + cactus_eaten, data = Alien3)
coef(mod_alien3)
caret::findLinearCombos(model.matrix(mod_alien3))  ## Tip: help to see what creates the issue
```



## Chalenge: analysing the Fungi dataset

```{r fungi}
head(Fungi)
```

<br>

### Goal:

quantifying the average growth rate of alive fungi for each species,
in each experimental condition, using a single linear model.

([solution](./Fungi_1.html))

# Predictor variables have fixed values

## Predictor variables have fixed values (in brief)

### Assumption

The dependent variable are represented by fixed values.

### Causes and consequences of violation

The presence of measurement errors is the main cause of violation. Violation can trigger both estimates and tests to be biased.

### Solution

* often ignored in practice
* better measurements

### Alternative

* multipurpose numerical approaches
* errors-in-variables models
* reduced major axis regression


## Example

```{r measurement error}
Alien4 <- simulate_Aliens(100)
summary(lm(size ~ humans_eaten, data = Alien4))$coef
Alien4$humans_eaten_err <- Alien4$humans_eaten + rnorm(nrow(Alien4), sd = 10)
summary(lm(size ~ humans_eaten_err, data = Alien4))$coef
```

## Accounting for errors-in-variables using ```sem```

### Seems to work here... (despite strange value for phi)

```{r sem}
library(sem)
eqns <- specifyEquations(text = "
                        size = alpha*Intercept + slope*humans_eaten
                        humans_eaten = 1*humans_eaten_err
                        V(size) = sigma
                        V(humans_eaten) = 1
                        V(humans_eaten_err) = phi
                        ")
fitted.mod <- sem(eqns, data = Alien4, raw = TRUE, fixed.x = "Intercept")
summary(fitted.mod, analytic.se = FALSE)$coef  ## use analytic.se = FALSE (uses z as consider variance known)
```

# Assumptions about the errors

# Independence

## Independence in brief

### Assumption

The errors (not the residuals) are uncorrelated: $\text{cov}(\epsilon_i, \epsilon_j) = 0$, with $i \neq j$.

### Causes and consequences of violation

A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). Lack of independence increases the risk of false positive (sometimes a lot).


### Solution

* transformation (see linearity)
* aggregation (be carefull)
* sub-sampling

### Alternative

* mixed models (LMM and GLMM)


## Testing for independence

We can use the Durbin-Watson test: D-W [0; 4]

* D-W = 2 no-autocorrelation
* D-W <<2 positive autocorrelation
* D-W >>2 negative autocorrelation

```{r DW}
library(car)
set.seed(1L)
durbinWatsonTest(modConv <- lm(fconvict ~ tfr + partic + degrees + mconvict, data = Hartnagel), max.lag = 3)
```

## Testing for independence induced by a specific variable

```{r DW 2, message = FALSE}
library(lmtest)
dwtest(modConv, order.by = modConv$model$degrees)
```

## Testing for independence by eye

### It is difficult when the problem is not extreme

```{r DW by eye}
plot(residuals(modConv) ~ fitted(modConv))
abline(h = 0, lty = 2, col = "red")
```

## Testing for independence by eye

### The origin of the problem is here the time!

```{r DW by eye 2}
plot(residuals(modConv) ~ Hartnagel$year, type = "o")
abline(h = 0, lty = 2, col = "red")
```

# Constant variance (homoscedasticity)

## Homoscedasticity in brief

### Assumption

The variance of the error (not residuals) is constant: $\text{var}(\epsilon_j) = \sigma^2$ for all $j$.

With matrix notation: if $\epsilon^\text{T}$ is the vector of all $\epsilon_j$, then we assume $\text{cov}(\epsilon, \epsilon) = \text{E}(\epsilon \epsilon^\text{T}) = \sigma^2I_n$, where $I_n$ is the $n \times n$ identity matrix. 

### Causes and consequences of violation

It can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. If can create both false positives and false negative.

### Solution

* post-hoc correction of the SE

### Alternative

* Generalized Linear Models (GLM)


## Example of heteroscedasticity

```{r rpois}
set.seed(1L)
Alien5 <- simulate_Aliens(N = 100)
Alien5$eggs <- rpois(100, lambda = 20 + 5 * Alien5$humans_eaten)
mod_alien5 <- lm(eggs ~ humans_eaten, data = Alien5)
summary(mod_alien5)$coef
bptest(mod_alien5)
```

# Outliers

## Detecting outliers

```{r}
influence.measures(mod_poison)
```


## What you need to remember

# Table of content

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Point estimates](./LM_point_estimates.html)
* 2.2 [Uncertainty in point estimates](./LM_uncertainty.html)
* 2.3 [Tests](./LM_tests.html)
* 2.4 [Assumptions](./LM_assumptions.html)
