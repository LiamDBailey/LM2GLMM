---
title: "LM: Assumptions and Outliers"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.3 Assumptions and Outliers}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(lmtest)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LM_assumptions/", fig.path = "./fig_knitr/LM_assumptions/", fig.align = "center", fig.width = 4, fig.height = 4)
options(width = 200)
set.seed(1L)
```

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Fitting procedure](./LM_fitting.html)
* 2.2 [Tests & Intervals](./LM_test_intervals.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions.html)
* 2.4 [Let's practice](./LM_practice.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>



## You will learn in this session

* the six main assumptions behind LM
* how to check if these assumptions are met (by means of tests and using your eyes)
* how to do the Box-Cox transformation
* that you can manipulate the design matrix to solve problems
* how to study carefully outliers
* that you will have to learn GLM and (G)LMM to solve some usual problems


# Introduction

## The main assumptions

### Model structure

* linearity
* lack of perfect multicollinearity (design matrix of full rank)
* predictor variables have fixed values

### Errors

* independence (no serial autocorrelation)
* constant variance (homoscedasticity)
* normality

# Assumptions about the model structure

# Linearity

## Linearity in brief

### Assumption

The mean of the response variable must be a linear combination of the parameters and the predictor variables, with no systematic dependence on any omitted terms.


### Causes and consequences of violation

Departure from linearity can originate from a multitude of reasons and can create all kinds of problems.

### Solution

* transform one or several predictors (e.g. polynomials)
* transform the response (e.g. log and power transformation)

### Alternative

* non-linear models


## Quiz

### Can you express the following models as LM?

* <font size = 8>$y_i = \alpha + \epsilon_i$ </font>
* <font size = 8>$y_i = x_i^\beta + \epsilon_i$ </font>
* <font size = 8>$y_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i$ </font>
* <font size = 8>$y_i = \frac{\beta x_i}{\alpha + x_i} + \epsilon_i$ </font> (Michaelis-Menten : V = Vmax[S]/(Km+[S]))

## Lineweaver Burk method for Michaelis-Menten

<center> ![](./LineweaverBurke.png) </center>

<br>

But using this method is not advised as results can be unreliable...

## A simple example of non-linearity

```{r simple non linear}
set.seed(1L)
Alien0 <- data.frame(humans_eaten = sample(1:100)) 
Alien0$size <- 10 + 50 * Alien0$humans_eaten - 0.02 * (Alien0$humans_eaten^2) + rnorm(100, sd = 5)
mod0a <- lm(size ~ humans_eaten, data = Alien0)
coef(mod0a)
```

## A simple example of non-linearity

```{r simple non linear plot}
plot(size ~ humans_eaten, data = Alien0, pch = 3, col = "red")
abline(mod0a, col = "blue", lwd = 2)
```


## A simple example of non-linearity

Solution for continuous design matrix column vectors: plot the residuals against them!
```{r simple non linear2}
plot(residuals(mod0a) ~ model.matrix(mod0a)[,2])
abline(h = 0, col = "red", lty = 2)
```


## A simple example of non-linearity

```{r simple non linear3}
mod0b <- lm(size ~ poly(humans_eaten, 2, raw = FALSE), data = Alien0)
summary(mod0b)$coef
```

```{r simple non linear3 raw}
mod0b <- lm(size ~ poly(humans_eaten, 2, raw = TRUE), data = Alien0)
summary(mod0b)$coef
```


## Another example of non-linearity

```{r poison}
poison$treat <- factor(poison$Treatment)
poison$poison <- factor(poison$Poison)
fit_poison <- lm(Time ~ poison + treat, data = poison)
plot(residuals(fit_poison) ~ fitted(fit_poison), xlab = "fitted values", ylab = "residuals")
abline(h = 0, col = "red", lty = 2)
```


## The Box-Cox transformation (Box & Cox 1964)

<center> ![](./BoxCox.png) </center>

## The Box-Cox transformation (Box & Cox 1964)

### It encompasses several classic transformations:

* log transformation ($\lambda = 0$)
* inverse transformation ($\lambda = -1$)
* square root transformation ($\lambda = 1/2$)
* square transformation ($\lambda = 2$)

<br>

(but it changes intercept and rescale the $\beta$)


## The Box-Cox transformation (Box & Cox 1964)

```{r boxcox, message = FALSE}
car::boxCox(fit_poison)  ## makes logLik profile!
```


## The Box-Cox transformation (Box & Cox 1964)

```{r boxcox 2}
summary(bc <- car::powerTransform(fit_poison))
car::testTransform(bc, lambda = -1)
```

<br>

We will consider -1 instead of ```r round(bc$lambda[[1]], 2)``` as it is close enough and easier to interpret!


## Poison example linearised

```{r poison bc}
fit_poison_bc <- update(fit_poison, car::bcPower(Time, lambda = -1) ~ .)
plot(residuals(fit_poison_bc) ~ fitted(fit_poison_bc), xlab = "fitted values", ylab = "residuals")
abline(h = 0, col = "red", lty = 2)
```


## Poison example linearised

### Predictions
```{r poison pred}
data.for.pred <- expand.grid(treat = levels(poison$treat), poison = levels(poison$poison))
(pred <- cbind(data.for.pred, predict(fit_poison_bc, newdata = data.for.pred, interval = "confidence")))
```

There are in the Box-Cox scales. In this case, it represents the survival rate, but you can always get back to the original scale:

```{r pred unbc}
lambda <- -1; (pred$fit * lambda + 1)^(1/lambda)
```


## Comparison of tests

```{r poison summary}
summary(fit_poison) ## the original model
```


## Comparison of tests

```{r poison bc summary}
summary(fit_poison_bc) ## the boxcoxed model
```


# Lack of perfect multicollinearity

## Lack of perfect multicollinearity in brief

### Assumption

The design matrix must have full rank. That means that the number of parameters to be estimated must be equal to the rank of the design matrix.

### Causes and consequences of violation

Caused by having less data than parameters or when there is linear dependence between the column vectors of the design matrix. In such case, some parameters cannot be computed.

### Solution

* change design matrix (change parameterization or drop redundant effects)
* change the experimental design
* collect more data

### Alternative

* none


## Degenerated design matrix: ```n < p```
```{r degenerated n too small, error = TRUE}
set.seed(1L)
n <- 3
Alien <- data.frame(humans_eaten = 1:n,
                    flowers_eaten = round(runif(n, min = 1, max = 15)),
                    cactus_eaten =  round(runif(n, min = 1, max = 10)))

Alien$size <- rnorm(n = nrow(Alien),
  mean = 50 + 0.2 * Alien$humans_eaten + 0.9 * Alien$flowers_eaten + 0.1 * Alien$cactus_eaten,
  sd = sqrt(25))

fit_alien1a <- lm(size ~  cactus_eaten + humans_eaten + flowers_eaten, data = Alien)
coef(fit_alien1a)
fit_alien1b <- lm(size ~  humans_eaten + flowers_eaten + cactus_eaten, data = Alien)
coef(fit_alien1b)
```


## Degenerated design matrix: trivial redundancy

```{r degenerated redundancy, error = TRUE}
set.seed(1L)
Alien2 <- simulate_Aliens()
Alien2$half_humans_eaten <-  0.5 * Alien2$humans_eaten
fit_alien2 <- lm(size ~ humans_eaten + half_humans_eaten, data = Alien2)
coef(fit_alien2)
det(crossprod(model.matrix(fit_alien2)))  ## when det(XTX) <= 0, XTX has no inverse!
fit_alien2$rank  == ncol(model.matrix(fit_alien2))
```


## Degenerated design matrix: miscellaneous

```{r degenerated redundancy subtle, error = TRUE}
set.seed(1L)
Alien3 <- data.frame(humans_eaten = 1:12,
                     flowers_eaten = round(runif(12, min = 1, max = 15)),
                     cactus_eaten = 0)
Alien3$food_units <- 1.2*Alien3$humans_eaten + 0.6*Alien3$flowers_eaten
Alien3$size <- rnorm(n = 12, mean = 50 + 1*Alien3$food_units, sd = sqrt(25))
fit_alien3 <- lm(size ~ food_units + humans_eaten + flowers_eaten + cactus_eaten, data = Alien3)
coef(fit_alien3)
caret::findLinearCombos(model.matrix(fit_alien3))  ## Tip: help to see what creates the issue
```


## Non-perfect multicollinearity

### Sometimes assumptions are met, but problems can still occur

```{r USA}
summary(fit_US  <- lm(Rape ~ Assault + Murder, data = USArrests))$coef
summary(fit_US2 <- lm(Rape ~ Murder, data = USArrests))$coef
```

<br>

It is a problem to infer causal inference, not so much for predictions!


## Non-perfect  multicollinearity: why?
```{r USA plot}
pairs(USArrests)
```


## Non-perfect  multicollinearity: why?

```{r USA 3}
cor(model.matrix(fit_US))  ## direct measure of correlation in the design matrix
cov2cor(vcov(fit_US))  ## direct measure of correlation between parameter estimates
```

<br>

In more complex models, the numbers do not necessarily match, so it is good practice to check both matrices.


## Non-perfect  multicollinearity: why?

### Often diagnosed using the Variance Inflation Factor

```{r USA 4}
car::vif(fit_US)
R2 <- summary(lm(Assault ~ Murder, data = USArrests))$r.squared  ## works too if more variables
1/(1 - R2)
```

<br>

Note: the model here considers the relationship between the predictors (the response variable of ```fit_US``` was ```rape```).

The VIF tells you by how much the variance in the uncertainty in parameter estimates increases due to multicollinearity.


## Non-perfect  multicollinearity: solution?

* drop one variable
* merge them

```{r pca}
pca <- prcomp(~ Assault + Murder, data = USArrests, scale. = TRUE)
USArrests$PC1 <- pca$x[, 1]
summary(fit_US3 <- lm(Rape ~ PC1, data = USArrests))
```


# Predictor variables have fixed values

## Predictor variables have fixed values (in brief)

### Assumption

The dependent variable are represented by fixed values.

### Causes and consequences of violation

The presence of measurement errors is the main cause of violation. Violation can trigger both estimates and tests to be biased.

### Solution

* often ignored in practice
* better measurements

### Alternative

* multipurpose numerical approaches
* errors-in-variables models
* reduced major axis regression


## Example

```{r measurement error}
set.seed(1L)
Alien4 <- simulate_Aliens(100)
summary(lm(size ~ humans_eaten, data = Alien4))$coef
Alien4$humans_eaten_err <- Alien4$humans_eaten + rnorm(nrow(Alien4), sd = 10)
summary(lm(size ~ humans_eaten_err, data = Alien4))$coef
```


## Accounting for errors-in-variables using ```sem```

### Structural equation modelling

```{r sem, message=FALSE}
eqns <- sem::specifyEquations(text = "
                        size = alpha*Intercept + slope*humans_eaten
                        humans_eaten = 1*humans_eaten_err
                        V(size) = sigma
                        V(humans_eaten) = 1
                        V(humans_eaten_err) = 1
                        ")
fit_sem <- sem::sem(eqns, data = Alien4, raw = TRUE, fixed.x = "Intercept")
summary(fit_sem, analytic.se = FALSE)$coef  ## use analytic.se = FALSE (uses z as consider variance known)
```

Other solutions are possible using mixed models.

# Assumptions about the errors

# Independence

## Independence in brief

### Assumption

The errors (not the residuals) are uncorrelated: $\text{cov}(\epsilon_i, \epsilon_j) = 0$, with $i \neq j$.

### Causes and consequences of violation

A lack of independence (serial autocorrelation) in the residuals can appear if there is a departure from linearity, if data have been sampled non-randomly (e.g. spatial or temporal series), or if there is an overarching structure (e.g. repeated measures within individuals, families, species, ...). The lack of independence increases the risk of false positive (sometimes a lot).


### Solution

* transformation (see linearity)
* aggregation (be carefull)
* sub-sampling

### Alternative

* mixed models (LMM and GLMM)


## Testing for independence

We can use the Durbin-Watson test: D-W [0; 4]

* D-W = 2 no-autocorrelation
* D-W <<2 positive autocorrelation
* D-W >>2 negative autocorrelation

```{r DW}
set.seed(1L)
car::durbinWatsonTest(modConv <- lm(fconvict ~ tfr + partic + degrees + mconvict, data = Hartnagel), max.lag = 3)
```


## Testing for independence

We can also compute the partial autocorrelations for the residuals serie,

```{r pacf}
pacf(modConv$residuals)
```

but mind that the CI plotted here are very aproximative.


## Testing for independence induced by a specific variable

```{r DW 2, message = FALSE}
lmtest::dwtest(modConv, order.by = modConv$model$degrees)
```


## Testing for independence by eye

### It is difficult when the problem is not extreme

```{r DW by eye}
plot(residuals(modConv) ~ fitted(modConv))
abline(h = 0, lty = 2, col = "red")
```


## Testing for independence by eye

### The origin of the problem is here the time!

```{r DW by eye 2}
plot(residuals(modConv) ~ Hartnagel$year, type = "o")
abline(h = 0, lty = 2, col = "red")
```


# Constant variance (homoscedasticity)

## Homos(c/k)edasticity in brief

### Assumption

The variance of the error (not residuals) is constant: $\text{var}(\epsilon_j) = \sigma^2$ for all $j$.
With matrix notation: if $\epsilon^\text{T}$ is the vector of all $\epsilon_j$, then we assume $\text{cov}(\epsilon, \epsilon) = \text{E}(\epsilon \epsilon^\text{T}) = \sigma^2I_n$, where $I_n$ is the $n \times n$ identity matrix.

### Causes and consequences of violation

Heteros(c/k)edasticity can emerge when there is a mean - variance relationship, when there is non independence between observations, when reaction norm changes acording to the treatement. It can create both false positives and false negative.

### Solution

* transformation (see linearity)
* post-hoc correction of the SE (not so great)

### Alternative

* Generalized Linear Models (GLM)
* heteroscedastic models


## Example of heteroscedasticity

```{r rpois}
set.seed(1L)
Alien5 <- simulate_Aliens(N = 100)
Alien5$eggs <- rpois(100, lambda = 2 + 1 * Alien5$humans_eaten)
fit_alien5 <- lm(eggs ~ humans_eaten, data = Alien5)
summary(fit_alien5)$coef
bptest(fit_alien5)
```

The Breusch-Pagan test follows a Chi-square distribution (thus stat | H0 = df).


## Testing for heteroscedasticity by eye

Residuals must be standardized as raw residuals always have some (minor) heteroscedasticity.

```{r plot heterosced}
plot(abs(rstandard(fit_alien5)) ~ fitted(fit_alien5))
```


## Post-hoc correction (not optimal)

```{r rpois correction}
vcov(fit_alien5)
car::hccm(fit_alien5)  ## correct the covariance matrix of parameter estimates
estimates <- coef(fit_alien5)
std.errors <- sqrt(diag(car::hccm(fit_alien5)))
t.values <- estimates/std.errors
p.values <- 2*pt(abs(t.values), df = fit_alien5$df.residual, lower.tail = FALSE)
cbind(estimates, std.errors, t.values, p.values)
```


## Post-hoc correction (not optimal)

Same using ```Anova```:
```{r}
car::Anova(fit_alien5, white.adjust = TRUE)  ## vcov = hccm
35.2490945224257^2  ## t^2 from previous slide
```


# Normality

## Normality in brief

### Assumption

The errors (not the residuals) should be normaly distributed.

### Causes and consequences of violation

The distribution of residuals can be skewed, this is often caused by the presence of outliers, and/or when the process generating the data is very different from normal (e.g. Poisson, Binomial...).

### Solution

* transformation (see linearity)
* taking outliers out (mindfully!)

### Alternative

* robust regressions
* GLM


## Testing normality

### There are many tests for normality out there...

Example: the Lilliefors (Kolmogorov-Smirnov) test for normality, Shapiro-Wilk Normality Test...

```{r norm test}
nortest::lillie.test(fit_poison$residuals)   ## stat = 0 when normal
shapiro.test(fit_poison$residuals)  ## stat = 1 when normal
```


## Testing normality by eye

<div class=columns-2>

```{r norm by eye}
qqnorm(residuals(fit_poison))
qqline(residuals(fit_poison), col = 2, lty = 2)
```

```{r norm by eye 2}
qqnorm(residuals(fit_poison_bc))  ## the BoxCoxed fit
qqline(residuals(fit_poison_bc), col = 2, lty = 2)
```

</div>


## Testing all assumptions on the errors at once

```{r plot mod, fig.height = 5, fig.width = 5}
par(mfrow = c(2, 2))
plot(fit_poison)
```


# Outliers

## Outliers in brief

### What are they?

They are observation that seem not to belong to the others.

### Why do they matter?

A few very deviant points can strongly influence all your estimations.

### What should you do with them?

It depends... but never trash them blindly.

If you have very good reasons to take them out, do mention it in the paper!


## Example of outlier

<div class=columns-2>

```{r Davis, fig.width = 3.5, fig.height = 3.5}
fit_davis <- lm(weight ~ height, data = Davis)
plot(weight ~ height, data = Davis)
abline(fit_davis, col = "red", lwd = 2)
```

```{r Davis 2, fig.width = 3.5, fig.height = 3.5}
fit_davis2 <- update(fit_davis, data = Davis[-12, ])
plot(weight ~ height, data = Davis[-12, ])
abline(fit_davis2, col = "blue", lwd = 2)
```

</div>

Note: to identify a given point on the plot, call `with(Davis, identify(height, weight, row.names(Davis)))`, then click close to the point and press escape.


## There are many ways to identify an outlier

```{r influence.measures 0}
influence.measures(fit_davis)  ## In the next slides we will see all that in details!
```


## Leverage vs Influence

A regression outlier is an observation that has an unusual value of the dependent variable Y, conditional on X.

Regression outliers may not look at outliers on any Y or X variables.


### Leverage

The leverage quantifies how unusual on observation is in terms of X values.

A high leverage is not necessarily a bad thing.


### Influence

An observation is influential if it strongly influences the predicted values.

A high influence is not necessarily a bad thing.

We do not want high influence caused by high leverage!

### Conclusion: we need to look at both!


## Measuring the leverage

This is done by extracting the diagonal element of the hat matrix: the hat values.

Recall: $\widehat{Y} = HY$

* range: [1/n; 1]
* mean : k/n

```{r hat}
head(sort(hatvalues(fit_davis), decreasing = TRUE))
```

<!--
```{r hat complex}
X <- model.matrix(fit_davis)
head(sort(diag(X %*% solve(crossprod(X)) %*% t(X)), decreasing = TRUE))
```
-->

For some computations and plots the leverage are rescaled as $\frac{h_{i,i}}{1-h_{i,i}}$, but it does not change the reasoning.


## Measuring the leverage

```{r hat plot}
plot(fit_davis, which = 5)
```


## Measuring the influence on predictions

```{r influence}
head(sort(dffits(fit_davis), decreasing = TRUE))
head(sort(cooks.distance(fit_davis), decreasing = TRUE))
```

Both the DFFITS statistics and the Cook distance (aka Cook's D) are two relative measures of the extent to which the predicted y-values changes if a given observation is dropped. The DFFITS measure is scaled by the standard deviation of the fit at the point. The Cook's D is based on a F statistics comparing simultaneously the changes in all estimates when the observation is dropped or not.

A high value is obtained when the observation is associated to high leverage, high residual or both.


## Measuring the influence on predictions

```{r influence2}
plot(dffits(fit_davis2), cooks.distance(fit_davis2), xlab = "DFFITS", ylab = "Cook's distance")
```


## Measuring the influence on predictions

```{r influence3, fig.width = 10}
par(mfrow = c(1, 3))
plot(fit_davis, which = 4:6)
```


## Measuring the influence on predictions with another example

```{r influence4, fig.width = 10}
fit_UK <- lm(height ~ sex * milk, data = UK[1:20, ])
par(mfrow = c(1, 3))
plot(fit_UK, which = 4:6)
```


## Measuring the influence on predictions with another example bis

```{r influence5, fig.width = 10}
fit_UK2 <- lm(height ~ sex * milk, data = UK)
par(mfrow = c(1, 3))
plot(fit_UK2, which = 4:6)
```


## Measuring the influence on each parameter estimates

```{r dfbetas}
head(dfbeta(fit_davis), n = 3)
coef(fit_davis) - coef(update(fit_davis, data = Davis[-1, ]))
head(dfbetas(fit_davis), n = 3) ## same in SE units of the coef
```


## Measuring the influence on the estimation of the covariance matrix

```{r cov}
head(sort(covratio(fit_davis), decreasing = TRUE))
det(vcov(update(fit_davis, data = Davis[-19, ]))) / det(vcov(fit_davis))

```


## Exploring in depth outliers: all at once

Note: don't trust much rule of thumbs to identify outliers "automatically". If you still want to know some of them, you can look at the beginning of the code of the function ```influence.measures()```.

```{r influence.measures}
influence.measures(fit_davis)  ## stars are just there to attract your attention, there is no proper tests!
```
<!- The dfbetas are shown for each parameter! -->


## What you need to remember

* the six main assumptions behind LM
* how to check if these assumptions are met (by means of tests and using your eyes)
* how to do the Box-Cox transformation
* that you can manipulate the design matrix to solve problems
* how to study carefully outliers
* that you will have to learn GLM and (G)LMM to solve some usual problems


# Table of contents

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Fitting procedure](./LM_fitting.html)
* 2.2 [Tests & Intervals](./LM_test_intervals.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions.html)
* 2.4 [Let's practice](./LM_practice.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>


