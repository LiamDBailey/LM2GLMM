---
title: "Linear models: Fitting procedure"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.1 Fitting LM}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(LM2GLMM)
```

# The toy dataset

## Let's simulate more Alien data

### Imagine an ideal (unrealistic) situation
We know that the process generating the height of aliens can be
approximated by the following linear model:

* $\texttt{height}_i = 50 + 1.5 \times \texttt{humans_eaten}_{i} + \epsilon_i$
* $\epsilon_i \sim \mathcal{N}(0, \sigma^2 = 25)$

<br>

We generate the data corresponding to 12 aliens that have eaten
between 1 and 12 humans.

```{r alien data}
set.seed(123L)
Alien <- data.frame(humans_eaten = sample(1:12))
Alien$size <- rnorm(n = 12, mean = 50 + 1.5*Alien$humans_eaten, sd = sqrt(25))
```


## In reality, model parameters are unknown, so we will have to estimate them!

```{r alien lm, echo = FALSE}
mod_alien_lm <- lm(size ~ humans_eaten, data = Alien)
```


<div class="columns-2">

```{r alien data 2 eval, fig.align='center', fig.asp=1, fig.width=4, echo = FALSE, fig.cap="True relationship"}
plot(size ~ humans_eaten, data = Alien, ylab = "Alien size (cm)", xlab = "No. of humans eaten")
abline(a = 50, b = 1.5, col = "green", lwd = 2)
```

```{r alien data 3 eval, fig.align='center', fig.asp=1, fig.width=4, echo = FALSE, fig.cap="Inferred relationship"}
plot(size ~ humans_eaten, data = Alien, ylab = "Alien size (cm)", xlab = "No. of humans eaten")
text(x = nrow(Alien)/2, y = 82, labels = "?", cex = 5, col = "blue")
abline(mod_alien_lm, col = "blue", lwd = 2, lty = 2)
```
</div>

## Fitting the model to the data...

### ... means estimating the values of the model parameters.

### Why?

* for drawing inferences about future (or past) obervable events

# Notations

## Mathematical notation of LM fits

### Simple notation

<center><font size="5"> $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_p} \times x_{p,i}$ </font></center>

<br>

* $x_{j,i}$ = constants derived from the predictors / explanatory variables / independent variables
* $\hat{y_i}$ = the predictions / predicted values
* $\hat{\beta_j}$ = the (model parameter / regression coefficient) estimates
* $y_i - \hat{y_i} = \varepsilon_i$ = the residuals (i.e. the estimates for the errors)

<br>

### Matrix notation

<center><font size = 5> $\widehat{Y} = X \widehat{\beta}$ </font></center>

<center><font size = 5> $\widehat{\epsilon} = \varepsilon = Y - \widehat{Y}$ </font></center>

# Fitting procedure

## How to fit a LM?

### By maximum likelihood
We want the $\widehat{\beta}$ maximizing the likelihood.

The likelihood of the model given the data is
equal to the probability density assumed for those data given those parameter
values, that is: $\displaystyle {\mathcal {L}}(\theta \mid x) = P(x \mid \theta)$.

<br>

### By ordinary least squares
We want the $\widehat{\beta}$ minimizing the residual sum of squares (RSS).

The RSS = $\displaystyle\sum_{i=1}^{n}{\varepsilon_i^2}$.

### Both methods are equivalent in the case of LM!

## Fitting the Alien data using ```lm()``` as a blackbox

```{r fit alien lm}
(mod_alien_lm <- lm(size ~ humans_eaten, data = Alien))
names(mod_alien_lm)
```


## We can extract all kind of information from the fit

```{r fit coef}
(coef_lm <- mod_alien_lm$coef) ## We expect something close to 50 and 1.5
(sigma2 <- summary(mod_alien_lm)$sigma^2)[1] ## We expect something close to 25
(logLik_lm <- logLik(mod_alien_lm)[[1]])
(rss_lm <- anova(mod_alien_lm)$"Sum Sq"[2])
```

# Reverse engineering the LM fit

## Recovering the design matrix

```{r design matrix}
X_Alien <- model.matrix(mod_alien_lm)  ## Tip: same as model.matrix(~ humans_eaten, data = Alien)
head(X_Alien)
```

## Recovering the predicted values

```{r predict}
pred_auto   <- predict(mod_alien_lm)  ## Tip: same as mod_alien_lm$fitted.values / fitted(mod_alien_lm)
pred_simple <- coef_lm[1] + coef_lm[2] * Alien$humans_eaten
pred_matrix <- X_Alien %*% coef_lm
```

All three methods are identical!

```{r show predict}
head(cbind("auto" = pred_auto, "simple" = pred_simple, "matrix" = c(pred_matrix)))
Alien$pred <- pred_auto
```

## Recovering the residuals
```{r residuals}
resid_auto   <- residuals(mod_alien_lm)  ## Tip: same as mod_alien_lm$residuals 
resid_simple <- Alien$size - (coef_lm[1] + coef_lm[2] * Alien$humans_eaten)
resid_matrix <- matrix(Alien$size) - X_Alien %*% coef_lm
```

```{r show residuals}
head(cbind("auto" = resid_auto, "simple" = resid_simple, "matrix" = c(resid_matrix)))
Alien$resid <- resid_auto
```

## Recovering the min Residual Sum of Squares

```{r rss alien}
(rss_lm <- anova(mod_alien_lm)$"Sum Sq"[2])

sum(Alien$resid^2)
```



## The Residual Sum of Squares (RSS)

<div class="columns-2">
```{r alien RSS plot, fig.align='center', fig.asp=1, fig.width=5, echo = FALSE, fig.cap="Residuals on best fit"}
plot(size ~ humans_eaten, data = Alien, ylab = "Alien size (cm)", xlab = "No. of humans eaten", asp = 1)
points(pred ~ humans_eaten, col = "blue", data = Alien, pch = 20)
with(Alien, segments(x0 = humans_eaten, x1 = humans_eaten, y0 = size, y1 = pred, col = "orange"))
legend("topleft", bty = "n", col = c("black", "blue"), pch = c(1, 20), legend = c("obs", "pred"))
```

```{r alien RSS plot2, fig.align='center', fig.asp=1, fig.width=5, echo = FALSE, fig.cap="Squared residuals on best fit"}
plot(
  size ~ humans_eaten,
  data = Alien,
  ylab = "Alien size (cm)",
  xlab = "No. of humans eaten",
  asp = 1
  )
points(pred ~ humans_eaten, col = "blue", data = Alien, pch = 20)
for (i in 1:nrow(Alien)) {
  with(Alien, polygon(
    x = c(
    humans_eaten[i],
    humans_eaten[i],
    humans_eaten[i] + abs(resid[i]),
    humans_eaten[i] + abs(resid[i])
    ),
    y = c(pred[i], size[i], size[i], pred[i])
    ))
}
with(Alien, segments(x0 = humans_eaten, x1 = humans_eaten, y0 = size, y1 = pred, col = "orange"))
legend("topleft", bty = "n", col = c("black", "blue"), pch = c(1, 20), legend = c("obs", "pred"))
```
</div>

## The Residual Sum of Squares (RSS)

<div class="columns-2">
```{r alien RSS bad fit, fig.align='center', fig.asp=1, fig.width=5, echo = FALSE, fig.cap="Squared residuals on bad fit"}
plot(
  size ~ humans_eaten,
  data = Alien,
  ylab = "Alien size (cm)",
  xlab = "No. of humans eaten",
  asp = 1
  )
badslope <- 0.5
badintercept <- mean(Alien$size) - badslope * mean(Alien$humans_eaten)
badpred <- badintercept + badslope * Alien$humans_eaten
badresid <- Alien$size - badpred
points(badpred ~ humans_eaten, col = "blue", data = Alien, pch = 20)
for (i in 1:nrow(Alien)) {
  with(Alien, polygon(
    x = c(
    humans_eaten[i],
    humans_eaten[i],
    humans_eaten[i] + abs(badresid[i]),
    humans_eaten[i] + abs(badresid[i])
    ),
    y = c(badpred[i], size[i], size[i], badpred[i])
    ))
}
with(Alien, segments(x0 = humans_eaten, x1 = humans_eaten, y0 = size, y1 = badpred, col = "orange"))
legend("topleft", bty = "n", col = c("black", "blue"), pch = c(1, 20), legend = c("obs", "pred"))
```
```{r alien RSS plot2 again, fig.align='center', fig.asp=1, fig.width=5, echo = FALSE, fig.cap="Squared residuals on best fit"}
plot(
  size ~ humans_eaten,
  data = Alien,
  ylab = "Alien size (cm)",
  xlab = "No. of humans eaten",
  asp = 1
  )
points(pred ~ humans_eaten, col = "blue", data = Alien, pch = 20)
for (i in 1:nrow(Alien)) {
  with(Alien, polygon(
    x = c(
    humans_eaten[i],
    humans_eaten[i],
    humans_eaten[i] + abs(resid[i]),
    humans_eaten[i] + abs(resid[i])
    ),
    y = c(pred[i], size[i], size[i], pred[i])
    ))
}
with(Alien, segments(x0 = humans_eaten, x1 = humans_eaten, y0 = size, y1 = pred, col = "orange"))
legend("topleft", bty = "n", col = c("black", "blue"), pch = c(1, 20), legend = c("obs", "pred"))
```

</div>

## Recovering the variance of the errors

```{r alien sigma2 error}
(sigma2_error <- summary(mod_alien_lm)$sigma^2)
sum(Alien$resid^2) / (nrow(Alien) - length(coef_lm))
var(Alien$resid) * (nrow(Alien) - 1) / (nrow(Alien) - length(coef_lm))
```


## Recovering the variance of the residuals

```{r alien sigma2 resid}
(sigma2_resid <- sum(Alien$resid^2) / nrow(Alien))
var(Alien$resid) * (nrow(Alien) - 1) / nrow(Alien) 
sigma2_error * (nrow(Alien) - length(coef_lm)) /  nrow(Alien)
```


## Recovering the max (log-)likelihood

```{r alien loglik}
Alien$density <- dnorm(x = Alien$size, mean = Alien$pred, sd = sqrt(sigma2_resid))
Alien[1, ]
```


<div class="columns-2">

```{r alien lik plot, fig.align='center', fig.asp=1, fig.width=4, echo = FALSE}
par(mar = c(4, 4, 1, 1))
curve(dnorm(x, mean = Alien[1, "pred"], sd = sqrt(sigma2_resid)), from = 30, to = 30 + Alien[1, "pred"], ylab = "probability density", xlab = "size")
abline(h = 0, lty = 2)
abline(v = Alien[1, "pred"], lty = 2, col = "blue")
points(x = Alien[1, "pred"], y = 0, pch = 20, col = "blue")
points(x = Alien[1, "size"], y = 0)
segments(x0 = Alien[1, "pred"], x1 = Alien[1, "size"], y0 = 0, y1 = 0, col = "orange")
segments(x0 = Alien[1, "size"], x1 = Alien[1, "size"], y0 = 0, y1 = Alien[1, "density"], col = "purple")
arrows(x0 = Alien[1, "size"], x1 = 30, y0 = Alien[1, "density"], y1 = Alien[1, "density"], col = "purple", length = 0.1)
```

```{r alien log density}
(logLik_lm <- logLik(mod_alien_lm)[[1]])
log(prod(Alien$density))
sum(log(Alien$density))
```

</div>


## Recovering the estimates numerically by maximum likelihood (ML)

```{r def comput_logLik_Alien}
compute_logLik_Alien <- function(param, data = Alien) {
  predicts <- param["intercept"] + param["slope"] * data$humans_eaten
  sigma2_resid <- abs(param["sigma2_resid"])
  logL <- sum(dnorm(data$size, mean = predicts, sd = sqrt(sigma2_resid), log = TRUE))
  return(logL)
  }

(theta.lm <- c("intercept" = coef_lm[1][[1]], "slope" = coef_lm[2][[1]],
               "sigma2_resid" = sigma2_resid)) ## For testing
compute_logLik_Alien(param = theta.lm)
```

## Recovering the estimates numerically by maximum likelihood (ML)

### We look for the estimates yielding to the maximum likelihood
```{r def logLik Alien computation}
result_opt <- optim(c("intercept" = 0, "slope" = 1, "sigma2_resid" = 1), compute_logLik_Alien,
       control = list(fnscale = -1))
result_opt$par
result_opt$value
```


## Recovering the estimates numerically by ordinary least squares (OLS)

```{r def compute_rss_Alien}
compute_rss_Alien <- function(param, data = Alien) {
  predicts <- param["intercept"] + param["slope"] * data$humans_eaten
  rss <- sum((data$size - predicts)^2)
  return(rss)
}

all.equal(rss_lm, compute_rss_Alien(param = theta.lm))  ## For testing
```

```{r}
optim(c("intercept" = 0, "slope" = 1), compute_rss_Alien)$par
theta.lm
```

## Computing estimates using linear algebra

<font size = 8> $\hat{\beta} = (X^\text{T}X)^{-1}X^\text{T}Y$ </font> (see [wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)) for demonstration)

### The direct implementation works:
```{r lin algebra}
Y <- matrix(Alien$size)
X <- model.matrix(mod_alien_lm)
solve(t(X) %*% X) %*% t(X) %*% Y  ## Tip: solve(x) returns the inverse of the matrix x
```

... but ```lm()``` does not do that because it is somewhat inefficient.


## The QR decomposition

The goal is simply to decompose the model matrix into two new matrices that present nice properties for doing math efficiently.

```{r QR}
qr_list <- qr(X)  ## same as mod_alien_lm$qr
Q <- qr.Q(qr_list, complete = TRUE)  ## orthogonal matrix n * n
# all.equal(Q %*% t(Q), diag(nrow(Alien))) ## TRUE
R <- qr.R(qr_list, complete = TRUE)  ## upper triangular matrix n * p
all.equal(Q %*% R, X, check.attributes = FALSE)  ## Q %*% R is equal to X!!
```

We can even simplify:
```{r Q1R1}
Q1 <- qr.Q(qr_list) ## dim = n * p
R1 <- qr.R(qr_list)  ## dim = p * p
all.equal(Q1 %*% R1, X, check.attributes = FALSE)  ## Q1R1 is equal to X!!
```
Note: other decompositions are sometimes used (e.g. Cholesky).

## Recovering the estimates using QR
<script> /* <font size = 8> $\hat{\beta} = R_1^{-1} Q_1^{\mathrm{T}}Y$ </font> */ </script>
```{r}
solve(R1) %*% t(Q1) %*% Y  ## Note: lm does that without computing solve(R1) by using QTY instead of Q1TY
QTY <- t(Q) %*% Y ## same as mod_alien_lm$effects
backsolve(R1, QTY)
```

(the matrix $Q^\text{T}Y$ is also used to compute sum of squares in ```anova()```)


## Recovering almost everything efficiently using QR
```{r QR recover all}
compare <- function(A, vB) {all.equal(A, as.matrix(vB), check.attributes = FALSE)}
compare(qr.coef(qr_list, Y), mod_alien_lm$coef)
compare(qr.fitted(qr_list, Y), mod_alien_lm$fitted.values)
compare(qr.resid(qr_list, Y), mod_alien_lm$residuals)
```

## Dissecting the output from ```lm()```

```{r lm output}
names(mod_alien_lm)
```
We have already seen all non-trivial components!

We have not yet seen:

* ```assign``` comes from ```attr(X, "assign")```: it indicates which parameters belong to each covariate (more later!).
* ```df.residuals``` is simply nrow(Alien) - rank.
* ```xlevels``` is here empty.
* ```call``` the clean function call to ```lm()```.
* ```terms``` an object of class terms: the formula with many attributes (```?terms.object```).
* ```model``` comes from ```model.frame()```: the data used for the fit.

## Difficulty 1: missing data

```{r Alien 2, error = TRUE}
rm(Alien) ## Tip: clean start
Alien <- data.frame(humans_eaten = 1:12)
set.seed(1L)
Alien$size <- rnorm(n = 12, mean = 50 + 1.5*Alien$humans_eaten, sd = sqrt(25))
Alien2 <- Alien
Alien2$size[c(2, 4)] <- NA
Alien2$humans_eaten[c(3, 4)] <- NA
head(Alien2)
mod_alien2_lm <- lm(size ~ humans_eaten, data = Alien2)
Alien2$pred <- mod_alien2_lm$fitted.values
```

## The rows with NA(s) are being omited

Think:
$\texttt{data} \rightarrow \texttt{model.frame()} \rightarrow \texttt{model.matrix()} \rightarrow \texttt{qr()} \rightarrow \texttt{qr.fitted()}$

```{r missing data}
mfAlien2 <- model.frame(formula = size ~ humans_eaten, data = Alien2)
head(mfAlien2)  ## same as mod_alien2_lm$model
head(model.matrix(object = ~ humans_eaten, data = mfAlien2))
```

## New ```lm()``` component

```{r na.action}
mod_alien2_lm$na.action
```


## Prediction with NAs

```{r pred NA manual}
mfAlien2NA <- model.frame(formula = size ~ humans_eaten, data = Alien2, na.action = NULL)
XNA <- model.matrix(object = ~ humans_eaten, data = mfAlien2NA)
as.numeric(XNA %*% coef(mod_alien2_lm))
```

```{r pred na}
Alien2$pred <- predict(mod_alien2_lm, newdata = Alien2)
head(Alien2)
```

## Difficulty 2: degenerated model matrix

```{r Alien3, error = TRUE}
Alien3 <- data.frame(humans_eaten = 1:12)
set.seed(1L)
Alien3$size <- rnorm(n = 12, mean = 50 + 1.5*Alien3$humans_eaten, sd = sqrt(25))
Alien3$half_humans_eaten <-  0.5 * Alien3$humans_eaten
mod_alien3_lm <- lm(size ~ humans_eaten + half_humans_eaten, data = Alien3)
coef(mod_alien3_lm)
det(crossprod(model.matrix(mod_alien3_lm)))  ## when det(XTX) <= 0, the matrix is singular / degenerated
mod_alien3_lm$rank
```

## More subtle linear combination
```{r Alien4, error = TRUE}
set.seed(1L)
Alien4 <- data.frame(humans_eaten = 1:12,
                     flowers_eaten = round(runif(12, min = 1, max = 15)),
                     cactus_eaten = 0)
Alien4$food_units <- 1.2*Alien4$humans_eaten + 0.6*Alien4$flowers_eaten
Alien4$size <- rnorm(n = 12, mean = 50 + 1*Alien4$food_units, sd = sqrt(25))
mod_alien4_lm <- lm(size ~ food_units + humans_eaten + flowers_eaten + cactus_eaten, data = Alien4)
coef(mod_alien4_lm)
caret::findLinearCombos(model.matrix(mod_alien4_lm))  ## Tip: help to see what creates the issue
```


## Degenerated because of less data than predictors
```{r Alien5, error = TRUE}
set.seed(1L)
Alien5 <- data.frame(humans_eaten = 1:3,
                     flowers_eaten = round(runif(3, min = 1, max = 15)),
                     cactus_eaten =  round(runif(3, min = 1, max = 10)))

Alien5$size <- rnorm(n = 3,
  mean = 50 + 0.2 * Alien5$humans_eaten + 0.9 * Alien5$flowers_eaten + 0.1 * Alien5$cactus_eaten,
  sd = sqrt(25))

mod_alien5_lm <- lm(size ~  cactus_eaten + humans_eaten + flowers_eaten, data = Alien5)
coef(mod_alien5_lm)
mod_alien5_lm <- lm(size ~  humans_eaten + flowers_eaten + cactus_eaten, data = Alien5)
coef(mod_alien5_lm)
```

## Real case: the growth data

Wait for Lena to give green light...


# Fitting linear models with both continuous and categorical predictors

## Back to earth: the UK dataset

```{r UK data}
head(UK)
mod_UK1 <- lm(height ~ drink + sex*weight, data = UK)
```

## Model frame of ```mod_UK1```
```{r UK model frame}
str(mod_UK1$model)
```

# Junk for later

##

```{r cov matrix}
summary(mod_alien_lm)$sigma^2 * solve(t(X) %*% X)  ## same as vcov(mod_alien_lm)
```


## A very simple example

```{r orange data}
OrangeTree1 <- subset(Orange, Tree == 1)
OrangeTree1
```


### Our first model
<font size="5"> $\texttt{circumference}_i = \beta_0 + \beta_{\texttt{age}} \times \texttt{age}_{i} + \epsilon_i$ </font>

```{r lm orange}
mod_orange_lm <- lm(circumference ~ age, data = OrangeTree1)
mod_orange_glm <- glm(circumference ~ age, data = OrangeTree1, family = gaussian())

```




## A very simple example




## A simple example

```{r UK boys data}
UKBoys <- subset(UK, sex == "Boy")
UKBoys$milk_f <- factor(UKBoys$milk)
mod_uk_boys <- lm(height ~ milk_f, data = UKBoys)
```

## terrible residuals
```{r terrible}
mod <- lm(log(weight) ~ sex*(weight + drink + height), data = UK) ## notice weight used twice...
```

