---
title: "LM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.0 LM: Introduction}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(LM2GLMM)
options(width = 110)
knitr::opts_chunk$set(error = TRUE)
```

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Fitting procedure](./LM_fitting.html)
* 2.2 [Tests & Intervals](./LM_test_intervals.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions.html)
* 2.4 [Let's practice](./LM_practice.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>


## You should learn during this session

* how to write a linear model (using 3 different notations)
* how to simulate the data according to simple linear models
* some key vocabulary (e.g. response variable, fitted values, residuals...)
* how to understand the meaning of parameters by investigating the design matrix
* how to manipulate design matrices via formula and contrasts
* how to compute predictions


# Definition and notations

## Disclaimer

There is (unfortunately) no such thing as a standard vocabulary to discuss LM.

The terms I chose to use are not used by all; even simple and widespread terms such as *explanatory variables* and *residuals* can mean different things for different people.

Statisticians perhaps do not need words as much as we do because they can rely on their understanding of precise mathematical definitions.

As we are not statisticians, for the sake of clear communication, I will try to stick to quite precise terms which I am going to define in my own way.

Keep this in mind if you are looking at other content online or in books.

The important thing is for you to grasp what these words represent, more than the word themselves.


## What is a Linear Model (LM)?

### What is a statistical model?

**A statistical model represents, often in considerably idealized form, the data-generating process.** [(wikipedia)](https://en.wikipedia.org/wiki/Statistical_model)
<br>

### What is a linear model?

**<span style="color:red"> A data-generating process produced by a linear function</span>: the data are generated from a set of terms by multiplying each term by a constant (a model parameter) and summing them.**


## Mathematical notation of LM: simple notation

<center><font size="5"> $y_i = \beta_0 + \beta_1 \times x_{1,i} + \beta_2 \times x_{2,i} + \dots + \beta_{p} \times x_{p,i} + \epsilon_i$ </font></center>

<br>

* $y_i$ = the response variable / dependent variable / observations to explain
* $\beta_j$ = the model parameters / regression coefficients
* $x_{j,i}$ = regressors / constants derived from the explanatory variables
* $\epsilon_i$ = the errors / residual errors

<br>

This is the notation that biologists should use in their paper/thesis to describe their model.

Note: the regressors are sometimes directly given by the columns from your dataset, but not always (more on that later).


## Mathematical notation of LM: matrix notation

<center><font size = 8> $Y = X \beta + \epsilon$ </font></center>

$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \dots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \dots \\ \beta_p
\end{bmatrix} +
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \dots \\ \epsilon_n
\end{bmatrix}
$$

<br>

* $Y$ = the vector of observations
* $X$ = a matrix called the design (or model) matrix
* $\beta$ = the vector of model parameters
* $\epsilon$ = the vector of errors

This is the notation for statisticians and computer scientists.


## Aliens generated by goddess *Emerald Lion*
<!-- Emerald Lion is an anagram to linear model -->

Emerald Lion informed us that she grows aliens of a certain size as follows:

* $\texttt{size}_i = 50 + 1.5 \times \texttt{humans_eaten}_{i} + \epsilon_i$
* $\epsilon_i \sim \mathcal{N}(0, \sigma^2 = 5)$

<br>

We can thus generate the size of 6 aliens that have eaten between 1 and 3 humans following EL recipe:

```{r alien data}
set.seed(123L)
Alien <- data.frame(humans_eaten = rep(1:3, each = 2), intercept = 50, slope = 1.5)
Alien$error <- rnorm(n = 6, mean = 0, sd = sqrt(5))
Alien$size <- Alien$intercept + Alien$slope*Alien$humans_eaten + Alien$error
```

Note: we could also write the last 2 lines as the following line:

```{r alien data2}
# Alien$size <- rnorm(n = 6, mean = 50 + 1.5*Alien$humans_eaten, sd = sqrt(25))
```


## The study of the Aliens

This is the data we simulated:
```{r alien study}
Alien
```

Can you recognise:

- the model parameters?
- the regressor (here = explanatory variable)?
- the errors?
- the response variable?


## In practice, model parameters are unknown

So we will have to estimate them (that is the whole point of statistical modelling...)

*Fitting* the model to the data means estimating the values of the model parameters that maximise the probability of the data (more on that in the next sessions).

```{r alien lm}
(fit_alien_lm <- lm(size ~ humans_eaten, data = Alien))
```

<br>

Note: we did not get $50$ and $1.5$, and only talking to Emerald Lion can get you such population-level values.

<!--
## A fit is never perfect
```{r alien data 2 eval, fig.align='center', fig.asp=1, fig.width=4, echo = FALSE, fig.cap="True vs inferred mean relationship"}
plot(size ~ humans_eaten, data = Alien, ylab = "Alien size (cm)", xlab = "No. of humans eaten")
abline(a = 50, b = 1.5, col = "green", lwd = 2)
abline(fit_alien_lm, col = "blue", lwd = 2, lty = 2)
legend("topleft", fill = c("green", "blue"), legend = c("TRUE", "fitted"), bty = "n")
```
-->

## R notation for fitting LM fits

`lm(var_obs ~ var_a + var_b + ... + var_p, data = somedata)`

<br>

* `lm()` the function fitting the linear model
* `somedata` the data used for the fitting procedure
* `var_obs ~ var_a + var_b + ... + var_p` the model formula
* `var_obs` = the name (not quoted) of the column with the observations for the response variable
* `var_a, var_b, ..., var_p` = the names (not quoted) of the columns with the observations for the explanatory ( = independent = predictor) variables

<br>

This is the notation that is good for talking to R and to R users, but it does not describe the assumed data generating process.


## Mathematical notation of LM fits

### Simple notation

<center><font size="5"> $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_p} \times x_{p,i}$ </font></center>

<br>

* $\hat{y_i}$ = the fitted values
* $\hat{\beta_j}$ = the (model parameter / regression coefficient) estimates
* $x_{j,i}$ = regressors / constants derived from the explanatory variables
* $y_i - \hat{y_i} = \varepsilon_i$ = the residuals (i.e. the estimates for the errors / residual errors)

<br>

### Matrix notation

<center><font size = 5> $\widehat{Y} = X \widehat{\beta}$ </font></center>

<center><font size = 5> $Y = X \widehat{\beta} + \varepsilon = \widehat{Y} + \varepsilon$ </font></center>

<center><font size = 5> $\varepsilon = \widehat{\epsilon} = Y - \widehat{Y}$ </font></center>


# Model parameters & model parameters estimates

## Model parameters

* one dimension: $p \times 1$ vector
* fixed non-observable quantities

<br>

### They mediate the relationship between variables and the data generated: they convert each column from the design matrix into the elements of the response variable.

<br>

The meaning of the parameters thus always depends on the design matrix (more on that soon!)


## Model parameters estimates

There are the estimations for of the model parameters produced by the fit of the model:

```{r estimates}
Alien$intercept_est <- coef(fit_alien_lm)[["(Intercept)"]]
Alien$slope_est <- coef(fit_alien_lm)[["humans_eaten"]]
Alien
```


# Response variable, mean response values & fitted values

## The response variable

### It is the set of observations produced by the data-generating process
### In LM: a single continuous variable

<br>

Characteristics:

* one dimension: $n \times 1$
* continuous
* no other restriction


## The response variable

```{r alien data show}
Alien ## it is the column "size"
```
<div class="columns-2">

### Simple notation
* $y_1 = 50 + 1.5 \times `r Alien$humans_eaten[1]` + `r Alien$error[1]`$
* $y_2 = 50 + 1.5 \times `r Alien$humans_eaten[2]` + `r Alien$error[2]`$
* $y_3 = 50 + 1.5 \times `r Alien$humans_eaten[3]` + `r Alien$error[3]`$
* $y_4 = 50 + 1.5 \times `r Alien$humans_eaten[4]` + `r Alien$error[4]`$
* $y_5 = 50 + 1.5 \times `r Alien$humans_eaten[5]` + `r Alien$error[5]`$
* $y_6 = 50 + 1.5 \times `r Alien$humans_eaten[6]` + `r Alien$error[6]`$

### Matrix notation
$$
Y = \begin{bmatrix}
1 & `r Alien$humans_eaten[1]` \\
1 & `r Alien$humans_eaten[2]` \\
1 & `r Alien$humans_eaten[3]` \\
1 & `r Alien$humans_eaten[4]` \\
1 & `r Alien$humans_eaten[5]` \\
1 & `r Alien$humans_eaten[6]`
\end{bmatrix}
\begin{bmatrix}
50 \\
1.5
\end{bmatrix}
+
\begin{bmatrix}
`r Alien$error[1]`\\
`r Alien$error[2]`\\
`r Alien$error[3]`\\
`r Alien$error[4]`\\
`r Alien$error[5]`\\
`r Alien$error[6]`
\end{bmatrix}
$$

</div>


## Mean response values

The mean response value is here the mean size of an infinite number of aliens eating a given amount of humans.

<div class="columns-2">

### Simple notation
* $\overline{y_1} = 50 + 1.5 \times `r Alien$humans_eaten[1]`$
* $\overline{y_2} = 50 + 1.5 \times `r Alien$humans_eaten[2]`$
* $\overline{y_3} = 50 + 1.5 \times `r Alien$humans_eaten[3]`$
* $\overline{y_4} = 50 + 1.5 \times `r Alien$humans_eaten[4]`$
* $\overline{y_5} = 50 + 1.5 \times `r Alien$humans_eaten[5]`$
* $\overline{y_6} = 50 + 1.5 \times `r Alien$humans_eaten[6]`$

### Matrix notation
$$
\overline{Y} = \begin{bmatrix}
1 & `r Alien$humans_eaten[1]` \\
1 & `r Alien$humans_eaten[2]` \\
1 & `r Alien$humans_eaten[3]` \\
1 & `r Alien$humans_eaten[4]` \\
1 & `r Alien$humans_eaten[5]` \\
1 & `r Alien$humans_eaten[6]`
\end{bmatrix}
\begin{bmatrix}
50 \\
1.5
\end{bmatrix}
$$

</div>

<br>

Note: it is also the response minus the errors.


## Mean response values

They can easily be computed in R using vectors:

```{r alien predict from error}
Alien$size_mean <- Alien$intercept + Alien$slope * Alien$humans_eaten
# or # Alien$size_mean <- Alien$size - Alien$error
Alien
```


## The fitted values

There are the estimations for of the mean responses produced by the fit of the model:

```{r alien fitted values}
Alien$size_fitted <- fitted.values(fit_alien_lm)
```
```{r alien fitted values_res}
Alien
```


## The fitted values

They can easily be computed in R using vectors too:

```{r alien predict from error2}
size_fitted2 <- Alien$intercept_est + Alien$slope_est * Alien$humans_eaten
```

```{r alien predict from error2res}
all.equal(size_fitted2, Alien$size_fitted)
```


## The fitted values

They can also be computed using matrices (also true for mean response values):

```{r predict matrix}
size_fitted3 <- model.matrix(fit_alien_lm) %*% coef(fit_alien_lm)
```
```{r predict matrix_res}
all.equal(as.numeric(size_fitted3), Alien$size_fitted)
```

<br>

Note : R uses design matrices extensively!


# The design matrix

## The design matrix

### It is made of the regressors (= adequate representations of the predictors)

###  <span style="color:red"> You cannot interpret model parameters correctly without understanding the design matrix of a model! </span>

<br>

Characteristics:

* multi-dimensional: $n \times p$ with $n > p$
* deduced from the predictors
* known and measured without error
* columns must be linearly independent


## The design matrix

It is not estimated and is thus the same for both the true data-generating process and the inferred one:

<br>

<center><font size = 8> $Y = \color{red}{X} \beta + \epsilon$ </font></center>

<br>

<center><font size = 8> $\widehat{Y} = \color{red}{X} \widehat{\beta}$ </font></center>


## Design matrix

The model matrix is automatically produced when fitting a model:

```{r dm from fit}
model.matrix(fit_alien_lm)
```

`lm()` always estimate one model parameter per column of the design matrix:
```{r coef from fit}
names(coef(fit_alien_lm))
```


## Design matrix = formula + the data (+ contrasts)

To see the design matrix without fitting the model, you can also use formula in `model.matrix()`:

```{r pred matrix}
model.matrix(object = ~ humans_eaten, data = Alien)
```

<br>

The term `~ humans_eaten` is a *formula* despite not having a response variable on the left.

The formula tells `model.matrix()` (direclty or via `lm()`) how to create a design matrix from the dataset.

Turning a biological problem into a linear model is nothing more that figuring out which design matrix you need and thus which formula to write!


## A design matrix without intercept

```{r fit no intercept}
model.matrix(object = ~ 0 + humans_eaten, data = Alien)
lm(size ~ 0 + humans_eaten, data = Alien) ## you can also use -1
```

Can you think of what such fit would look like if plotted?


## A design matrix with function calls

```{r fit fn1}
(fit_log_humans_within <- lm(size ~ log(humans_eaten), data = Alien))
```

You can also apply such transformations in your data frame instead:

```{r fit fn2}
Alien_temp <- Alien
Alien_temp$log_humans_eaten <- log(Alien_temp$humans_eaten)
(fit_log_humans_without <- lm(size ~ log_humans_eaten, data = Alien_temp))
```


## A design matrix with function calls

Note: the coefficients did not change because the content of the 2 model matrices is the same:

```{r model matrix fn1}
model.matrix(fit_log_humans_within)
```

```{r model matrix fn2}
model.matrix(fit_log_humans_without)
```


## A design matrix with polynomials

```{r poly ex1}
(fit_poly_alien <- lm(size ~ poly(humans_eaten, 2), data = Alien))
```

```{r poly ex2}
(fit_poly_raw_alien <- lm(size ~ poly(humans_eaten, 2, raw = TRUE), data = Alien))
```


## A design matrix with polynomials

Note: the coefficients did change because the content of the 2 model matrices is not the same:

```{r model matrix poly}
model.matrix(fit_poly_alien)
```

```{r model matrix poly_raw}
model.matrix(fit_poly_raw_alien)
```


## A design matrix with polynomials

Note: alternative parametrizations do not necessarily impact the fitted values as is the case for the 2 parametrization for polynomials.

```{r fitted poly}
data.frame(fitted_poly = fitted.values(fit_poly_alien),
           fitted_poly_raw = fitted.values(fit_poly_raw_alien))
```

<br>

In case you are curious, the default polynomial formulation for the design matrix produces columns in the design matrix which are orthogonal (no correlation), which presents better numerical properties for linear algebra.


## (Galactic) Interlude

Goddess Emerald Lion informed us that Aliens have colonised different exoplanets since their creation and that their age depends on when the arrived on these planets, on their sex (the sex ZZ ages much faster), and on how many times they visited a black hole (due to the time dilatation happening there):

```{r Alien}
set.seed(123)
Alien2 <- data.frame(planet = factor(c(rep("Chambr", 2), rep("Riplyx", 2), rep("Wickor", 2))),
                     sex = factor(rep(c("Z", "ZZ"), times = 3)),
                     bh_trips = 6:1)
Alien2$age <- rep(0, nrow(Alien2))
Alien2$age[Alien2$planet == "Chambr"] <- Alien2$age[Alien2$planet == "Chambr"] + 100 
Alien2$age[Alien2$planet == "Riplyx"] <- Alien2$age[Alien2$planet == "Riplyx"] + 1000
Alien2$age[Alien2$planet == "Wickor"] <- Alien2$age[Alien2$planet == "Wickor"] + 10000
Alien2$age[Alien2$sex == "Z"] <- Alien2$age[Alien2$sex == "Z"] + -20
Alien2$age[Alien2$sex == "ZZ"] <- Alien2$age[Alien2$sex == "ZZ"] + 20
Alien2$age <- Alien2$age + 0.5*Alien2$bh_trips
Alien2$age <- Alien2$age + rnorm(nrow(Alien2), mean = 0, sd = 4)
Alien2
```


## A design matrix with a factor

```{r factor 1}
model.matrix(object =  ~ planet, data = Alien2)
```

<br>

This parametrization of factors such as `planet` follows the so-called matrix of contrasts called "treatment".

It is the easiest one to interpret and the one use by default in R (but not in all software).


## Interpreting the treatment contrasts

```{r treatment contrast}
model.matrix(object =  ~ planet, data = Alien2)
```

* The intercept represents the mean response value for the baseline (here, reference level = ```"Chambr"```).
* ```planetRiplyx``` represents the mean response value for ```"Riplyx"``` minus the mean of the baseline.
* ```planetWickor``` represents the mean response value for ```"Wickor"``` minus the mean of the baseline.

Note: in R the default level of reference is always the first level of a factor (which level is first [can be changed](./Introduction.html#4)).


## A design matrix with a factor

```{r factor 2}
(fit_alien_lm2 <- lm(age ~ planet, data = Alien2))
```


## There are alternative contrasts parametrizations!

```{r sum contrast 1}
model.matrix(object =  ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

<br>

Alternative parametrizations of the contrast matrix can ease convergence in GLM(M) (for continuous variable) and can allow for testing specific hypotheses or fit particular experimental design (for qualitative variables). 

It will not change the predictions, the likelihood, the AIC, ...; only the parameter values and their interpretation!

<!--

## Interpreting the Helmert contrasts

Average of the three levels =
$$
\frac{(1*\text{Int}-1*\text{x1}-1*\text{x2})+(1*\text{Int}+1*\text{x1}-1*\text{x2})+(1*\text{Int}+0*\text{x1}-2*\text{x2})}{3}=\text{Int}
$$
The intercept thus represents the mean of the mean of all groups. (nice!)

You can use the same idea to show that the first coefficient corresponds to the mean of the first 2 groups minus the mean of the first group, that the second coefficient corresponds to the mean of the first 3 groups minus the mean of the first 2 groups, and so forth. (weird ?!)
-->

## The example of "sum contrasts"

```{r contr.sum 1}
model.matrix(object =  ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
\frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{Int}
$$

* The intercept thus represents the mean of the mean responses of all groups.


## The example of "sum contrasts"

```{r contr.sum 2}
model.matrix(object =  ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
(\text{Int} + \text{plt1}) - \frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{plt1}
$$

* The intercept thus represents the mean of the mean responses of all groups.
* `planet1` represents the mean response of the first group minus the intercept.


## The example of "sum contrasts"

```{r contr.sum 3}
model.matrix(object =  ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
(\text{Int} + \text{plt2}) - \frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{plt2}
$$

* The intercept thus represents the mean of the mean responses of all groups.
* `planet1` represents the mean response of the first group minus the intercept.
* `planet2` represents the mean response of the second group minus the intercept.

Note: not much used by R users (because not the default) but useful when there is no clear control.




## Example of estimates under different contrasts

```{r sum contrast panoply}
lm(age ~ planet, data = Alien2, contrasts = list(planet = "contr.treatment"))
lm(age ~ planet, data = Alien2, contrasts = list(planet = "contr.sum"))
```

<!--
## Interim: the product XTX

The product XTX is essential for many computations around linear models.

It represents all pairwise products between the columns of the design matrix.

```{r XTX_explained}
X <- matrix(c(1, 1, 1, 1, 2, 3, 4, 5, 6), byrow = FALSE, ncol = 3)
(XTX_1 <- t(X) %*% X)
XTX_2 <- crossprod(X)

XTX_3 <- matrix(0, nrow = 3, ncol = 3)

for (i in 1:3) for (j in 1:3) XTX_3[i, j] <- sum(X[, i]*X[, j])

identical(XTX_1, XTX_2) & identical(XTX_1, XTX_3)
```

```{r XTX}
crossprod(DM_raw)  ## high values mean that predictors are collinear -> will be hard to fit
zapsmall(crossprod(DM_ortho))  ## 0 means that predictors are orthogonal -> will be easy to fit
```

<br>

It produces a symmetric diagonal matrix. Those are particularly maths friendly.
-->

## A design matrix with interactions with > 1 factors

```{r factorial}
model.matrix(object = ~ planet + sex, data = Alien2)
```
Now the intercept corresponds to observations that have the first level for all factors.


## A design matrix with interactions

```{r pred matrix 4}
model.matrix(object = ~ planet + sex + planet:sex, data = Alien2)
```

<br>

Interactions allow for the consideration of the effect of one predictor dependent on the value taken by other(s). If interactions are not considered, effects are assumed to be independent.


## A design matrix with interactions

```{r pred matrix 5}
model.matrix(object = ~ planet*sex, data = Alien2)
```

The `*` is a shortcut to `+` & `:`


## A design matrix with interactions

Interaction between a quantitative and qualitative predictor:

```{r pred matrix 6}
model.matrix(object = ~ planet * bh_trips, data = Alien2)
```


## A design matrix with interactions

```{r alien interaction}
lm(age ~ planet * bh_trips, data = Alien2)
```

<br>

How do you interpret each parameter estimate?


# Errors and residuals

## The errors

Characteristics:

* one dimension: $n \times 1$ vector
* random variable
* Gaussian (normally distributed), independent (no autocorrelation) and identically distributed (homoscedastic)

<center><font size="8"> $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$</font></center>

<br>

With the covariance matrix $\sigma^2 I$ being the following $n \times n$ matrix:

$$
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2 \\
\end{bmatrix}
$$


## The residuals

There are the estimations for of the errors produced during the fit of the model:

```{r error vs residuals}
Alien$residuals <- residuals(fit_alien_lm)
Alien
```

```{r error vs residals 2}
cor(Alien$residuals, Alien$error)
```

```{r error vs residals 3}
mean(Alien$residuals) ## residuals have a mean of 0 is the sample, while error have a mean 0 in the population
```


# Predictions

## Predictions using the design matrix 

Predictions are the equivalent to fitted values for future or hypothetical observations.

Example: prediction for the size of Aliens having eaten 1, 1.5, 2, 2.5 and 3 humans.

```{r prediction by hand}
(new_design <- cbind(Intercept = 1, humans_eaten = seq(1, 3, by = 0.5)))
new_design %*% coef(fit_alien_lm)
```

## Predictions using `predict()`

The function `predict()` saves you the trouble of building a design matrix by hand:

```{r prediction with predict}
data_for_predict <- data.frame(humans_eaten = seq(1, 3, by = 0.5))
predict(fit_alien_lm, newdata = data_for_predict)
```

<br>

The fact that you provide predictors instead of regressors is particularly useful when you have qualitative variable (cf exercises).

##
<center> <img src="./Extrapolation.png" alt="xkcd" style="width: 300x;"/> </center>


# Summary

## What you need to remember

* how to write a linear model (using simple maths notation and R formulas)
* how to simulate the data according to simple linear models
* some key vocabulary (e.g. response variable, fitted values, residuals...)
* how to understand the meaning of parameters by investigating the design matrix
* how to manipulate design matrices via formula
* how to compute predictions

<br>

<div align="right">
[Exercises](./LM_intro_ex.html)
</div>


# Table of contents

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Fitting procedure](./LM_fitting.html)
* 2.2 [Tests & Intervals](./LM_test_intervals.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions.html)
* 2.4 [Let's practice](./LM_practice.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>
