---
title: "Answers to Exercises: Let's practice more GLMs"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  html_vignette:
    toc: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{3.X Answers to Exercises: Let's practice more GLMs}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(LM2GLMM)
library(car)
knitr::opts_chunk$set(cache = TRUE, fig.align = "center", fig.width = 6, fig.height = 6,
                      cache.path = "./cache_knitr/Exo_GLM_solution/", fig.path = "./fig_knitr/Exo_GLM_solution/")
options(width = 200)
```

## Disclamer

In this vignette I illustrate the key steps required to solve the exercises. By no means I am trying to provide a detailed report of the analysis of each dataset. I am trying to put emphasis on different things you could do in the different exercise. Any final real report would need additional analyses and results would have to be written in good English, not in telegraphic style or lines of codes. I have also written this vignette very quickly, so double check I did not mess anything up...


# Dataset: InsectSprays

## Goal

* Compare the effectiveness of various insecticides on the number of insects in agricultural experimental units.
* What is the efficiency of spray C?
* Compare results between the LM approach (with and without BoxCox transformation) and the GLM approach.

Note: I will not check the assumptions for LM, nor detail the dataset (and plotting variables) since we did it before.


## Fitting the models

We fit the LM:

```{r}
mod_insect <- lm(count ~ spray, data = InsectSprays)
```

We fit the LM on the BoxCox transformed response:

```{r}
InsectSprays$count_bis <- InsectSprays$count + 1
mod_insect_bis <- update(mod_insect, count_bis ~ .)
bc <- car::powerTransform(mod_insect_bis)
InsectSprays$count_bc <- car::bcPower(InsectSprays$count_bis, bc$lambda)
mod_insect_bc <- update(mod_insect_bis, count_bc ~ .)
```

We fit the GLM:

```{r}
mod_insect_glm <- glm(count ~ spray, data = InsectSprays, family = poisson())
```

## Checking the assumptions for GLM

All assumptions about model structure have been checked before (see exercises for LM).

We still have to test for lack of serial auto-correlation and independance:

```{r insect_glm_resid}
plot(residuals(mod_insect_glm, type = "pearson"))
library(DHARMa)
insect_glm_resid <- simulateResiduals(mod_insect_glm, refit = TRUE, n = 1000)
plot(insect_glm_resid)
testUniformity(insect_glm_resid)
plot(insect_glm_resid, asFactor = TRUE)
testTemporalAutocorrelation(insect_glm_resid, time = insect_glm_resid$fitted)
```

All look good. We also have to test for the goodness of fit:

```{r over}
testOverdispersion(insect_glm_resid, alternative = "both")
```

There seem to be slight overdispersion :-(

## Solving the overdispersion issue

Is the overdispersion caused by two many zeros?

```{r}
table(mod_insect_glm$model$count)
testZeroInflation(insect_glm_resid)
```

nope...

Let's try to fit alternative models to account for the overdispersion:

```{r}
mod_insect_glm_quasi <- glm(count ~ spray, data = InsectSprays, family = quasipoisson())
library(MASS)
mod_insect_glm_nb <- glm.nb(count ~ spray, data = InsectSprays)
rbind(deviance(mod_insect_glm),
      deviance(mod_insect_glm_quasi),
      deviance(mod_insect_glm_nb))
```

Note that the prediction are the same:

```{r}
plot(predict(mod_insect_glm, type = "response"), mod_insect_glm$model$count)
abline(0, 1, col = "red")
plot(predict(mod_insect_glm_quasi, type = "response"), mod_insect_glm_quasi$model$count)
abline(0, 1, col = "red")
plot(predict(mod_insect_glm_nb, type = "response"), mod_insect_glm_nb$model$count)
abline(0, 1, col = "red")
```

The only thing that differ is the variance around those predictions.

Note also that the gaussian models have deviance expressed on another scale, which makes the comparison between all models particularly difficult.

```{r}
pred_bc <- ((predict(mod_insect_bc) * bc$lambda) + 1)^(1/bc$lambda) - 1
(deviance_LM_bc <- sum((pred_bc - InsectSprays$count)^2))
deviance(mod_insect)
```

We can compare the poisson model with the negative binomial one very simply:

```{r, message = FALSE}
library(lmtest)
lrtest(mod_insect_glm_nb, mod_insect_glm)
```

Let us stick to the negative binomial model as our best GLM!

## Retesting assumptions on the negative binomial model

```{r insect_glm_nb_resid, warning = FALSE}
insect_glm_nb_resid   <- simulateResiduals(mod_insect_glm_nb, refit = TRUE, n = 1000)
testUniformity(insect_glm_nb_resid)
testTemporalAutocorrelation(insect_glm_nb_resid)
testOverdispersion(insect_glm_nb_resid, alternative = "both")
testZeroInflation(insect_glm_nb_resid)
```

Everything looks very good!

## Tests comparing the fitted models to their respective null models
```{r}
anova(mod_insect, update(mod_insect, . ~ 1))
anova(mod_insect_bc, update(mod_insect_bc, . ~ 1))
anova(mod_insect_glm, update(mod_insect_glm, . ~ 1), test = "LR")
anova(mod_insect_glm_nb, update(mod_insect_glm_nb, . ~ 1))
```

No matter the model the effect of the insecticed is detected!

## Predictions

We now compare the predictions and CI between the different models:

```{r}
data.for.pred <- data.frame(spray = levels(InsectSprays$spray))
pred_lm <- predict(mod_insect, newdata = data.for.pred, interval = "confidence")
pred_bc <- predict(mod_insect_bc, newdata = data.for.pred, interval = "confidence")
unboxcox <- function(x, lambda) (x*lambda + 1)^(1/lambda)
pred_bc_unboxcox <- unboxcox(x = pred_bc, lambda = bc$lambda) - 1  ## we remove one as we added one so to be able to do the BoxCox
pred_glm_eta <- predict(mod_insect_glm, newdata = data.for.pred, se.fit = TRUE)
pred_glm_table <- data.frame(fit = exp(pred_glm_eta$fit))
pred_glm_table$lwr <- exp(pred_glm_eta$fit + qnorm(0.025)*pred_glm_eta$se.fit)
pred_glm_table$upr <- exp(pred_glm_eta$fit + qnorm(0.975)*pred_glm_eta$se.fit)
pred_glm_nb_eta <- predict(mod_insect_glm_nb, newdata = data.for.pred, se.fit = TRUE)
pred_glm_nb_table <- data.frame(fit = spaMM::negbin()$linkinv(pred_glm_eta$fit))
pred_glm_nb_table$lwr <- spaMM::negbin()$linkinv(pred_glm_nb_eta$fit + qnorm(0.025)*pred_glm_nb_eta$se.fit)
pred_glm_nb_table$upr <- spaMM::negbin()$linkinv(pred_glm_nb_eta$fit + qnorm(0.975)*pred_glm_nb_eta$se.fit)
```

Observe carefully the results:

## Plot
```{r}
plot(InsectSprays$count ~  as.numeric(InsectSprays$spray), xlim = c(0.5, 6.5), ylim = c(0, 30),
     ylab = "Predicted number of instects", xlab = "Sprays", axes = FALSE)
axis(2, las = 2)
axis(1, at = 1:6, labels = levels(InsectSprays$spray))
box()

points(pred_lm[, "fit"] ~  I(-0.2 + 1:6), col = "blue", pch = 20)

arrows(x0 = -0.2 + 1:6, x1 = -0.2 + 1:6, y0 = pred_lm[, "lwr"],
       y1 = pred_lm[, "upr"], code = 3, col = "blue", angle = 90, length = 0.1)

points(pred_bc_unboxcox[, "fit"] ~  I(-0.1 + 1:6), col = "green", pch = 20)

arrows(x0 = -0.1 + 1:6, x1 = -0.1 + 1:6, y0 = pred_bc_unboxcox[, "lwr"],
       y1 = pred_bc_unboxcox[, "upr"], code = 3, col = "green", angle = 90, length = 0.1)

points(pred_glm_table[, "fit"] ~  I(0.1 + 1:6), col = "red", pch = 20)

arrows(x0 = 0.1 + 1:6, x1 = 0.1 + 1:6, y0 = pred_glm_table[, "lwr"],
       y1 = pred_glm_table[, "upr"], code = 3, col = "red", angle = 90, length = 0.1)

points(pred_glm_nb_table[, "fit"] ~  I(0.2 + 1:6), col = "orange", pch = 20)

arrows(x0 = 0.2 + 1:6, x1 = 0.2 + 1:6, y0 = pred_glm_nb_table[, "lwr"],
       y1 = pred_glm_table[, "upr"], code = 3, col = "orange", angle = 90, length = 0.1)

legend("top", fill = c("blue", "green", "red", "orange"), legend = c("LM", "LM + BoxCox", "GLM Poisson", "GLM Negative Binomial"), bty = "n")
```

## Testing properly if BoxCox model is better than NegBin one

```{r boot param, warning = FALSE}
set.seed(1L)
logLik_diff_H0 <- replicate(1000, {
  new.y <- round(((simulate(mod_insect_bc)[, 1] * bc$lambda) + 1)^(1/bc$lambda) - 1)
  new.y_plus <- new.y + abs(min(new.y)) - min(new.y + abs(min(new.y)))
  new.y_plus1 <- new.y_plus + 1
  mod_insect_bis <- lm(new.y_plus1 ~ spray, data = InsectSprays)
  bc_new <- car::powerTransform(mod_insect_bis)
  new.y_plus1_bc <- car::bcPower(new.y_plus1, bc_new$lambda)
  mod_insect_bc_new <- lm(new.y_plus1_bc ~ spray, data = InsectSprays)
  mod_insect_glm_nb_new <- glm.nb(new.y_plus ~ spray, data = InsectSprays)
  logLik(mod_insect_bc_new)[[1]] - logLik(mod_insect_glm_nb_new)[[1]]
  })
(logLik_diff_obs <- logLik(mod_insect_bc)[[1]] - logLik(mod_insect_glm_nb)[[1]])
(sum(logLik_diff_obs < (logLik_diff_H0)) + 1) / (length(logLik_diff_H0) + 1)
hist(logLik_diff_H0, nclass = 10)
abline(v = logLik_diff_obs, col = "red", lwd = 2, lty = 2)
```


## Testing properly if NegBin model is better than BoxCox one

```{r boot param 2, warning = FALSE}
set.seed(1L)
logLik_diff_H0 <- replicate(1000, {
  new.y <- simulate(mod_insect_glm_nb)[, 1]
  new.y_plus <- new.y + abs(min(new.y)) - min(new.y + abs(min(new.y)))
  new.y_plus1 <- new.y_plus + 1
  mod_insect_bis <- lm(new.y_plus1 ~ spray, data = InsectSprays)
  bc_new <- car::powerTransform(mod_insect_bis)
  new.y_plus1_bc <- car::bcPower(new.y_plus1, bc_new$lambda)
  mod_insect_bc_new <- lm(new.y_plus1_bc ~ spray, data = InsectSprays)
  mod_insect_glm_nb_new <- glm.nb(new.y ~ spray, data = InsectSprays)
  logLik(mod_insect_bc_new)[[1]] - logLik(mod_insect_glm_nb_new)[[1]]
  })
(logLik_diff_obs <- logLik(mod_insect_bc)[[1]] - logLik(mod_insect_glm_nb)[[1]])
(sum(logLik_diff_obs < (logLik_diff_H0)) + 1) / (length(logLik_diff_H0) + 1)
hist(logLik_diff_H0, nclass = 10)
abline(v = logLik_diff_obs, col = "red", lwd = 2, lty = 2)
```

The two models do not differ, we could choose either!

This is not a general results. On other data LM on BoxCoxed data and a negative binomial fit of the data may greatly differ!!!

<br>


# Dataset: LM2GLMM::Surprise

## Goal

* Do children value more the type of the present, the cost of the present, or both?

## Solution

Try figuring out this one on your own! It's all in the title.

<br>

# Dataset: esoph

## Goal

* Find out whether you should rather limit alcohol consumption or tabacco consumption in order to avoid developing an oesophageal cancer?


## Exploration of the data

```{r}
esoph
str(esoph)
```

The factors are ordered which will select for different default contrast, so we set them as unordered:

```{r}
esoph$agegp_f <- factor(esoph$agegp, ordered = FALSE)
esoph$alcgp_f <- factor(esoph$alcgp, ordered = FALSE)
esoph$tobgp_f <- factor(esoph$tobgp, ordered = FALSE)
```


```{r}
summary(esoph)
esoph$cancer_freq <- esoph$ncases / (esoph$ncases + esoph$ncontrols)
esoph$cancer_N <- esoph$ncases + esoph$ncontrols
summary(esoph$cancer_freq) ## proportion of cancer
table(esoph$alcgp_f, esoph$tobgp_f)
table(esoph$alcgp_f, esoph$tobgp_f, esoph$agegp_f)
```

We plot the data:

```{r}
coplot(cancer_freq ~ tobgp_f | agegp_f, data = esoph)
coplot(cancer_freq ~ alcgp_f | agegp_f, data = esoph)
```

## We fit the models

```{r}
mod_esoph_logit <- glm(cbind(ncases, ncontrols) ~ agegp_f + alcgp_f + tobgp_f, data = esoph,
                       family = binomial(link = "logit"))
mod_esoph_cauchit <- glm(cbind(ncases, ncontrols) ~ agegp_f + alcgp_f + tobgp_f, data = esoph,
                         family = binomial(link = "cauchit"))
mod_esoph_probit <- glm(cbind(ncases, ncontrols) ~ agegp_f + alcgp_f + tobgp_f, data = esoph,
                        family = binomial(link = "probit"))
AIC(mod_esoph_logit)
AIC(mod_esoph_cauchit)
AIC(mod_esoph_probit)
```

The probit model fits the data best.

## Comparison to null model

```{r}
mod_esoph_probit_H0 <- glm(cbind(ncases, ncontrols) ~ 1, data = esoph,
                           family = binomial(link = "probit"))
anova(mod_esoph_probit, mod_esoph_probit_H0, test = "LR")
```

The model is much better than a null model!

## Testing the model assumptions

```{r resid_esoph_probit}
esoph_probit_resid <- simulateResiduals(mod_esoph_probit, refit = TRUE, n = 1000)
plot(esoph_probit_resid)
testUniformity(esoph_probit_resid)
testTemporalAutocorrelation(esoph_probit_resid, time = 1:nrow(esoph))
testTemporalAutocorrelation(esoph_probit_resid, time = mod_esoph_probit$fitted.values)
testZeroInflation(esoph_probit_resid)
testOverdispersion(esoph_probit_resid, alternative = "both")
```

## Testing the model effects

```{r}
Anova(mod_esoph_probit)
```

## Plotting the effects

```{r}
library(effects)
plot(allEffects(mod_esoph_probit))
plot(allEffects(mod_esoph_probit, transformation = list(link = NULL, inverse = NULL)))
```

```{r}
library(visreg)
visreg(mod_esoph_probit, scale = "response")
visreg2d(mod_esoph_probit, "alcgp_f", "tobgp_f", plot.type = "image", scale = "response")
visreg2d(mod_esoph_probit, "alcgp_f", "tobgp_f", plot.type = "persp", scale = "response")
```

Note: the two packages differ in how they compute predictions. The package effects seems to here compute the average effect of the non focal factor by weighing them by their sample size. The package visreg seems to here select the level with the highest number of observation for the non focal factor. I am not sure what it does when there are equalities in sample size... Doing predictions yourself is perhaps the easiest way to know what is actually happening!

## Predictions by hand

Let us compare extreme situations

```{r}
newdata_esoph_young <- expand.grid(agegp_f = c("25-34"),
                                   alcgp_f = c("0-39g/day", "120+"),
                                   tobgp_f = c("0-9g/day", "30+"))
pred_esoph_young <- predict(mod_esoph_probit, newdata = newdata_esoph_young, type = "link", se.fit = TRUE)
newdata_esoph_young$pred <- binomial(link = "probit")$linkinv(pred_esoph_young$fit)
newdata_esoph_young$lwr <- binomial(link = "probit")$linkinv(pred_esoph_young$fit + qnorm(0.025)*pred_esoph_young$se.fit)
newdata_esoph_young$upr <- binomial(link = "probit")$linkinv(pred_esoph_young$fit + qnorm(0.975)*pred_esoph_young$se.fit)
newdata_esoph_young
```

```{r}
newdata_esoph_old <- expand.grid(agegp_f = c("75+"),
                                 alcgp_f = c("0-39g/day", "120+"),
                                 tobgp_f = c("0-9g/day", "30+"))
pred_esoph_old <- predict(mod_esoph_probit, newdata = newdata_esoph_old, type = "link", se.fit = TRUE)
newdata_esoph_old$pred <- binomial(link = "probit")$linkinv(pred_esoph_old$fit)
newdata_esoph_old$lwr <- binomial(link = "probit")$linkinv(pred_esoph_old$fit + qnorm(0.025)*pred_esoph_old$se.fit)
newdata_esoph_old$upr <- binomial(link = "probit")$linkinv(pred_esoph_old$fit + qnorm(0.975)*pred_esoph_old$se.fit)
newdata_esoph_old
```

So alcool seems to be a bit worse than smoking but we are extrapolating a bit since very few individuals correspond to the predicted conditions! (see ```table()``` call above)

One thing is clear, not smoking and not drinking seems to be the way to avoid these cancers.

## Trying to fit the interaction to answer our question

```{r}
esoph$drink_smoke <- factor(paste(esoph$alcgp_f, esoph$tobgp_f, sep = "_"))
mod_esoph_probit2 <- glm(cbind(ncases, ncontrols) ~ agegp_f + drink_smoke, data = esoph,
                        family = binomial(link = "probit"))
Anova(mod_esoph_probit2)
summary(mod_esoph_probit2)
```

Ideally we should again check all assumptions (you should do it!).

Let us do the minimum here:

```{r mod_esoph_probit2 resid, warning = FALSE}
plot(simulateResiduals(mod_esoph_probit2, refit = TRUE, n = 1000))
```

Let us now compare the alcool and drink effects.

```{r, message = FALSE}
library(multcomp)
table(esoph$drink_smoke)
contr_base <- matrix(c(1, rep(0, 15)), nrow = 1) ## define the desing matrix for the posthoc test
contr <- contr_base
contr[levels(esoph$drink_smoke) == "120+_0-9g/day"] <- 1
contr[levels(esoph$drink_smoke) == "0-39g/day_30+"] <- -1
colnames(contr) <- levels(esoph$drink_smoke)
rownames(contr) <- "alcool - smoke"
contr
posthoc <- glht(mod_esoph_probit2, linfct = mcp(drink_smoke = contr))
par(oma = c(1, 5, 1, 1))
plot(posthoc)
confint(posthoc)
summary(posthoc, test = Chisqtest()) ## choose tests the better suited to your case
```

We have compared two different situations, using a posthoc approach.

## Alternative: effects considered as linear

```{r}
esoph$alc_num <- unclass(esoph$alcgp)
esoph$tob_num <- unclass(esoph$tobgp)
mod_esoph_probit3 <- glm(cbind(ncases, ncontrols) ~ agegp_f + alc_num + tob_num, data = esoph,
                         family = binomial(link = "probit"))
plot(simulateResiduals(mod_esoph_probit3, refit = TRUE, n = 1000))
crPlots(mod_esoph_probit3)
AIC(mod_esoph_probit)
AIC(mod_esoph_probit2)
AIC(mod_esoph_probit3)
confint(mod_esoph_probit3)
```

This last model seems quite good and it allows to answer our question!

## More?

* One could also do a model with only age and tabaco, one with only age and alcool and compare those by parametric bootstrap to get a p-value. But let us stop here for this exercise.
* One could explore interactions.

<br>


# Dataset: TitanicSurvival

## Goal

* Find out who mostly survived and who mostly died during the Titanic disaster of 1912.
* Compute the odds-ratio for survival between the two most contrasting groups of individuals. 

## Exploration of the data
```{r}
head(TitanicSurvival)
str(TitanicSurvival)
summary(TitanicSurvival)
with(data = TitanicSurvival,
     tapply(survived, sex, function(x) mean(x == "yes")))
with(data = TitanicSurvival,
     tapply(survived, passengerClass, function(x) mean(x == "yes")))
with(data = TitanicSurvival, table(sex, passengerClass))
```

## Visualization of the data
```{r}
hist(TitanicSurvival$age)
library(car)
scatterplot(survived == "yes" ~ age, data = TitanicSurvival)
```

The effect of age does not seem linear...

## We recode some variables
```{r}
TitanicSurvival$surv <-  TitanicSurvival$survived == "yes"
TitanicSurvival$age_f <- cut(TitanicSurvival$age, breaks = c(0, 10, 25, 35, 50, 81), include.lowest = TRUE)
table(TitanicSurvival$age_f)
```


## Fitting the models

We fit a serie of models with different links and considering age as factor or continuous:

```{r}
mod_logit <- glm(surv ~ age_f+passengerClass+sex, data = TitanicSurvival, family = binomial(link = "logit"))
mod_probit <- glm(surv ~ age_f+passengerClass+sex, data = TitanicSurvival, family = binomial(link = "probit"))
mod_cauchit <- glm(surv ~ age_f+passengerClass+sex, data = TitanicSurvival, family = binomial(link = "cauchit"))

mod_logit2 <- glm(surv ~ age+passengerClass+sex, data = TitanicSurvival,
                 family = binomial(link = "logit"))
mod_probit2 <- glm(surv ~ age+passengerClass+sex, data = TitanicSurvival,
                  family = binomial(link = "probit"))
mod_cauchit2 <- glm(surv ~ age+passengerClass+sex, data = TitanicSurvival,
                   family = binomial(link = "cauchit"))

rbind(AIC(mod_logit),
      AIC(mod_probit),
      AIC(mod_cauchit),
      AIC(mod_logit2),
      AIC(mod_probit2),
      AIC(mod_cauchit2))
```

The cauchit model with age as a factor provides a much better fit than other models.


## Checking the assumptions

```{r resid_Titanic}
library(DHARMa)
resid_Titanic <- simulateResiduals(mod_cauchit, refit = TRUE, n = 1000)
plot(resid_Titanic)
testTemporalAutocorrelation(resid_Titanic, time = mod_cauchit$fitted.values)
testTemporalAutocorrelation(resid_Titanic, time = 1:nrow(mod_cauchit$model))
```

No need to test for overdispersion in a binary model!


## We look at predictions

```{r}
data.for.pred <- with(data = TitanicSurvival,
                 expand.grid(age_f = levels(age_f), passengerClass = levels(passengerClass), sex = levels(sex)))
pred <- predict(mod_cauchit, newdata = data.for.pred, type = "link")
data.to.plot <- cbind(data.for.pred, pred = pred)
coplot(pred ~ unclass(age_f) | passengerClass + sex, data = data.to.plot)
library(lattice) ## same, more fancy
xyplot(pred ~ unclass(age_f) | passengerClass + sex, data = data.to.plot)
```

## GAM?

Another way to cope with the non-linearity of the age effect would be to do a GAM model:

```{r}
library(mgcv)
mod_cauchit_gam <- gam(surv ~ s(age) + passengerClass + sex, data = TitanicSurvival, family = binomial(link = "cauchit"))
AIC(mod_cauchit_gam)
```

The model is a bit better, we could use it.

## GAM model assumptions:

```{r cauchit_gam_resid}
cauchit_gam_resid <- simulateResiduals(mod_cauchit_gam)
plot(cauchit_gam_resid)
scatter.smooth(mod_cauchit_gam$model$age, cauchit_gam_resid$scaledResiduals, col = 2) ## linearity
```

## Tests
```{r}
Anova(mod_cauchit)
anova(mod_cauchit_gam)
```

## Predictions

```{r}
data.for.pred2 <- with(data = TitanicSurvival,
                      expand.grid(age_f = levels(TitanicSurvival$age_f), passengerClass = levels(passengerClass), sex = levels(sex)))
data.for.pred2$age <- NA
data.for.pred2$age[data.for.pred2$age_f == "[0,10]"] <- (10 - 0)/2
data.for.pred2$age[data.for.pred2$age_f == "(10,25]"] <- (25 + 10)/2
data.for.pred2$age[data.for.pred2$age_f == "(25,35]"] <- (25 + 35)/2
data.for.pred2$age[data.for.pred2$age_f == "(35,50]"] <- (35 + 50)/2
data.for.pred2$age[data.for.pred2$age_f == "(50,81]"] <- (50 + 81)/2

pred_age_f <- predict(mod_cauchit, newdata = data.for.pred2, type = "response")
pred_age_gam <- predict(mod_cauchit_gam, newdata = data.for.pred2, type = "response")
data.to.plot2 <- cbind(data.for.pred2, pred_age_f = pred_age_f, pred_age_gam = pred_age_gam)

pred_f_1 <- subset(data.to.plot2, sex == "female" & passengerClass == "1st")
pred_f_2 <- subset(data.to.plot2, sex == "female" & passengerClass == "2nd")
pred_f_3 <- subset(data.to.plot2, sex == "female" & passengerClass == "3rd")

pred_m_1 <- subset(data.to.plot2, sex == "male" & passengerClass == "1st")
pred_m_2 <- subset(data.to.plot2, sex == "male" & passengerClass == "2nd")
pred_m_3 <- subset(data.to.plot2, sex == "male" & passengerClass == "3rd")
```

```{r}
par(mfrow = c(1, 2))
plot(pred_f_1$pred_age_f ~ pred_f_1$age, type = "o", lwd = 2, ylim = c(0, 1.4),
    ylab = "Prob. Survival", xlab = "Age", main = "Age as factor")
points(pred_f_2$pred_age_f ~ pred_f_2$age, type = "o", lwd = 2, col = 2)
points(pred_f_3$pred_age_f ~ pred_f_3$age, type = "o", lwd = 2, col = 3)

points(pred_m_1$pred_age_f ~ pred_m_1$age, type = "o", lwd = 2, col = 1, lty = 2)
points(pred_m_2$pred_age_f ~ pred_m_2$age, type = "o", lwd = 2, col = 2, lty = 2)
points(pred_m_3$pred_age_f ~ pred_m_3$age, type = "o", lwd = 2, col = 3, lty = 2)

legend("top", lty = c(1, 1, 1, 2, 2, 2), col = c(1:3, 1:3),
       legend = c("f_1", "f_2", "f_3", "m_1", "m_2", "m_3"),
       lwd = rep(2, 6), cex = 0.6)

plot(pred_f_1$pred_age_gam ~ pred_f_2$age, type = "o", ylim = c(0, 1.4),
    ylab = "Prob. Survival", xlab = "Age", main = "Age as GAM")
points(pred_f_2$pred_age_gam ~ pred_f_2$age, type = "o", col = 2)
points(pred_f_3$pred_age_gam ~ pred_f_3$age, type = "o", col = 3)

points(pred_m_1$pred_age_gam ~ pred_m_1$age, type = "o", col = 1, lty = 2)
points(pred_m_2$pred_age_gam ~ pred_m_2$age, type = "o", col = 2, lty = 2)
points(pred_m_3$pred_age_gam ~ pred_m_3$age, type = "o", col = 3, lty = 2)

legend("top", lty = c(1, 1, 1, 2, 2, 2), col = c(1:3, 1:3),
       legend = c("f_1", "f_2", "f_3", "m_1", "m_2", "m_3"),
       lwd = rep(1, 6), cex = 0.6)
```


## Odds-ratio

As the model is not fitted with link = logit, the odd ratio are not constant with age...

```{r}
p1 <- pred_f_1[pred_f_1$age == 5, "pred_age_f"]
p2 <- pred_m_3[pred_m_3$age == 5, "pred_age_f"]
(p1/(1 - p1)) / (p2/(1 - p2))

p1 <- pred_f_1[pred_f_1$age == 65.5, "pred_age_f"]
p2 <- pred_m_3[pred_m_3$age == 65.5, "pred_age_f"]
(p1/(1 - p1)) / (p2/(1 - p2))

p1 <- pred_f_1[pred_f_1$age == 5, "pred_age_gam"]
p2 <- pred_m_3[pred_m_3$age == 5, "pred_age_gam"]
(p1/(1 - p1)) / (p2/(1 - p2))

p1 <- pred_f_1[pred_f_1$age == 65.5, "pred_age_gam"]
p2 <- pred_m_3[pred_m_3$age == 65.5, "pred_age_gam"]
(p1/(1 - p1)) / (p2/(1 - p2))

```

Note: the differences between the two models are at the extrems for which have few observations.

```{r}
with(subset(TitanicSurvival, age_f == "[0,10]"), table(passengerClass, sex))
with(subset(TitanicSurvival, age_f == "(50,81]"), table(passengerClass, sex))
```

<br>


# Dataset: LM2GLMM::Challenger

### Goal

* What was the probability of O-ring failure when the space shuttle Challenger took off on the 28th of January 1986 by 31 degrees F?


## Let's explore the data

```{r}
str(Challenger)
summary(Challenger)
table(nb_orings = Challenger$oring_tot, nb_pb_orings = Challenger$oring_dt)
```

## Let's visualize the data

```{r}
plot(jitter(oring_dt, factor = 0.3) ~ temp, data = Challenger)
```
Notice the user of jitter!

## We recode the data

```{r}
Challenger$oring_fine <- Challenger$oring_tot - Challenger$oring_dt
```

## We fit the models

```{r}
mod_binom_logit <- glm(cbind(oring_fine, oring_dt) ~ temp, data = Challenger,
                       family = binomial(link = "logit"))
mod_binom_probit <- glm(cbind(oring_fine, oring_dt) ~ temp, data = Challenger,
                        family = binomial(link = "probit"))
mod_binom_cauchit <- glm(cbind(oring_fine, oring_dt) ~ temp, data = Challenger,
                         family = binomial(link = "cauchit"))
AIC(mod_binom_logit)
AIC(mod_binom_probit)
AIC(mod_binom_cauchit)
```

We could choose the logit or the probit model. It would be easier with the logit to interprete the estimates but we do not need this here, so let us consider the probit model since it is best.

## Let's check assumptions
```{r}
r_probit <- simulateResiduals(mod_binom_probit, n = 1000)
plot(r_probit)
r_probit2 <- simulateResiduals(mod_binom_probit, n = 1000, seed = 2)
plot(r_probit2) ## replicate with other seed
testUniformity(r_probit)
testTemporalAutocorrelation(r_probit, time = 1:nrow(Challenger))
testTemporalAutocorrelation(r_probit, time = mod_binom_probit$fitted.values)
testTemporalAutocorrelation(r_probit, time = Challenger$temp)
testZeroInflation(r_probit)  ## does not work (fix in progress)
table(Challenger$oring_fine == 0)
```

Let us try to check the zero-inflation differently:

```{r}
test <- replicate(1000, sum(simulate(mod_binom_probit, 1)[[1]][, 2] < 1))
hist(test)
abline(v = sum(Challenger$oring_dt < 1), col = "red", lwd = 2)
```

## Tests

```{r}
Anova(mod_binom_probit)
summary(mod_binom_probit)
```

## Predictions

```{r}
pred <- data.frame(temp = 31:81)
pred_probit <- predict(mod_binom_probit, newdata = pred, se.fit = TRUE)
pred$pred <- binomial(link = "probit")$linkinv(pred_probit$fit)
pred$lwr <- binomial(link = "probit")$linkinv(
  pred_probit$fit + qnorm(0.025) * pred_probit$se.fit)
pred$upr <- binomial(link = "probit")$linkinv(
  pred_probit$fit + qnorm(0.975) * pred_probit$se.fit)

plot(1 - pred$pred ~ pred$temp, type = "l",
     ylim = c(0, 1), ylab = "Probability of failure",
     xlab = "Temp (F)")
points(1 - pred$lwr ~ pred$temp, type = "l", lty = 2)
points(1 - pred$upr ~ pred$temp, type = "l", lty = 2)
points(oring_dt / oring_tot ~ temp, data = Challenger, col = "red")

## Predict logit
pred2 <- data.frame(temp = 31:81)
pred_logit <- predict(mod_binom_logit, newdata = pred, se.fit = TRUE)
pred2$pred <- binomial(link = "logit")$linkinv(pred_logit$fit)
pred2$lwr <- binomial(link = "logit")$linkinv(
  pred_logit$fit + qnorm(0.025) * pred_logit$se.fit)
pred2$upr <- binomial(link = "logit")$linkinv(
  pred_logit$fit + qnorm(0.975) * pred_logit$se.fit)

points(1 - pred2$pred ~ pred2$temp, type = "l", col = 3)
points(1 - pred2$lwr ~ pred2$temp, type = "l", lty = 2, col = 3)
points(1 - pred2$upr ~ pred2$temp, type = "l", lty = 2, col = 3)
```

## More

Since a single failure could be fatal, perhaps a binary model would make more sense!

You could try to replicate the original study that excluded all the OK flights!

<br>


# Dataset: LM2GLMM::UK

## Goal

* Try to identify the influential determinants of the smoking behaviour of children.
* Try to identify the influential determinants of bronchitis in children.
* Try to identify the influential determinants of the variable ```backward```.

## Smoking behaviour

### Looking at the data

```{r}
str(UK)
lapply(UK, summary)
UK$cigarettes_kid_bin <- UK$cigarettes_kid != "Never" 
table(UK$cigarettes_kid_bin, UK$cigarettes_friends)
scatterplot(cigarettes_kid_bin ~ age + sex, data = UK)
scatterplot(cigarettes_kid_bin ~ cigarettes + sex, data = UK)
scatter.smooth(UK$age, UK$cigarettes, lpars = list(col = "red", lwd = 2))
```

### Fiting the model

```{r}
mod_logit <- glm(cigarettes_kid_bin ~ sex*(cigarettes + age + cigarettes_friends),
                 family = binomial(link = "logit"), data = UK)
mod_probit <- glm(cigarettes_kid_bin ~ sex*(cigarettes + age + cigarettes_friends),
                  family = binomial(link = "probit"), data = UK)
mod_cauchit <- glm(cigarettes_kid_bin ~ sex*(cigarettes + age + cigarettes_friends),
                   family = binomial(link = "cauchit"), data = UK)
c(AIC(mod_logit), AIC(mod_probit), AIC(mod_cauchit))
```

### Comparison to null model

```{r}
anova(mod_logit, update(mod_logit, . ~ 1, data = mod_logit$model), test = "LR")
```

### Test of the model assumptions

```{r}
resid_mod_logit <- simulateResiduals(mod_logit, refit = TRUE, n = 1000) ## slow
plot(resid_mod_logit)
testUniformity(resid_mod_logit)
testTemporalAutocorrelation(resid_mod_logit, time = mod_logit$fitted.values)
testTemporalAutocorrelation(resid_mod_logit, time = 1:nrow(mod_logit$model))
```

### Tests

```{r}
Anova(mod_logit)
```

### Predictions

Using the nice feature "cond" from visreg:

```{r}
visreg(mod_logit, "cigarettes", by = "sex", scale = "response",
       cond = list(cigarettes_friends = "None of them", age = mean(mod_logit$model$age)))
visreg(mod_logit, "cigarettes", by = "sex", scale = "response",
       cond = list(cigarettes_friends = "Most of them", age = mean(mod_logit$model$age)))
visreg(mod_logit, "age", by = "sex", scale = "response",
       cond = list(cigarettes = 0, cigarettes_friends = "None of them"))
visreg(mod_logit, "cigarettes_friends", by = "sex", scale = "response",
       cond = list(cigarettes = 0, age = mean(mod_logit$model$age)))
```

<br>

# Dataset: HSE98women, exercise menopause

## Goal

* Study the influences of age, body mass index and smoking status upon the probability of being menopaused
* Estimate the proportion of menopaused women in the population, depending on their age (40, 45, 50, 55, 60 yrs)

Note: here, for the sake of simplicity we simply define menopause as the lack of periods in old ages.


## Data preparation and exploration

We create the response variable:
```{r}
HSE98women$menopause <-  HSE98women$period == FALSE
```

We fit a first model to help us easily exploring the data used for fitting and not all data (there are many missing values in the dataset).

```{r}
mod_menop_logit <- glm(menopause ~ age + bmi + smoked, data = HSE98women, family = binomial())
nrow(HSE98women) ## original number of observations
nrow(mod_menop_logit$model)  ## number of observations considered in the model
```

We compare the variables before and after filtering to see if any obvious biais is introduced by the observations we discared:
```{r}
summary(HSE98women[, c("menopause", "age", "bmi", "smoked")])
summary(mod_menop_logit$model)
```

The only big discrepancy between the two datasets is that the minimal age is now 18 yrs old while the original dataset also contains children. That suggests that girls too young to have periods are not being considered, which will greatly help us to model menopause.

Let's check the correlation between our two continuous predictor:

```{r}
cor.test(HSE98women$age, HSE98women$bmi)
cor.test(mod_menop_logit$model$age, mod_menop_logit$model$bmi)
```

The correlation is not very high, especially in the dataset that is used for the fit, so multicollinearity should not be an issue.

We visualise the relationship between the predictor and the response:
```{r}
library(car)
mod_menop_logit$model$smoked <- as.factor(mod_menop_logit$model$smoked) ## patch to solve bug in car 3.0
scatterplot(menopause ~ age + smoked, data = mod_menop_logit$model)
scatterplot(menopause ~ bmi + smoked, data = mod_menop_logit$model)
```
There don't seem to be any obvious interaction between the age and smoked or between BMI and smoked, so we won't consider any. We will however consider that the effect of age may depend on the BMI.


## Fitting the models

```{r}
mod_menop_logit   <- glm(menopause ~ age * bmi + smoked, data = HSE98women, family = binomial())
mod_menop_probit  <- glm(menopause ~ age * bmi + smoked, data = HSE98women, family = binomial(link = "probit"))
mod_menop_cauchit <- glm(menopause ~ age * bmi + smoked, data = HSE98women, family = binomial(link = "cauchit"))
```

Because it is very difficult to explore the shape of the relationship between the age and bmi upon the probability of being menopaused, we will also fit general additive models (GAM), which allows parameters to vary along the predictors.

```{r, message=FALSE}
library(mgcv)
mod_menop_logit_gam   <- gam(menopause ~ s(age, bmi) + smoked, data = HSE98women, family = binomial())
mod_menop_probit_gam  <- gam(menopause ~ s(age, bmi) + smoked, data = HSE98women, family = binomial(link = "probit"))
mod_menop_cauchit_gam <- gam(menopause ~ s(age, bmi) + smoked, data = HSE98women, family = binomial(link = "cauchit"))
```

```{r}
cbind(logit = AIC(mod_menop_logit),
      probit = AIC(mod_menop_probit),
      cauchit = AIC(mod_menop_cauchit),
      logit_GAM = AIC(mod_menop_logit_gam),
      probit_GAM = AIC(mod_menop_probit_gam),
      cauchit_GAM = AIC(mod_menop_cauchit_gam)
      )
```

The GAM logit model seems to be the best model, which will prevent us to express simply the effect of coefficients compared to a usual GLM. That's too bad but as the model is much better, we will go for it.

## Tests

We are going to test for the effect of the interaction by Likelihood ratio test:
```{r}
mod_menop_logit_gam_no_int <- gam(menopause ~ s(age) + s(bmi) + smoked, data = HSE98women, family = binomial())
LR <- -2 * (logLik(mod_menop_logit_gam_no_int)[[1]] - logLik(mod_menop_logit_gam)[[1]])
DF <- (mod_menop_logit_gam$df.null - mod_menop_logit_gam$df.residual) - (mod_menop_logit_gam_no_int$df.null - mod_menop_logit_gam_no_int$df.residual)
(PV <- pchisq(LR, DF, lower.tail = FALSE))
```

Note: you can also use ```lmtest::lrtest()``` but it rounds the DF :-/

The interaction between age and BMI is not significant (LRT, LR = ```r round(LR, 1)```, df = ```r round(DF, 2)```, p-value = ```r round(PV, 2)```).

Let's now test the effect of BMI by Likelihood ratio test:

```{r}
mod_menop_logit_gam_no_bmi <- gam(menopause ~ s(age) + smoked, data = HSE98women, family = binomial())
LR <- -2 * (logLik(mod_menop_logit_gam_no_bmi)[[1]] - logLik(mod_menop_logit_gam)[[1]])
DF <- (mod_menop_logit_gam$df.null - mod_menop_logit_gam$df.residual) - (mod_menop_logit_gam_no_bmi$df.null - mod_menop_logit_gam_no_bmi$df.residual)
(PV <- pchisq(LR, DF, lower.tail = FALSE))
```

The effect of BMI is strongly significant (LRT, LR = ```r round(LR, 1)```, df = ```r round(DF, 2)```, p-value < 0.001).

Note that models with GAM have non integer degrees of freedom. This is normal.

Let's now test the effect of age by Likelihood ratio test:

```{r}
mod_menop_logit_gam_no_age <- gam(menopause ~ s(bmi) + smoked, data = HSE98women, family = binomial())
LR <- -2 * (logLik(mod_menop_logit_gam_no_age)[[1]] - logLik(mod_menop_logit_gam)[[1]])
DF <- (mod_menop_logit_gam$df.null - mod_menop_logit_gam$df.residual) - (mod_menop_logit_gam_no_age$df.null - mod_menop_logit_gam_no_age$df.residual)
(PV <- pchisq(LR, DF, lower.tail = FALSE))
```

The effect of age is strongly significant (LRT, LR = ```r round(LR, 1)```, df = ```r round(DF, 2)```, p-value < 0.001).

Let's now test the effect of smoking by Likelihood ratio test:

```{r}
mod_menop_logit_gam_never_smoked <- gam(menopause ~ s(age, bmi), data = HSE98women, family = binomial())
LR <- -2 * (logLik(mod_menop_logit_gam_never_smoked)[[1]] - logLik(mod_menop_logit_gam)[[1]])
DF <- (mod_menop_logit_gam$df.null - mod_menop_logit_gam$df.residual) - (mod_menop_logit_gam_never_smoked$df.null - mod_menop_logit_gam_never_smoked$df.residual)
(PV <- pchisq(LR, DF, lower.tail = FALSE))
```

The effect of have smooked is also significant (LRT, LR = ```r round(LR, 1)```, df = ```r round(DF, 2)```, p-value < 0.001).


## Predictions

Let's first explore the model output with the pacakge ```visreg```, which also works on GAM models:

```{r}
library(visreg)
visreg(mod_menop_logit_gam)
```

These plots show the patterns identified by the smoothing parameters in the GAM: the effect of age seems to change after 40 yrs and the effect of BMI seem to be changing between 23 and 32 yrs. It is difficult to account for such threshold effects by polynoms, so we will have to stick to GAM here...

Le't us do the same plots on the scale of the response variable:
```{r}
visreg(mod_menop_logit_gam, scale = "response")
visreg2d(mod_menop_logit_gam, "age", "bmi", scale = "response")
#visreg2d(mod_menop_logit_gam, "age", "bmi", scale = "response", plot.type = "rgl")
```

We will now plot the predictions for the model with the logit link, the model with the cauchit link and the model with the GAM model with the logit link to explore their differences. We will predict the influence of age and consider the median BMI and smokers.

```{r}
age.for.pred <- seq(min(mod_menop_cauchit_gam$model$age), max(mod_menop_cauchit_gam$model$age), length = 100)
data.for.pred <- expand.grid(
  age = age.for.pred,
  bmi = median(mod_menop_cauchit_gam$model$bmi),
  smoked = TRUE
  )

  gam.p <- predict(mod_menop_logit_gam, newdata = data.for.pred, se.fit = TRUE)
gam.upr <- binomial(link = "logit")$linkinv(gam.p$fit + qnorm(0.975) * gam.p$se.fit)
gam.lwr <- binomial(link = "logit")$linkinv(gam.p$fit + qnorm(0.025) * gam.p$se.fit)
gam.fit <- binomial(link = "logit")$linkinv(gam.p$fit)

cauchit.p <- predict(mod_menop_cauchit, newdata = data.for.pred, se.fit = TRUE)
cauchit.upr <- binomial(link = "cauchit")$linkinv(cauchit.p$fit + qnorm(0.975) * cauchit.p$se.fit)
cauchit.lwr <- binomial(link = "cauchit")$linkinv(cauchit.p$fit + qnorm(0.025) * cauchit.p$se.fit)
cauchit.fit <- binomial(link = "cauchit")$linkinv(cauchit.p$fit)

logit.p <- predict(mod_menop_logit, newdata = data.for.pred, se.fit = TRUE)
logit.upr <- binomial(link = "logit")$linkinv(logit.p$fit + qnorm(0.975) * logit.p$se.fit)
logit.lwr <- binomial(link = "logit")$linkinv(logit.p$fit + qnorm(0.025) * logit.p$se.fit)
logit.fit <- binomial(link = "logit")$linkinv(logit.p$fit)

plot(gam.fit ~ age.for.pred, type = "l", las = 1, ylab = "Probability of being menopaused", xlab = "Age (yrs)", ylim = c(0, 1))
points(gam.upr ~ age.for.pred, type = "l", lty = 2)
points(gam.lwr ~ age.for.pred, type = "l", lty = 2)

points(cauchit.fit ~ age.for.pred, type = "l", lty = 1, col = "green")
points(cauchit.upr ~ age.for.pred, type = "l", lty = 2, col = "green")
points(cauchit.lwr ~ age.for.pred, type = "l", lty = 2, col = "green")

points(logit.fit ~ age.for.pred, type = "l", lty = 1, col = "red")
points(logit.upr ~ age.for.pred, type = "l", lty = 2, col = "red")
points(logit.lwr ~ age.for.pred, type = "l", lty = 2, col = "red")

legend("topleft", fill = c("black", "green", "red"), legend = c("logit GAM", "cauchit", "logit"), bty = "n")
```

Selecting the best model, let's now look at the effect of BMI, still using smokers:

```{r}
data.for.pred <- expand.grid(
  age = age.for.pred,
  bmi = c(18.5, 25, 30),
  smoked = TRUE
  )

gam.skiny.p <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 18.5), se.fit = TRUE)
gam.skiny.upr <- binomial(link = "logit")$linkinv(gam.skiny.p$fit + qnorm(0.975) * gam.skiny.p$se.fit)
gam.skiny.lwr <- binomial(link = "logit")$linkinv(gam.skiny.p$fit + qnorm(0.025) * gam.skiny.p$se.fit)
gam.skiny.fit <- binomial(link = "logit")$linkinv(gam.skiny.p$fit)

gam.overweight.p <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 25), se.fit = TRUE)
gam.overweight.upr <- binomial(link = "logit")$linkinv(gam.overweight.p$fit + qnorm(0.975) * gam.overweight.p$se.fit)
gam.overweight.lwr <- binomial(link = "logit")$linkinv(gam.overweight.p$fit + qnorm(0.025) * gam.overweight.p$se.fit)
gam.overweight.fit <- binomial(link = "logit")$linkinv(gam.overweight.p$fit)

gam.obese.p <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 30), se.fit = TRUE)
gam.obese.upr <- binomial(link = "logit")$linkinv(gam.obese.p$fit + qnorm(0.975) * gam.obese.p$se.fit)
gam.obese.lwr <- binomial(link = "logit")$linkinv(gam.obese.p$fit + qnorm(0.025) * gam.obese.p$se.fit)
gam.obese.fit <- binomial(link = "logit")$linkinv(gam.obese.p$fit)

plot(gam.skiny.fit ~ age.for.pred, type = "l", las = 1, ylab = "Probability of being menopaused", xlab = "Age (yrs)", ylim = c(0, 1))
points(gam.skiny.upr ~ age.for.pred, type = "l", lty = 2)
points(gam.skiny.lwr ~ age.for.pred, type = "l", lty = 2)

points(gam.overweight.fit ~ age.for.pred, type = "l", lty = 1, col = "green")
points(gam.overweight.upr ~ age.for.pred, type = "l", lty = 2, col = "green")
points(gam.overweight.lwr ~ age.for.pred, type = "l", lty = 2, col = "green")

points(gam.obese.fit ~ age.for.pred, type = "l", lty = 1, col = "red")
points(gam.obese.upr ~ age.for.pred, type = "l", lty = 2, col = "red")
points(gam.obese.lwr ~ age.for.pred, type = "l", lty = 2, col = "red")

legend("topleft", fill = c("black", "green", "red"), legend = c("skiny", "overweight", "obese"), bty = "n")
```

We can see on this plot that obesity seems to accelerate the onset of menopause, but it does not seem to make any difference later on.

We will now predict the proportion of women in menopause at 40, 45, 50, 55 and 60 yrs. To illustrate the effect ofthe BMI will will perform prediction for a BMI of 21.75 (middle of normal BMI range according to WHO) and 30 (lower limit for obesity) Kg/(m^2). Again, we will perform the predictions considering smoker since we have data for this category. We will also compute the confidence intervals (prediction intervals make little sense in the context of the prediction of proportion...).

```{r}
data.for.pred <- expand.grid(
  age = c(40, 45, 50, 55, 60),
  bmi = c(21.75, 30),
  smoked = TRUE
  )
pred_normal <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 21.75), se.fit = TRUE)
tab_normal <- cbind(
  predict = plogis(pred_normal$fit),
  lwr.CI = plogis(pred_normal$fit + qnorm(0.025)*pred_normal$se.fit),
  upr.CI = plogis(pred_normal$fit + qnorm(0.975)*pred_normal$se.fit)
)
rownames(tab_normal) <- c(40, 45, 50, 55, 60)
round(tab_normal, 2)
```

This gives the information for individuals for normal BMI. Let's now look at the corresponding proportion for the obeses:

```{r}
pred_obses <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 30), se.fit = TRUE)
tab_obses <- cbind(
  predict = plogis(pred_obses$fit),
  lwr.CI = plogis(pred_obses$fit + qnorm(0.025)*pred_obses$se.fit),
  upr.CI = plogis(pred_obses$fit + qnorm(0.975)*pred_obses$se.fit)
)
rownames(tab_obses) <- c(40, 45, 50, 55, 60)
round(tab_obses, 2)
```

These tables confirm our expectation concerning the influenc of BMI upon the onset of menopause.

For the sake of comparion, let's compare the first table to what we would have predicted using the cauchit model or the logit GLM model:

```{r}
data.for.pred <- expand.grid(age = c(40, 45, 50, 55, 60), bmi = 21.75, smoked = TRUE)
pred_GAM <- predict(mod_menop_logit_gam, newdata = subset(data.for.pred, bmi == 21.75), se.fit = TRUE)
pred_logit <- predict(mod_menop_logit, newdata = subset(data.for.pred, bmi == 21.75), se.fit = TRUE)
pred_cauchit <- predict(mod_menop_cauchit, newdata = subset(data.for.pred, bmi == 21.75), se.fit = TRUE)
tab <- cbind(
  logit_GAM = plogis(pred_GAM$fit),
  logit_GLM = plogis(pred_logit$fit),
  cauchit_GLM = binomial(link = "cauchit")$linkinv(pred_cauchit$fit)
)
rownames(tab) <- c(40, 45, 50, 55, 60)
round(tab, 2)
```

Let's look at the same proportion in the raw data. We cannot be as strict otherwise we won't get much individuals, so let's enlarge a bit the selection and see that we get a few hundred individuals at least:
```{r}
tab <- with(HSE98women,
     rbind(sum(age >= 39 & age <= 41 & bmi > 18.5 & bmi < 25, na.rm = TRUE),
           sum(age >= 44 & age <= 46 & bmi > 18.5 & bmi < 25, na.rm = TRUE),
           sum(age >= 49 & age <= 51 & bmi > 18.5 & bmi < 25, na.rm = TRUE),
           sum(age >= 54 & age <= 56 & bmi > 18.5 & bmi < 25, na.rm = TRUE),
           sum(age >= 59 & age <= 61 & bmi > 18.5 & bmi < 25, na.rm = TRUE)
           )
)
rownames(tab) <- c(40, 45, 50, 55, 60)
tab
```
That seems a good selection process (+/- 1 year and the whole WHO normal BMI category), so let's check the proportion of menopaused individuals in each of these categories (keeping both smokers and non smokers as the effect is rather small, see later):

```{r}
tab <- with(HSE98women, 
     rbind(
       mean(menopause[age >= 39 & age <= 41 & bmi > 18.5 & bmi < 25], na.rm = TRUE),
       mean(menopause[age >= 44 & age <= 46 & bmi > 18.5 & bmi < 25], na.rm = TRUE),
       mean(menopause[age >= 49 & age <= 51 & bmi > 18.5 & bmi < 25], na.rm = TRUE),
       mean(menopause[age >= 54 & age <= 56 & bmi > 18.5 & bmi < 25], na.rm = TRUE),
       mean(menopause[age >= 59 & age <= 61 & bmi > 18.5 & bmi < 25], na.rm = TRUE)
     )
)
rownames(tab) <- c(40, 45, 50, 55, 60)
round(tab, 2)
```

These results are roughly consistent with the predictions from the GAM model. It confirms that the GLM logit and cauchit are underestimating the prevalence of menopause at old age. All models however may overestimate the prevalence of menaupose at young age. Depending on the goal of the study, other link functions, not implemented in R base, could be tried but it is quite advanced so we won't do that.

We can now turn to the exploration of the smoking effect by comparing predictions between smokers and non smokers at median BMI:

Selecting the best model, let's now look at the effect of BMI, still using smokers:

```{r}
data.for.pred.nosmoke <- expand.grid(
  age = age.for.pred,
  bmi = median(mod_menop_cauchit_gam$model$bmi),
  smoked = FALSE
  )

gam.p.nosmoke <- predict(mod_menop_logit_gam, newdata = data.for.pred.nosmoke, se.fit = TRUE)
gam.upr.nosmoke <- binomial(link = "logit")$linkinv(gam.p.nosmoke$fit + qnorm(0.975) * gam.p.nosmoke$se.fit)
gam.lwr.nosmoke <- binomial(link = "logit")$linkinv(gam.p.nosmoke$fit + qnorm(0.025) * gam.p.nosmoke$se.fit)
gam.fit.nosmoke <- binomial(link = "logit")$linkinv(gam.p.nosmoke$fit)


plot(gam.fit ~ age.for.pred, type = "l", las = 1, ylab = "Probability of being menopaused", xlab = "Age (yrs)", ylim = c(0, 1), col = "red")
points(gam.upr ~ age.for.pred, type = "l", lty = 2, col = "red")
points(gam.lwr ~ age.for.pred, type = "l", lty = 2, col = "red")

points(gam.fit.nosmoke ~ age.for.pred, type = "l", col = "green")
points(gam.upr.nosmoke ~ age.for.pred, type = "l", lty = 2, col = "green")
points(gam.lwr.nosmoke ~ age.for.pred, type = "l", lty = 2, col = "green")

legend("topleft", fill = c("green", "red"), legend = c("non-smoker", "smoker"), bty = "n")
```

Although significant, the effect of smoking is rather small. Because the interaction between the smoking status and the BMI was not significant, we won't draw predictions to explore that.

<br>


# Dataset: MASS::mammals

## Goal

* What is the allometric exponent for the growth of brain size with body mass?
* Does this exponent agree with the usual expectation of 2/3?
* How large do we expect a 1kg animal to be?
* Rank organisms by relative brain size (i.e. controlled for body size)

## Explore the data

```{r}
data(mammals, package = "MASS")
head(mammals)
str(mammals)
plot(brain ~ body, data = mammals)
plot(brain ~ body, data = mammals, log = "xy")
plot(log(brain, 10) ~ log(body, 10), data = mammals, pch = ".")
text(log(mammals$body, 10), log(mammals$brain, 10),
     labels = rownames(mammals), cex = 0.8)
```

## Reshaping the data
```{r}
mammals$log10_brain <- log(mammals$brain, base = 10)
mammals$log10_body <-  log(mammals$body, base = 10)
```

## Fitting the model
```{r}
mod <- lm(log10_brain ~ log10_body, data = mammals)
plot(mod)
influence.measures(mod)
plot(mod, which = 4)
summary(mod)
mod_nohuman <- lm(log10_brain ~ log10_body, data = mammals[rownames(mammals)!="Human", ])
plot(mod_nohuman, which = 4)
```

## Comparing to 2/3

```{r}
t <- (mod$coefficients[2] - 2/3) / sqrt(diag(vcov(mod))[2])
2*pt(abs(t), df = mod$df.residual, lower.tail = FALSE)
confint(mod)
mod2 <- lm(log10_brain ~ 1 + offset(2/3*log10_body), data = mammals)
anova(mod, mod2)
```


```{r}
t <- (mod_nohuman$coefficients[2] - 2/3) / sqrt(diag(vcov(mod_nohuman))[2])
2*pt(abs(t), df = mod_nohuman$df.residual, lower.tail = FALSE)
confint(mod_nohuman)
mod2_nohuman <- lm(log10_brain ~ 1 + offset(2/3*log10_body), data = mammals[rownames(mammals)!="Human", ])
anova(mod_nohuman, mod2_nohuman)
```

We can plot the predictions from the models

```{r}
plot(log(brain, 10) ~ log(body, 10), data = mammals, pch = ".")
text(log(mammals$body, 10), log(mammals$brain, 10),
     labels = rownames(mammals), cex = 0.8)
abline(mod, col = "blue", lwd = 2)
abline(mod_nohuman, col = "blue", lwd = 2, lty = 2)
abline(coef(mod2), 2/3, col = "red", lwd = 2)
abline(coef(mod2_nohuman), 2/3, col = "red", lwd = 2, lty = 2)
legend("topleft", lty = c(1, 1, 2, 2), lwd = c(2, 2, 2, 2), col = c("blue", "red", "blue", "red"),
       legend = c("slope estimated", "slope = 2/3", "slope estimated (no human)", "slope = 2/3 (no human)"))
```


## Prediction of the brain size for an animal of 1Kg

```{r}
p <- predict(mod_nohuman, newdata = data.frame(log10_body = log(1, base = 10)))
10^p
```

An animal of 1 Kg is predicted to have a brain size of ```r round(10^p, 2)``` grams.


## Let us rank the organism by brain size controling for body size

```{r}
matrix(names(sort(resid(mod), decreasing = TRUE)), ncol = 1)
```


# Dataset: MASS::Animals

## Goal

* What is the allometric exponent for the growth of brain size with body mass?
* Does this exponent agree with the usual expectation of 2/3?
* How large do we expect a 1kg animal to be?
* Rank organisms by relative brain size (i.e. controlled for body size)

## Explore the data

```{r}
data(Animals, package = "MASS")
head(Animals)
str(Animals)
plot(brain ~ body, data = Animals)
plot(log(brain, 10) ~ log(body, 10), data = Animals, pch = ".")
text(log(Animals$body, 10), log(Animals$brain, 10),
     labels = rownames(Animals), cex = 0.8)
```

## Reshaping the data
```{r}
Animals$log10_brain <- log(Animals$brain, base = 10)
Animals$log10_body <-  log(Animals$body, base = 10)
```

## Fitting the model
```{r}
mod <- lm(log10_brain ~ log10_body, data = Animals)
plot(mod)
influence.measures(mod)
plot(mod, which = 4)
summary(mod)
Animals_nodino <- Animals[!rownames(Animals) %in%
                          c("Triceratops", "Brachiosaurus", "Dipliodocus"), ]
mod_nodino <- lm(log10_brain ~ log10_body, data = Animals_nodino)
plot(mod_nodino)
influence.measures(mod_nodino)
plot(mod_nodino, which = 4)
summary(mod_nodino)
```

## Plot

```{r}
plot(log10_brain ~ log10_body, data = Animals, col = NULL)
text(x = Animals$log10_body, y = Animals$log10_brain,
     labels = rownames(Animals), cex = 0.8,
     col = ifelse( rownames(Animals) %in% c("Triceratops", "Brachiosaurus", "Dipliodocus"), "red", "black"))
p <- predict(mod, newdata = data.frame("log10_body" = range(log(Animals$body, 10))))
p_nodino <- predict(mod_nodino, newdata = data.frame("log10_body" = range(log(Animals$body, 10))))
points(x = range(Animals$log10_body), y = p, col = "red", type = "l", lty = 2)
points(x = range(Animals$log10_body), y = p_nodino, col = "blue", type = "l")
```




