---
title: "LMM: Random effects"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{4.1 Random effects}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(spaMM)
library(lme4)
library(car)
spaMM.options(nb_cores = 4L)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/LMM_random/", fig.path = "./fig_knitr/LMM_random/", fig.width = 5, fig.height = 5, fig.align = "center", error = TRUE)
```

## Mixed-effects models

* 4.0 [Introduction to LMM](./LMM_intro.html)
* 4.1 [Random effects](./LMM_random.html)
* 4.2 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.3 [A showcase of some useful applications](./LMM_showcase.html)
* 4.4 [Introduction to GLMM](./GLMM_intro.html)
* 4.5 [More complex mixed models](./MM_more.html)


## You will learn in this session

* when to consider effects as fixed or random
* that REML fits are best to study variances
* how to implement different random effects structure
* what random slopes are


# To mix or not to mix?

## Choosing between fixed and random effects

The choice between considering a predictor has having fixed or random effects can be difficult; it depends on the trade-off between the pros and cons of both approaches.

### Fixed effects

* no assumption about the distribution of the effects associated with each level of a predictor
* requires many new datapoints for each additional levels to get reliable results
* allows for the prediction of the effect of observed levels only (for factors)
* simple to study

### Random effects

* the effects associated with each level of a predictor follow a given probability distribution
* requires at least one new datapoint for each additional levels to get reliable results (more is better)
* requires many levels for the variance estimates to be reliable (5 being strict minimum)
* allow for the prediction of the effect of both observed and unobserved levels (for factors)
* more difficult to study


# Studying variation using LMM

## Estimating a variance

### Let's simulate a dataset under the assumptions of LMM

```{r}
set.seed(1)
Aliens <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
```


## Estimating a variance

* Estimating variance components and estimating BLUPS are the only situation in which parameters must be fitted to the data by REstricted (or REsidual) Maximum Likelihood instead of Maximum Likelihood.
* A ML fit would lead to underestimate the variances.

<br>

Note:

* different packages and different functions within the same package may have ML or REML as a default fitting method, so always double check!
* unlike ML, REML is sensitive to changes in contrasts.


## Estimating a variance

### Model fit with ```lmer``` (```REML = TRUE``` by default)

```{r, message = FALSE}
library(lme4)
(mod <- lmer(y ~ x + (1|group), data = Aliens))
```


## Estimating a variance

### Model fit with ```fitme```

```{r, message = FALSE}
library(spaMM)
(mod2 <- fitme(y ~ x + (1|group), data = Aliens, method = "REML"))
```


## Distribution of the variance estimate

```{r distrib lambda large, warning = FALSE}
lambdas_large <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 10, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda, warning = FALSE}
lambdas <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```

```{r distrib lambda small, warning = FALSE}
lambdas_small <- replicate(1000, {
  d <- simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  as.numeric(mod$lambda)
})
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 10
hist(lambdas_large[lambdas_large < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 30, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group <- 2
hist(lambdas[lambdas < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 7, add = TRUE, lwd = 2, col = "red")
```


## Distribution of the variance estimate

```{r, fig.width = 4, fig.height = 4}
var.between.group2 <- 0.1
hist(lambdas_small[lambdas_small < 100], nclass = 50, probability = TRUE)
shape <- (10 - 1)/2 ## with 10 being the number of levels
scale <- (2*var.between.group2)/(10 - 1)
curve(dgamma(x, shape = shape, scale = scale), from = 0, to = 1, add = TRUE, lwd = 2, col = "red")
```


## Testing the variance

### Model fit with ```fitme```

```{r}
mod2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML")
mod2_H0_REML <- fitme(y ~ x, data = Aliens, method = "REML")
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]), df = 1, lower.tail = FALSE)
mod2_H2_REML <- fitme(y ~ x + (1|group), data = Aliens, method = "REML", fixed = list(lambda = 2))
pchisq(2*(logLik(mod2_REML)[[1]] - logLik(mod2_H2_REML)[[1]]), df = 1, lower.tail = FALSE)
```

Note 1: this asymptotic test is poor when the variance is low.

Note 2: ```spaMM``` allows for testing difference with a specific value.

Note 3: ```anova()``` from ```spaMM``` will not run in this case. 

## Testing the variance

### Model fit with ```lmer```

```{r}
mod_REML <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
mod_H0 <- lm(y ~ x, data = Aliens)
pchisq(2*(logLik(mod_REML)[[1]] - logLik(mod_H0)[[1]]), df = 1, lower.tail = FALSE)
anova(mod_REML, mod_H0)
```

Note: ```lme4``` does not allow for testing difference with a specific value.


## Reliability of the test

```{r test spaMM, warning=FALSE}
test <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 2, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 2))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability of the test

```{r}
plot(ecdf(test), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```


## Reliability 2 (small variance)

```{r test spaMM 2, warning=FALSE}
test2 <- replicate(1000, {
  d <-  simulate_Mix(intercept = 50, slope = 1.5, n = 30, group_nb = 10, var.rand = 0.1, var.error = 0.5)
  mod <- fitme(y ~ x + (1|group), data = d, method = "REML")
  mod0 <- fitme(y ~ x + (1|group), data = d, method = "REML",
                fixed = list(lambda = 0.1))
  pchisq(2*(logLik(mod) - logLik(mod0)), df = 1, lower.tail = FALSE)
})
```


## Reliability 2 (small variance)

```{r, fig.width=4, fig.height=4}
plot(ecdf(test2), xlim = c(0, 0.1), ylim = c(0, 0.1))
abline(0, 1, col = "red")
```

Likelihood ratio tests never work well close to parameter boundaries... (better use parametric bootstrap!)


## Testing variance using parametric bootsrap

### Based on the models fitted with ```lme4```

```{r param boot lme4 by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(mod_REML)[[1]] - logLik(mod_H0)[[1]]))
LRTH0 <- replicate(200, {
                    Aliens$y <- simulate(mod_H0)[, 1]
                    2*(logLik(lmer(y ~ x + (1|group), data = Aliens, REML = TRUE))[[1]] -
                      logLik(lm(y ~ x, data = Aliens))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

Note using update creates problems here...


## Testing variance using parametric bootsrap

### Based on the models fitted with ```spaMM```

```{r param boot spaMM by hand}
set.seed(1L)
(LRTobs <- 2*(logLik(mod2_REML)[[1]] - logLik(mod2_H0_REML)[[1]]))
LRTH0_bis <- replicate(200, {
                    Aliens$y <- simulate(mod2_H0_REML)
                    2*(logLik(fitme(y ~ x + (1|group), data = Aliens, method = "REML"))[[1]] -
                      logLik(fitme(y ~ x, data = Aliens, method = "REML"))[[1]])
                    })
(sum(LRTH0 >= LRTobs) + 1) / (length(LRTH0) + 1)
```

## Confidence interval for the variance with ```lme4```

```{r CI lambda lme4}
mod_lmer <- lmer(y ~ x + (1|group), data = Aliens, REML = TRUE)
round(confint(mod_lmer, method = "profile")[1, ]^2, 2)  ## more at ?lme4:::confint.merMod
round(confint(mod_lmer, method = "boot", nsim = 1000)[1, ]^2, 2)
```

<br>

Note: there is not yet an alternative for ```spaMM``` but you can use ```boot``` as we did for LM!


## Confidence interval for the variance using ```spaMM + boot```

```{r spaMM and boot, message = FALSE, warnings = FALSE}
library(boot)
n_boot <- 1000
newYs <- simulate(mod2_REML, type = "marginal", nsim = n_boot)
res_sim <- sapply(1:n_boot, function(i) {
  Aliens$y <- newYs[, i]
  log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]])}) ## lambda on new data
exp(boot.ci(boot.out = list(R = n_boot),
        t0 = log(fitme(y ~ x + (1|group), data = Aliens, method = "REML")$lambda[[1]]), ## original lambda
        t  = matrix(res_sim, ncol = 1), type = "basic")$basic[, 4:5])
```


# Specifying multiple random effects

## The ```lme4::Penicillin``` dataset

```{r}
str(Penicillin)
table(Penicillin$sample, Penicillin$plate)
```

## The ```lme4::Penicillin``` dataset

### The random effects are "crossed"

```{r}
mod <- fitme(diameter ~ 1 + (1|plate) + (1|sample), data = Penicillin)
mod$lambda
```
<div class="columns-2">

```{r}
head(mod$ZAlist[[1]])
```

```{r}
head(mod$ZAlist[[2]], 10)
```
</div>


## The ```lme4::cake``` dataset

```{r}
head(cake)
str(cake)
```


## The ```lme4::cake``` dataset

```{r}
table(cake$recipe, cake$replicate, cake$temperature)
```


## The ```lme4::cake``` dataset

### The random effect is nested within a fixed effect:

```{r}
mod <- fitme(angle ~ recipe + temperature + (1|recipe:replicate), data = cake)
mod$lambda
```


## The ```lme4::cake``` dataset

### The random effect is nested within a fixed effect (alternative):

```{r}
cake$replicate_tot <- factor(paste(cake$recipe, cake$replicate, sep = "_"))
levels(cake$replicate_tot)
mod <- fitme(angle ~ recipe + temperature + (1|replicate_tot), data = cake)
mod$lambda
```


## The ``` carnivora``` dataset

```{r}
data("carnivora", package = "ape")
carnivora$log_brain <- log(carnivora$SB)
carnivora$log_body <- log(carnivora$SW)
str(carnivora)
```


## The ``` carnivora``` dataset

```{r}
tapply(carnivora$Genus, carnivora$Family, function(x) length(unique(x)))
```


## The ``` carnivora``` dataset

```{r}
coplot(log_brain ~ log_body | Family, data = carnivora)
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1 <- fitme(log_brain ~ log_body + (1|Family/Genus), data = carnivora, method = "REML")
mod1
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1bis <- fitme(log_brain ~ log_body + (1|Family) + (1|Family:Genus), data = carnivora, method = "REML")
mod1bis
```


## The ``` carnivora``` dataset

### Two nested random effects:

```{r}
mod1ter <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
mod1ter
```


## The ``` ape::carnivora``` dataset

### Two nested random effects:

<br>

* the formula ```(1|Family/Genus)```
* the formula ```(1|Family) + (1|Family:Genus)```
* the formula ```(1|Family) + (1|Genus)```

are the same as long as genus are not being recycled between families!


## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(mod1$ZAlist[[1]]))
```


## Checking the random structure

### You can check the Z matrices to make sure you did it right

```{r}
crossprod(as.matrix(mod1$ZAlist[[2]]))
```


## Checking the random structure

### You can also use the ```model.matrix``` clone from ```lme4```:

```{r}
lF <- lFormula(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora)
lF$reTrms$flist  ## list of grouping factors used in the random-effects terms; see ?mkReTrms
```


## Checking the random structure

### You can also check the BLUPs structure

```{r}
lapply(ranef(mod1), head, n = 20)
```


## Estimating the variances of two subgroups

### Two variances between genus

```{r}
carnivora$Canidae  <- as.numeric(carnivora$Family == "Canidae")
carnivora$Others   <- as.numeric(carnivora$Family != "Canidae")

mod2 <- fitme(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora, method = "REML")
```

<br>

Note: it does not seem to work with more than 2 variances, which I don't understand...


## Estimating the variances of two subgroups

```{r}
mod2
```


## Estimating the variances of two subgroups

```{r}
as.data.frame(ranef(mod2))
```


## Estimating the variances of two subgroups

### Same using ```lme4```

```{r}
mod2_lme4 <- lmer(log_brain ~ log_body + (0 + Canidae|Genus) + (0 + Others|Genus), data = carnivora)
lapply(VarCorr(mod2_lme4), function(r) attr(r, "stddev")^2)
head(ranef(mod2_lme4))
```


# Random slopes


## Fitting a random slope model

```{r}
(mod3 <- fitme(log_brain ~ log_body + (log_body|Family) + (1|Genus), data = carnivora, method = "REML"))
```


## The BLUPs for the slopes

```{r}
ranef(mod3)$`( log_body | Family )`
```


## Predictions

```{r, fig.width = 4, fig.height = 4}
plot(log_brain ~ log_body, data = subset(carnivora, Family == "Felidae"), col = "red",
     ylim = range(carnivora$log_brain), xlim = range(carnivora$log_body))
points(log_brain ~ log_body, data = subset(carnivora, Family == "Mustelidae"), col = "blue")
points(log_brain ~ log_body, data = subset(carnivora, Family == "Viverridae"), col = "orange")
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Felidae", ], col = "red", lwd = 2, lty = 2)
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Mustelidae", ], col = "blue", lwd = 2, lty = 2)
abline(mod3$fixef + ranef(mod3)$`( log_body | Family )`["Viverridae", ], col = "orange", lwd = 2, lty = 2)
```


## Practice

<br>

  Perform the predictions of all slopes using the function predict instead, both using ```spaMM``` and ```lme4```.


## Testing if slopes differ between families

### ```lme4```
```{r}
mod4 <- lmer(log_brain ~ log_body + (log_body|Family) + (1|Genus), data = carnivora, REML = TRUE)
mod4noRS <- lmer(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, REML = TRUE)
anova(mod4, mod4noRS) ## count 2 DF as covariance is included... correct?
```


## Testing if slopes differ between families

### ```spaMM```
```{r}
mod3noRS <- fitme(log_brain ~ log_body + (1|Family) + (1|Genus), data = carnivora, method = "REML")
pchisq(2*(logLik(mod3) - logLik(mod3noRS)), df = 2, lower.tail = FALSE) ## count 2 DF to include covariance as lme4
```


## Testing if slopes differ between families

### LM
```{r}
mod5 <- lm(log_brain ~ log_body * Family + Genus, data = carnivora)
mod5noIS <- lm(log_brain ~ log_body + Family + Genus, data = carnivora)
anova(mod5, mod5noIS)
```


## Fitting uncorrelated random intercept and slope

```{r}
(mod3_alt <- fitme(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora))
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt <- lmer(log_brain ~ log_body + (1|Family)  + (0 + log_body|Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt), function(r) attr(r, "stddev")^2)
```


## Fitting uncorrelated random intercept and slope

```{r}
mod4_alt_bis <- lmer(log_brain ~ log_body + (log_body||Family) + (1|Genus), data = carnivora, REML = FALSE)
lapply(VarCorr(mod4_alt_bis), function(r) attr(r, "stddev")^2)
```

<br>

Note 1: the syntax ```||``` does not work in ```spaMM``` but it is just a shortcut, so it is not really needed.

Note 2: unless you have a very good reason not to, you should consider the correlations between random effects!


## What you need to remember

* when to consider effects as fixed or random
* that REML fits are best to study variances
* how to implement different random effects structure
* what random slopes are


# Table of contents

## Mixed-effects models

* 4.0 [Introduction to LMM](./LMM_intro.html)
* 4.1 [Random effects](./LMM_random.html)
* 4.2 [Solving LM problems using LMM](./LMM_solving_pb.html)
* 4.3 [A showcase of some useful applications](./LMM_showcase.html)
* 4.4 [Introduction to GLMM](./GLMM_intro.html)
* 4.5 [More complex mixed models](./MM_more.html)

