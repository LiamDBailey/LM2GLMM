---
title: "LM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.0 LM: Introduction}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(LM2GLMM)
options(width = 110)
knitr::opts_chunk$set(error = TRUE)
```

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro_course.html)
* 2.1 [Fitting procedure](./LM_fitting_course.html)
* 2.2 [Tests & Intervals](./LM_test_intervals_course.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions_course.html)
* 2.4 [Let's practice](./LM_practice_exercises.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>


## You should learn during this session `r .emo("info")`

* how to write a linear model (using 3 different notations)
* how to simulate the data according to simple linear models
* some key vocabulary (e.g. response variable, fitted values, residuals...)
* how to understand the meaning of parameters by investigating the design matrix
* how to manipulate design matrices via formula and contrasts
* how to compute predictions


# Definition and notations

## Disclaimer `r .emo("warn")`

There is (unfortunately) no such thing as a standard vocabulary to discuss LM.

The terms I chose to use are not used by all; even simple and widespread terms such as *predictor variables* and *residuals* can mean different things for different people.

Statisticians perhaps do not need words as much as we do because they can rely on their understanding of precise mathematical definitions.

As we are not statisticians, for the sake of clear communication, I will try to stick to quite precise terms which I am going to define in my own way.

Keep this in mind if you are looking at other content online or in books.

The important thing is for you to grasp what these words represent, more than the word themselves.


## What is a Linear Model (LM)? `r .emo("info")`

### What is a statistical model?

**A statistical model represents, often in considerably idealized form, the data-generating process.** [(wikipedia)](https://en.wikipedia.org/wiki/Statistical_model)
<br>

### What is a linear model?

**<span style="color:red"> A data-generating process produced by a linear function</span>: the data are generated from a set of terms by multiplying each term by a constant (a model parameter) and summing them.**


## Mathematical notation of LM: simple notation `r .emo("info")`

<center><font size="5"> $y_i = \beta_0 + \beta_1 \times x_{1,i} + \beta_2 \times x_{2,i} + \dots + \beta_{p} \times x_{p,i} + \epsilon_i$ </font></center>

<br>

* $y_i$ = the response variable / dependent variable / observations to explain
* $\beta_j$ = the model parameters / regression coefficients
* $x_{j,i}$ = regressors / constants derived from the predictor variables (aka predictors for short)
* $\epsilon_i$ = the errors / residual errors

<br>

This is the notation that biologists should use in their paper/thesis to describe their model.

Note: the regressors are sometimes directly given by the columns from your dataset (i.e. the predictors), but not always (more on that later).


## Mathematical notation of LM: matrix notation `r .emo("info")`

<center><font size = 8> $Y = X \beta + \epsilon$ </font></center>

$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \dots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \dots \\ \beta_p
\end{bmatrix} +
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \dots \\ \epsilon_n
\end{bmatrix}
$$

<br>

* $Y$ = the vector of observations
* $X$ = a matrix called the design (or model) matrix
* $\beta$ = the vector of model parameters
* $\epsilon$ = the vector of errors

This is the notation for statisticians and computer scientists.


## Aliens generated by goddess *Emerald Lion* `r .emo("alien")`
<!-- Emerald Lion is an anagram to linear model -->

Emerald Lion informed us that she grows aliens of a certain size as follows:

* $\texttt{size}_i = 50 + 1.5 \times \texttt{humans_eaten}_{i} + \epsilon_i$
* $\epsilon_i \sim \mathcal{N}(0, \sigma^2 = 5)$

<br>

We can thus generate the size of 6 aliens that have eaten between 1 and 3 humans following EL recipe:

```{r alien data}
set.seed(123L)
Alien <- data.frame(humans_eaten = rep(1:3, each = 2), intercept = 50, slope = 1.5)
Alien$error <- rnorm(n = 6, mean = 0, sd = sqrt(5))
Alien$size <- Alien$intercept + Alien$slope*Alien$humans_eaten + Alien$error
```

Note: we could also write the last 2 lines as the following line:

```{r alien data2}
# Alien$size <- rnorm(n = 6, mean = 50 + 1.5*Alien$humans_eaten, sd = sqrt(5))
```


## The study of the Aliens `r .emo("practice")`

This is the data we simulated:
```{r alien study}
Alien
```

Can you recognise:

- the model parameters?
- the regressor (here = predictor)?
- the errors?
- the response variable?


## In practice, model parameters are unknown `r .emo("practice")`

So we will have to estimate them (that is the whole point of statistical modelling...)

*Fitting* the model to the data means estimating the values of the model parameters that maximise the probability of the data (more on that in the next sessions).

```{r alien lm}
(fit_alien_lm <- lm(size ~ humans_eaten, data = Alien))
```

<br>

Note: we did not get $50$ and $1.5$, and only talking to Emerald Lion can get you such population-level values.

<!--
## A fit is never perfect
```{r alien data 2 eval, fig.align='center', fig.asp=1, fig.width=4, echo = FALSE, fig.cap="True vs inferred mean relationship"}
plot(size ~ humans_eaten, data = Alien, ylab = "Alien size (cm)", xlab = "No. of humans eaten")
abline(a = 50, b = 1.5, col = "green", lwd = 2)
abline(fit_alien_lm, col = "blue", lwd = 2, lty = 2)
legend("topleft", fill = c("green", "blue"), legend = c("TRUE", "fitted"), bty = "n")
```
-->

## R notation for fitting LM fits `r .emo("info")`

`lm(var_obs ~ var_a + var_b + ... + var_p, data = somedata)`

<br>

* `lm()` the function fitting the linear model
* `somedata` the data used for the fitting procedure
* `var_obs ~ var_a + var_b + ... + var_p` the model formula
* `var_obs` = the name (not quoted) of the column with the observations for the response variable
* `var_a, var_b, ..., var_p` = the names (not quoted) of the columns with the observations for the predictor (= independent or explanatory variables)

<br>

This is the notation that is good for talking to R and to R users, but it does not describe the assumed data generating process.


## Mathematical notation of LM fits `r .emo("info")`

### Simple notation

<center><font size="5"> $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} \times x_{1,i} + \hat{\beta_2} \times x_{2,i} + \dots + \hat{\beta_p} \times x_{p,i}$ </font></center>

<br>

* $\hat{y_i}$ = the fitted values
* $\hat{\beta_j}$ = the (model parameter / regression coefficient) estimates
* $x_{j,i}$ = regressors / constants derived from the predictors
* $y_i - \hat{y_i} = \varepsilon_i$ = the residuals (i.e. the estimates for the errors / residual errors)

<br>

### Matrix notation `r .emo("info")`

<center><font size = 5> $\widehat{Y} = X \widehat{\beta}$ </font></center>

<center><font size = 5> $Y = X \widehat{\beta} + \varepsilon = \widehat{Y} + \varepsilon$ </font></center>

<center><font size = 5> $\varepsilon = \widehat{\epsilon} = Y - \widehat{Y}$ </font></center>


# Model parameters & model parameters estimates

## Model parameters `r .emo("info")`

* one dimension: $p \times 1$ vector
* fixed non-observable quantities

<br>

### They mediate the relationship between variables and the data generated: they convert each column from the design matrix into the elements of the response variable.

<br>

The meaning of the parameters thus always depends on the design matrix (more on that soon!)


## Model parameters estimates `r .emo("practice")`

These are the estimations of the model parameters produced by the fit of the model:

```{r estimates}
Alien$intercept_est <- coef(fit_alien_lm)[["(Intercept)"]]
Alien$slope_est <- coef(fit_alien_lm)[["humans_eaten"]]
Alien
```


# Response variable, mean response values & fitted values

## The response variable `r .emo("info")`

### It is the set of observations produced by the data-generating process
### In LM: a single continuous variable

<br>

Characteristics:

* one dimension: $n \times 1$
* continuous
* no other restriction


## The response variable `r .emo("info")`

```{r alien data show}
Alien ## it is the column "size"
```
<div class="columns-2">

### Simple notation
* $y_1 = 50 + 1.5 \times `r Alien$humans_eaten[1]` + `r Alien$error[1]`$
* $y_2 = 50 + 1.5 \times `r Alien$humans_eaten[2]` + `r Alien$error[2]`$
* $y_3 = 50 + 1.5 \times `r Alien$humans_eaten[3]` + `r Alien$error[3]`$
* $y_4 = 50 + 1.5 \times `r Alien$humans_eaten[4]` + `r Alien$error[4]`$
* $y_5 = 50 + 1.5 \times `r Alien$humans_eaten[5]` + `r Alien$error[5]`$
* $y_6 = 50 + 1.5 \times `r Alien$humans_eaten[6]` + `r Alien$error[6]`$

### Matrix notation
$$
Y = \begin{bmatrix}
1 & `r Alien$humans_eaten[1]` \\
1 & `r Alien$humans_eaten[2]` \\
1 & `r Alien$humans_eaten[3]` \\
1 & `r Alien$humans_eaten[4]` \\
1 & `r Alien$humans_eaten[5]` \\
1 & `r Alien$humans_eaten[6]`
\end{bmatrix}
\begin{bmatrix}
50 \\
1.5
\end{bmatrix}
+
\begin{bmatrix}
`r Alien$error[1]`\\
`r Alien$error[2]`\\
`r Alien$error[3]`\\
`r Alien$error[4]`\\
`r Alien$error[5]`\\
`r Alien$error[6]`
\end{bmatrix}
$$

</div>


## Mean response values `r .emo("info")`

The mean response values are here the mean sizes of an infinite number of aliens eating the given amounts of humans.

<div class="columns-2">

### Simple notation
* $\overline{y_1} = 50 + 1.5 \times `r Alien$humans_eaten[1]`$
* $\overline{y_2} = 50 + 1.5 \times `r Alien$humans_eaten[2]`$
* $\overline{y_3} = 50 + 1.5 \times `r Alien$humans_eaten[3]`$
* $\overline{y_4} = 50 + 1.5 \times `r Alien$humans_eaten[4]`$
* $\overline{y_5} = 50 + 1.5 \times `r Alien$humans_eaten[5]`$
* $\overline{y_6} = 50 + 1.5 \times `r Alien$humans_eaten[6]`$

### Matrix notation
$$
\overline{Y} = \begin{bmatrix}
1 & `r Alien$humans_eaten[1]` \\
1 & `r Alien$humans_eaten[2]` \\
1 & `r Alien$humans_eaten[3]` \\
1 & `r Alien$humans_eaten[4]` \\
1 & `r Alien$humans_eaten[5]` \\
1 & `r Alien$humans_eaten[6]`
\end{bmatrix}
\begin{bmatrix}
50 \\
1.5
\end{bmatrix}
$$

</div>

<br>

Note: it is also the response minus the errors.


## Mean response values `r .emo("info")`

They can easily be computed in R using vectors:

```{r alien predict from error}
Alien$size_mean <- Alien$intercept + Alien$slope * Alien$humans_eaten
# or # Alien$size_mean <- Alien$size - Alien$error
Alien
```


## The fitted values `r .emo("practice")`

These are the estimations of the mean response values produced by the fit of the model:

```{r alien fitted values}
Alien$size_fitted <- fitted.values(fit_alien_lm)
```
```{r alien fitted values_res}
Alien
```

## The fitted values `r .emo("practice")`

They can easily be computed in R using vectors too:

```{r alien predict from error2}
size_fitted2 <- Alien$intercept_est + Alien$slope_est * Alien$humans_eaten
```

```{r alien predict from error2res}
all.equal(size_fitted2, Alien$size_fitted)
```


## The fitted values `r .emo("practice")`

They can also be computed using matrices (also true for mean response values):

```{r predict matrix}
size_fitted3 <- model.matrix(fit_alien_lm) %*% coef(fit_alien_lm)
```
```{r predict matrix_res}
all.equal(as.numeric(size_fitted3), Alien$size_fitted)
```

<br>

Note : R uses design matrices extensively (internally)!

## Visualizing LM concepts `r .emo("info")`

```{r fig.height=5, fig.width=10, echo = FALSE}
mod_intercept <- coef(fit_alien_lm)[["(Intercept)"]]
mod_slope     <- coef(fit_alien_lm)[["humans_eaten"]]

#Data for mean and fitted lines
line_data <- data.frame(humans_eaten = rep(c(0, 3), times = 2),
                        var = rep(c("Mean response\nvalue", "Fitted value"), each = 2))
line_data$size <- ifelse(line_data$var == "Mean response\nvalue",
                         50 + line_data$humans_eaten*1.5,
                         mod_intercept + line_data$humans_eaten*mod_slope)

#Line and text data
label_data <- data.frame(label = c("Observed value",
                                   "Fitted value",
                                   "Mean response\nvalue"),
                         x = 3.5, xend = 3.05,
                         y = c(59.5, 56.5, 52.5),
                         yend = c(Alien[nrow(Alien), "size"] + 0.2,
                                  Alien[nrow(Alien), "size_fitted"],
                                  Alien[nrow(Alien), "size_mean"]),
                         curvature = c(0.15, -0.15, -0.3))

#Palette
my_palette <- c("#f06543", "#ddd92a", "#7371fc")


library(ggplot2)
ggplot(data = Alien) +
  geom_point(aes(x = humans_eaten, y = size, colour = "Observed value"),
             shape = 4, size = 1, stroke = 2) +
  geom_segment(aes(x = 0, xend = humans_eaten, y = size_mean, yend = size_mean),
               lty = 2) +
  geom_segment(aes(x = humans_eaten, xend = humans_eaten, y = size_mean, yend = 45),
               lty = 2) +
  geom_line(data = line_data, aes(x = humans_eaten, y = size, colour = var),
            size = 1) +
  geom_point(aes(x = humans_eaten, y = size_mean, colour = "Mean response\nvalue"),
             size = 2) +
  geom_point(aes(x = humans_eaten, y = size_fitted, colour = "Fitted value"),
             size = 2) +
  ### ADD CURVED ARROWS THAT POINT TO IMPORTANT VALUES
  geom_curve(data = label_data,
             aes(x = x, xend = xend, y = y, yend = yend, colour = label),
             curvature = 0.15, arrow = arrow(length = unit(0.3, units = "cm"))) +
  ### ADD TEXT TO LABELS
  geom_text(data = label_data,
            aes(x = x, y = y, label = label, colour = label),
            hjust = 0, vjust = 0.75) +
  scale_colour_manual(values = my_palette) +
  scale_x_continuous(limits = c(0, 4.25),
                     breaks = 0:3,
                     labels = c(0, 1, 2, 3),
                     expand = c(0, 0)) +
  scale_y_continuous(limits = c(45, 60),
                     breaks = c(45, mod_intercept, 50, 51.5, 53.0, 54.5, 60),
                     labels = c("45.0",
                                paste0("Intercept estimate --> ", signif(mod_intercept, 3)),
                                "Intercept -->  50.0",
                                "51.5", "53.0", "54.5", "60.0"),
                     expand = c(0, 0)) +
  labs(x = "Humans eaten", y = "Size") +
  theme_classic() +
  theme(legend.position = "none",
        axis.title.y = element_text(vjust = -25),
        plot.background = element_blank(),
        panel.background = element_blank(),
        axis.title = element_text(size = 20, colour = "black"),
        axis.text = element_text(size = 12, colour = "black"),
        axis.ticks = element_line(colour = "black"),
        plot.margin = margin(t = 10, r = 10, b = 10, l = 10))
```

# The design matrix

## The design matrix `r .emo("info")`

### It is made of the regressors (= adequate representations of the predictors)

###  <span style="color:red"> You cannot interpret model parameters correctly without understanding the design matrix of a model! </span>

<br>

Characteristics:

* multi-dimensional: $n \times p$ with $n > p$
* deduced from the predictors
* known and measured without error (more on that later)
* columns must be linearly independent (more on that later)


## The design matrix `r .emo("info")`

It is not estimated and is thus the same for both the true data-generating process and the inferred one:

<br>

<center><font size = 8> $Y = \color{red}{X} \beta + \epsilon$ </font></center>

<br>

<center><font size = 8> $\widehat{Y} = \color{red}{X} \widehat{\beta}$ </font></center>


## Design matrix `r .emo("practice")`

The model matrix is automatically produced when fitting a model:

```{r dm from fit}
model.matrix(fit_alien_lm)
```

`lm()` always estimate one model parameter per column of the design matrix:
```{r coef from fit}
names(coef(fit_alien_lm))
```


## Design matrix = formula + the data (+ contrasts) `r .emo("practice")`

To see the design matrix without fitting the model, you can also use formula in `model.matrix()`:

```{r pred matrix}
model.matrix(object = ~ humans_eaten, data = Alien)
```

The term `~ humans_eaten` is a *formula* despite not having a response variable on the left.

The formula tells `model.matrix()` (direclty or via `lm()`) how to create a design matrix from the dataset.

Turning a biological problem into a linear model is nothing more that figuring out which design matrix you need and thus which formula to write when calling `lm()`!


## A design matrix without intercept `r .emo("practice")`

```{r fit no intercept}
model.matrix(object = ~ 0 + humans_eaten, data = Alien)
lm(size ~ 0 + humans_eaten, data = Alien) ## you can also use -1
```

Can you think of what such fit would look like if plotted?


## A design matrix with function calls `r .emo("practice")`

```{r fit fn1}
(fit_log_humans_within <- lm(size ~ log(humans_eaten), data = Alien))
```

You can also apply such transformations in your data frame instead:

```{r fit fn2}
Alien_temp <- Alien
Alien_temp$log_humans_eaten <- log(Alien_temp$humans_eaten)
(fit_log_humans_without <- lm(size ~ log_humans_eaten, data = Alien_temp))
```


## A design matrix with function calls `r .emo("info")`

Note: the coefficients did not change because the content of the 2 model matrices is the same:

```{r model matrix fn1}
model.matrix(fit_log_humans_within)
```

```{r model matrix fn2}
model.matrix(fit_log_humans_without)
```


## A design matrix with polynomials `r .emo("practice")`

```{r poly ex1}
(fit_poly_alien <- lm(size ~ poly(humans_eaten, 2), data = Alien))
```

```{r poly ex2}
(fit_poly_raw_alien <- lm(size ~ poly(humans_eaten, 2, raw = TRUE), data = Alien))
```


## A design matrix with polynomials `r .emo("info")`

Note: the coefficients did change because the content of the 2 model matrices is not the same:

```{r model matrix poly}
model.matrix(fit_poly_alien)
```

```{r model matrix poly_raw}
model.matrix(fit_poly_raw_alien)
```


## A design matrix with polynomials `r .emo("info")`

Note: alternative parametrizations do not necessarily impact the fitted values as is the case for the 2 parametrization for polynomials.

```{r fitted poly}
data.frame(fitted_poly = fitted.values(fit_poly_alien),
           fitted_poly_raw = fitted.values(fit_poly_raw_alien))
```

<br>

In case you are curious: the default polynomial formulation for the design matrix produces columns in the design matrix which are orthogonal (no correlation), which presents better numerical properties for linear algebra, and which makes tests on them more informative. E.g. `poly1` provides information on whether considering a linear relationship is useful or not, irrespective of if there is a quadratic signal or not. Yet, the interpretation of parameter values is easier when polynomials are expressed under their raw form.


## (Galactic) Interlude `r .emo("alien")`

Goddess Emerald Lion informed us that Aliens have colonised different exoplanets since their creation and that their age depends on when they arrived on these planets, on their sex (the sex ZZ ages much faster), and on how many times they visited a black hole (due to the time dilatation happening there):

```{r Alien}
set.seed(123)
Alien2 <- data.frame(planet = factor(c(rep("Chambr", 2), rep("Riplyx", 2), rep("Wickor", 2))),
                     sex = factor(rep(c("Z", "ZZ"), times = 3)),
                     bh_trips = 6:1)
Alien2$age <- 0.5*Alien2$bh_trips
Alien2$age[Alien2$planet == "Chambr"] <- Alien2$age[Alien2$planet == "Chambr"] + 100 
Alien2$age[Alien2$planet == "Riplyx"] <- Alien2$age[Alien2$planet == "Riplyx"] + 1000
Alien2$age[Alien2$planet == "Wickor"] <- Alien2$age[Alien2$planet == "Wickor"] + 10000
Alien2$age[Alien2$sex == "Z"] <- Alien2$age[Alien2$sex == "Z"] + -20
Alien2$age[Alien2$sex == "ZZ"] <- Alien2$age[Alien2$sex == "ZZ"] + 20
Alien2$age <- Alien2$age + rnorm(nrow(Alien2), mean = 0, sd = 4)
Alien2
```


## A design matrix with a factor `r .emo("info")`

```{r factor 1}
model.matrix(object = ~ planet, data = Alien2)
```

<br>

This parametrization of factors such as `planet` follows the so-called matrix of contrasts called "treatment".

It is the easiest one to interpret and the one used by default in R (but not in all software).

## A design matrix with a factor `r .emo("practice")`

```{r factor 2}
(fit_alien_lm2 <- lm(age ~ planet, data = Alien2))
```

Can you reproduce the `fitted` values in the following table?

```{r factor 3}
D <- model.matrix(object = ~ planet, data = Alien2) ## same as D <- model.matrix(fit_alien_lm2)
cbind(D, data.frame(fitted = D %*% coef(fit_alien_lm2))) ## same as fitted = fitted.values(fit_alien_lm2)
```

## Interpreting the treatment contrasts `r .emo("info")`

```{r treatment contrast}
model.matrix(object = ~ planet, data = Alien2)
```

* The intercept represents the mean response value for the baseline (here, reference level = ```"Chambr"```).
* ```planetRiplyx``` represents the mean response value for ```"Riplyx"``` minus the mean of the baseline.
* ```planetWickor``` represents the mean response value for ```"Wickor"``` minus the mean of the baseline.

Note: in base R the default level of reference is always the first level of a factor. Which level comes first can be changed, but by default it is the first level when ranked by alphabetical order given the locale (mind that `forcats::as_factor()` behave differently than `as.factor()`; the forcats function follows order in which levels appear).

## Interpreting the treatment contrasts `r .emo("info")`

```{r refactor 1}
Alien2$planet <- factor(Alien2$planet, levels = c("Riplyx", "Chambr", "Wickor")) #Change reference to 'Riplyx'
(fit_alien_lm_revel <- lm(age ~ planet, data = Alien2)) #Different regression coefficients
```

Regression coefficients are different, but fitted values are unchanged!

```{r refactor 2}
D <- model.matrix(object = ~ planet, data = Alien2)
cbind(D, data.frame(fitted = D %*% coef(fit_alien_lm2)))
```

```{r, include = FALSE}
#Relevel to make sure following code still works
Alien2$planet <- factor(Alien2$planet, levels = c("Chambr", "Riplyx", "Wickor"))
```

## There are alternative parametrizations! `r .emo("warn")`

```{r sum contrast 1}
model.matrix(object = ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

Alternative parametrizations of the contrasts can ease convergence in GLM(M) (for continuous variable) and can allow for testing specific hypotheses or fit particular experimental design (for qualitative variables). 

It will not change the predictions, the likelihood, the AIC, ...; only the parameter values and their interpretation!

<!--

## Interpreting the Helmert contrasts

Average of the three levels =
$$
\frac{(1*\text{Int}-1*\text{x1}-1*\text{x2})+(1*\text{Int}+1*\text{x1}-1*\text{x2})+(1*\text{Int}+0*\text{x1}-2*\text{x2})}{3}=\text{Int}
$$
The intercept thus represents the mean of the mean of all groups. (nice!)

You can use the same idea to show that the first coefficient corresponds to the mean of the first 2 groups minus the mean of the first group, that the second coefficient corresponds to the mean of the first 3 groups minus the mean of the first 2 groups, and so forth. (weird ?!)
-->

## The example of "sum contrasts" `r .emo("nerd")`

```{r contr.sum 1}
model.matrix(object = ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
\frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{Int}
$$

* The intercept thus represents the mean of the mean responses across all groups.


## The example of "sum contrasts" `r .emo("nerd")`

```{r contr.sum 2}
model.matrix(object = ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
(\text{Int} + \text{plt1}) - \frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{plt1}
$$

* The intercept thus represents the mean of the mean responses of all groups.
* `planet1` represents the mean response of the first group minus the intercept.


## The example of "sum contrasts" `r .emo("nerd")`

```{r contr.sum 3}
model.matrix(object = ~ planet, data = Alien2, contrasts.arg = list(planet = "contr.sum"))
```

$$
(\text{Int} + \text{plt2}) - \frac{(\text{Int}+\text{plt1})+(\text{Int}+\text{plt2})+(\text{Int}-\text{plt1}-\text{plt2})}{3}=\text{plt2}
$$

* The intercept thus represents the mean of the mean responses of all groups.
* `planet1` represents the mean response of the first group minus the intercept.
* `planet2` represents the mean response of the second group minus the intercept.

Note: not much used by R users (because not the default) but useful when there is no clear control.


## Example of estimates across contrasts `r .emo("nerd")`

```{r sum contrast panoply}
lm(age ~ planet, data = Alien2, contrasts = list(planet = "contr.treatment"))
lm(age ~ planet, data = Alien2, contrasts = list(planet = "contr.sum"))
```

<!--
## Interim: the product XTX

The product XTX is essential for many computations around linear models.

It represents all pairwise products between the columns of the design matrix.

```{r XTX_explained}
X <- matrix(c(1, 1, 1, 1, 2, 3, 4, 5, 6), byrow = FALSE, ncol = 3)
(XTX_1 <- t(X) %*% X)
XTX_2 <- crossprod(X)

XTX_3 <- matrix(0, nrow = 3, ncol = 3)

for (i in 1:3) for (j in 1:3) XTX_3[i, j] <- sum(X[, i]*X[, j])

identical(XTX_1, XTX_2) & identical(XTX_1, XTX_3)
```

```{r XTX}
crossprod(DM_raw)  ## high values mean that predictors are collinear -> will be hard to fit
zapsmall(crossprod(DM_ortho))  ## 0 means that predictors are orthogonal -> will be easy to fit
```

<br>

It produces a symmetric diagonal matrix. Those are particularly maths friendly.
-->

## A design matrix with > 1 factors `r .emo("practice")`
```{r factorial}
model.matrix(object = ~ planet + sex, data = Alien2)
```
Now the intercept corresponds to observations that have the first level for all factors.

## A design matrix with > 1 factors `r .emo("practice")`

```{r factorial coef}
lm(age ~ planet + sex, data = Alien2)
```

## A design matrix with interactions `r .emo("practice")`

```{r pred matrix 4}
model.matrix(object = ~ planet + sex + planet:sex, data = Alien2)
```

<br>

Interactions allow for the consideration of the effect of one predictor dependent on the value taken by other(s). If interactions are not considered, effects are assumed to be independent.


## A design matrix with interactions `r .emo("practice")`

```{r pred matrix 5}
model.matrix(object = ~ planet*sex, data = Alien2)
```

The `*` is a shortcut to `+` & `:`


## A design matrix with interactions `r .emo("practice")`

Interaction between a quantitative and qualitative predictor:

```{r pred matrix 6}
model.matrix(object = ~ planet * bh_trips, data = Alien2)
```


## A design matrix with interactions `r .emo("practice")`

```{r alien interaction}
lm(age ~ planet * bh_trips, data = Alien2)
```

<br>

How do you interpret each parameter estimate?


# Errors and residuals

## The errors `r .emo("info")`

Characteristics:

* one dimension: $n \times 1$ vector
* random variable
* Gaussian (normally distributed), independent (no autocorrelation) and identically distributed (homoscedastic)

<center><font size="8"> $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$</font></center>

<br>

With the covariance matrix $\sigma^2 I$ being the following $n \times n$ matrix:

$$
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2 \\
\end{bmatrix}
$$


## The residuals `r .emo("practice")`

These are the estimations of the errors produced during the fit of the model:

```{r error vs residuals}
Alien$residuals <- residuals(fit_alien_lm)
Alien
```

```{r error vs residals 2}
cor(Alien$residuals, Alien$error)
```

```{r error vs residals 3}
mean(Alien$residuals) ## residuals have a mean of 0 in the sample, while error have a mean 0 in the population
```

## Variances for the error & residuals `r .emo("info")`

The variance of the error ($\sigma^2$) cannot be directly observed...

... but it can be estimated from the variance of the residual.

<br>

## The residuals `r .emo("practice")`

```{r fig.height=5, fig.width=10, echo = FALSE}
mod_intercept <- coef(fit_alien_lm)[["(Intercept)"]]
mod_slope     <- coef(fit_alien_lm)[["humans_eaten"]]

#Data for mean and fitted lines
line_data <- data.frame(humans_eaten = rep(c(0, 3), times = 2),
                        var = rep(c("Mean response\nvalue", "Fitted value"), each = 2))
line_data$size <- ifelse(line_data$var == "Mean response\nvalue",
                         50 + line_data$humans_eaten*1.5,
                         mod_intercept + line_data$humans_eaten*mod_slope)

#Line and text data
label_data <- data.frame(label = c("Observed value",
                                   "Fitted value",
                                   "Mean response\nvalue",
                                   "Error",
                                   "Residual"),
                         x = c(3.5, 3.5, 3.5, 1.7, 2.3),
                         xend = c(3.05, 3.05, 3.05, 1.9, 2.1),
                         y = c(59.5, 56.5, 52.5, Alien[3, "size"] + 1, Alien[3, "size"] + 1),
                         yend = c(Alien[nrow(Alien), "size"] + 0.2,
                                  Alien[nrow(Alien), "size_fitted"],
                                  Alien[nrow(Alien), "size_mean"],
                                  Alien[3, "size"] - 1,
                                  Alien[3, "size"] - 1))

#Palette
my_palette <- c("#3a405a", "#f06543", "#ddd92a", "#7371fc", "#20BF55")

ggplot(data = Alien) +
  ### ADD POINTS SHOWING OBSERVED VALUES
  geom_point(aes(x = humans_eaten, y = size, colour = "Observed value"),
             shape = 4, size = 1, stroke = 2) +
  geom_segment(aes(x = 0, xend = humans_eaten, y = size_mean, yend = size_mean),
               lty = 2) +
  geom_segment(aes(x = humans_eaten, xend = humans_eaten, y = size_mean, yend = 45),
               lty = 2) +
  geom_line(data = line_data, aes(x = humans_eaten, y = size, colour = var),
            size = 1) +
  geom_point(aes(x = humans_eaten, y = size_mean, colour = "Mean response\nvalue"),
             size = 2) +
  geom_point(aes(x = humans_eaten, y = size_fitted, colour = "Fitted value"),
             size = 2) +
  ### ADD SEGMENTS TO SHOW RESIDUALS V. ERRORS
  #### ERROR SEGMENTS
  geom_segment(data = Alien[3, ], aes(xend = humans_eaten, x = humans_eaten - 0.05,
                                      y = size - 0.02, yend = size - 0.02, colour = "Error"),
               size = 1) +
  geom_segment(data = Alien[3, ], aes(xend = humans_eaten, x = humans_eaten - 0.05,
                                      y = size_mean + 0.02, yend = size_mean + 0.02, colour = "Error"),
               size = 1) +
  geom_segment(data = Alien[3, ], aes(x = humans_eaten - 0.05, xend = humans_eaten - 0.05, y = size, yend = size_mean,
                                      colour = "Error"),
               lty = 1, size = 1) +
  #### RESIDUAL SEGMENTS
    geom_segment(data = Alien[3, ], aes(xend = humans_eaten, x = humans_eaten + 0.05,
                                      y = size - 0.02, yend = size - 0.02, colour = "Residual"),
               size = 1) +
  geom_segment(data = Alien[3, ], aes(xend = humans_eaten, x = humans_eaten + 0.05,
                                      y = size_fitted + 0.02, yend = size_fitted + 0.02, colour = "Residual"),
               size = 1) +
  geom_segment(data = Alien[3, ], aes(x = humans_eaten + 0.05, xend = humans_eaten + 0.05, y = size, yend = size_fitted, 
                                      colour = "Residual"),
               lty = 1, size = 1) +
  ### ADD CURVED ARROWS THAT POINT TO IMPORTANT VALUES
  geom_curve(data = label_data[1:3, ],
             aes(x = x, xend = xend, y = y, yend = yend, colour = label),
             curvature = 0.15, arrow = arrow(length = unit(0.3, units = "cm"))) +
  geom_curve(data = label_data[4, ],
             aes(x = x, xend = xend, y = y, yend = yend, colour = label),
             curvature = 0.15, arrow = arrow(length = unit(0.3, units = "cm"))) +
  geom_curve(data = label_data[5, ],
             aes(x = x, xend = xend, y = y, yend = yend, colour = label),
             curvature = -0.15, arrow = arrow(length = unit(0.3, units = "cm"))) +
  ### ADD TEXT TO LABELS
  geom_text(data = label_data[1:3, ],
            aes(x = x, y = y, label = label, colour = label),
            hjust = 0, vjust = 0.75) +
  geom_text(data = label_data[4:5, ],
            aes(x = x, y = y + 0.2, label = label, colour = label),
            hjust = 0.5, vjust = 0.5) +
  scale_colour_manual(values = my_palette) +
  scale_x_continuous(limits = c(0, 4.25),
                     breaks = 0:3,
                     labels = c(0, 1, 2, 3),
                     expand = c(0, 0)) +
  scale_y_continuous(limits = c(45, 60),
                     breaks = c(45, mod_intercept, 50, 51.5, 53.0, 54.5, 60),
                     labels = c("45.0",
                                paste0("Intercept estimate --> ", signif(mod_intercept, 3)),
                                "Intercept -->  50.0",
                                "51.5", "53.0", "54.5", "60.0"),
                     expand = c(0, 0)) +
  labs(x = "Humans eaten", y = "Size") +
  theme_classic() +
  theme(legend.position = "none",
        axis.title.y = element_text(vjust = -25),
        plot.background = element_blank(),
        panel.background = element_blank(),
        axis.title = element_text(size = 20, colour = "black"),
        axis.text = element_text(size = 12, colour = "black"),
        axis.ticks = element_line(colour = "black"),
        plot.margin = margin(t = 10, r = 10, b = 10, l = 10))
```

## Interlude: estimation of variances `r .emo("info")`

Do you remember that?

### Biased estimation of the variance for $x$

<center><font size = 5>
$$\frac{\displaystyle\sum^n_{i = 1}(x_i - \bar{x})^2}{n}$$
</font></center>

### Unbiased estimation of the variance for $x$

<center><font size = 5>
$$\frac{\displaystyle\sum^n_{i = 1}(x_i - \bar{x})^2}{n\color{red}{-1}}$$
</font></center>


## Interlude: estimation of variances `r .emo("proof")`

```{r vars}
set.seed(1)
bias_vars_list <- replicate(100000, {
  observations <- rnorm(n = 20, mean = 0, sd = sqrt(5))
  numerator <- sum((observations - mean(observations))^2)
  c(bias_var1    = (numerator/20) - 5,
    bias_var2 = (numerator/19) - 5)
}, simplify = FALSE)
bias_vars_df <- data.frame(do.call("rbind", bias_vars_list)) ## to turn the output as a clean dataframe
```

```{r vars2}
apply(bias_vars_df, 2, mean)
```

The bias corresponds to an underestimation stemming from looking at the variation around the mean estimate: the variation around the mean estimate is always smaller that the variation around the true mean in the population. This is because by construction the estimate of the mean is the value generally the closest to all **sampled** points.

The bias can be removed by considering $n - p$ in the denominator to correct for the bias when you estimate $p$ parameters ($p = 1$, when there is only one mean). 


## Estimating the variances for the error `r .emo("info")` `r .emo("practice")`

The variance of the error ($\sigma^2$) cannot be directly observed...

... but it can be estimated from the variance of the residuals ($\varepsilon_i$):

<center><font size = 5>
$$\hat{\sigma}^2 = \frac{\displaystyle\sum^n_{i = 1}\varepsilon_i^2}{n\color{red}{-p}}$$
</font></center>

```{r estimate var error}
sum(residuals(fit_alien_lm)^2) / (nrow(Alien) - ncol(model.matrix(fit_alien_lm)))
summary(fit_alien_lm)$sigma^2 ## same thing, simpler :-)
```

Note: this term is (somewhat confusingly) referred to as an unbiased estimate of the **residual variance**.

# Predictions

## Predicting the mean response using $X$ `r .emo("info")`

We here want to predict the average of large groups of future observations.

Example: prediction for the average size of Aliens having eaten 1, 1.5, 2, 2.5 and 3 humans.

```{r prediction by hand}
(new_design <- cbind(Intercept = 1, humans_eaten = seq(1, 3, by = 0.5)))
new_design %*% coef(fit_alien_lm)
```

## Same using `predict()` `r .emo("practice")`

The function `predict()` saves you the trouble of building a design matrix by hand:

```{r prediction with predict}
data_for_predict <- data.frame(humans_eaten = seq(1, 3, by = 0.5))
predict(fit_alien_lm, newdata = data_for_predict)
```

<br>

Note: the fact that you must feed the function `predict()` with predictors and not regressors is particularly useful when you have qualitative variable (see [exercises](./LM_intro_exercises.html)).


## Predicting the mean response with several predictors `r .emo("practice")`

When several predictors are involved, predictions can be computed differently...

Option 1: you can set non-focal variable to 0 for quantitative variables (or to the control group for qualitative variables):

```{r, predictions_multiple_pred1}
fit_apb <- lm(age ~ planet * bh_trips, data = Alien2)
(newd0 <- data.frame(planet = c("Chambr", "Riplyx", "Wickor"),
                     bh_trips = 0))
(pred0 <- predict(fit_apb, newdata = newd0))
```

## Predicting the mean response with several predictors `r .emo("practice")`

When several predictors are involved, predictions can be computed differently...

Option 2: you can set non-focal variable to their mean (for quantitative variables) or mode (for qualitative variables) within the set of all sampled observations:

```{r, predictions_multiple_pred2}
fit_apb <- lm(age ~ planet * bh_trips, data = Alien2)
(newd_marginal <- data.frame(planet = c("Chambr", "Riplyx", "Wickor"),
                             bh_trips = mean(Alien2$bh_trips)))
(pred_marginal <- predict(fit_apb, newdata = newd_marginal))
```


## Predicting the mean response with several predictors `r .emo("practice")`

When several predictors are involved, predictions can be computed differently...

Option 3: you can set non-focal variable to their mean (for quantitative variables) or mode (for qualitative variables) within the set of observations matching each given value for the focal predictor:

```{r, predictions_multiple_pred3}
fit_apb <- lm(age ~ planet * bh_trips, data = Alien2)
(newd_conditional <- data.frame(planet = c("Chambr", "Riplyx", "Wickor"),
                                bh_trips = tapply(Alien2$bh_trips, Alien2$planet, mean)))
(pred_conditional <- predict(fit_apb, newdata = newd_conditional))
```

## Predicting the mean response with several predictors `r .emo("info")`

When several predictors are involved, predictions can be computed differently...

And this can make a big difference:

```{r, predictions_all}
cbind(data.frame(planet = c("Chambr", "Riplyx", "Wickor"),
                 method = c("0", "marginal", "conditional")),
      pred0, pred_marginal, pred_conditional)
```

Nothing is incorrect, the different predictions simply represent answers to different questions!

Note: it could be difficult to identify what packages computing (or plotting) automatically predictions actually do.


## Predicting future observations `r .emo("info")`

Sometimes we do not want to predict the mean response (i.e. the average response value for a group of observations) but single future observations.

For this, you need to predict the mean response and then draw a realisation of the error.

Example:  prediction for the size of 3 aliens having eaten 1.5 humans:

```{r 3_aliens}
(pred_mean_response <- predict(fit_alien_lm, newdata = data.frame(humans_eaten = 1.5)))
set.seed(123)
rnorm(n = 3, mean = pred_mean_response, sd = summary(fit_alien_lm)$sigma)
```

##
<center> <img src="./Extrapolation.png" alt="xkcd" style="width: 300x;"/> </center>


# Summary

## What you need to remember `r .emo("goal")`

* how to write a linear model (using simple maths notation and R formulas)
* how to simulate the data according to simple linear models
* some key vocabulary (e.g. response variable, fitted values, residuals...)
* how to understand the meaning of parameters by investigating the design matrix
* how to manipulate design matrices via formula
* how to compute predictions

<br>

<div align="right">
[Exercises](./LM_intro_exercises.html)
</div>


# Table of contents

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro_course.html)
* 2.1 [Fitting procedure](./LM_fitting_course.html)
* 2.2 [Tests & Intervals](./LM_test_intervals_course.html)
* 2.3 [Assumptions & Outliers](./LM_assumptions_course.html)
* 2.4 [Let's practice](./LM_practice_exercises.html)

<br>

<div align="right">
[Back to main menu](./Title.html#2)
</div>
