---
title: "LM: Introduction"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{2.0 Linear models}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---
```{r setup, include=FALSE}
library(LM2GLMM)
options(width = 120)
```

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Point estimates](./LM_point_estimates.html)
* 2.2 [Uncertainty in point estimates](./LM_uncertainty.html)
* 2.3 [Tests](./LM_tests.html)
* 2.4 [Assumptions and Outliers](./LM_assumptions.html)
* 2.5 [Let's practice more](./LM_practice.html)


## You will learn in this session

* how to write a linear model (using two notations)
* what a response, a design matrix and model parameters are
* how to get the design matrix
* that the meaning of the parameter depends on the design matrix
* how to simulate the data for a linear model

# Definition and notations

## What is a Linear Model (LM)?

### What is a statistical model?

###***A statistical model represents, often in considerably idealized form, the data-generating process.*** [(wikipedia)](https://en.wikipedia.org/wiki/Statistical_model)
<br>

### What is a linear model?

###***The data-generating process is assumed to be a linear function: it is constructed from a set of terms by multiplying each term by a constant (a model parameter) and adding the results .***

## Mathematical notation of LM: simple notation

<center><font size="5"> $y_i = \beta_0 + \beta_1 \times x_{1,i} + \beta_2 \times x_{2,i} + \dots + \beta_{p} \times x_{p,i} + \epsilon_i$ </font></center>

<br>

* $y_i$ = the observations to explain / response variable / dependent variable
* $x_{j,i}$ = constants derived from the predictors / explanatory variables / independent variables
* $\beta_j$ = the regression coefficients / model parameters
* $\epsilon_i$ = the errors / residual errors


## Mathematical notation of LM: matrix notation

<center><font size = 8> $Y = X \beta + \epsilon$ </font></center>

$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \dots \\ y_n \end{bmatrix} =
\begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,p} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,p} \\
1 & x_{3,1} & x_{3,2} & \dots & x_{3,p} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \dots \\ \beta_p
\end{bmatrix} +
\begin{bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \dots \\ \epsilon_n
\end{bmatrix}
$$

<br>

* $Y$ = the vector of observations
* $X$ = a matrix called the design (or model) matrix
* $\beta$ = the vector of model parameters
* $\epsilon$ = the vector of errors

# The response

## What kind of responses LM can handle?

* one dimension: $n \times 1$
* continuous
* no other restriction

# The design matrix

## The design matrix

### It is the representation of the predictors!

<br>

* multi-dimensional: $n \times p$ with $n > p$
* deduced from the predictors
* known and measured without error
* columns should be linearly independent

<br>

Sad but ```TRUE```:

### You cannot interpret model parameters correctly without knowing the design matrix of the model!!!

## How to compute the design (or model) matrix?

```{r pred matrix}
somedata <- data.frame(x1 = 1:6)
(mm <- model.matrix(object = ~ x1, data = somedata))
```

<br>

The term "```~ x1```" is an object of class formula.

Note: the functions fitting linear models will call ```model.matrix()``` for you.

## Ploting the design matrix
```{r plot mm, fig.align = "center", fig.width = 5, fig.height = 5}
matplot(mm)
```

## A design matrix without intercept
```{r pred matrix 0}
model.matrix(object = ~ 0 + x1, data = somedata)
model.matrix(object = ~ -1 + x1, data = somedata)
```

## A design matrix with function calls

```{r pred matrix 1}
model.matrix(object = ~ log(x1) + poly(x1, 2, raw = TRUE), data = somedata)
```

## An alternative parametrization for polynomials

```{r pred matrix 1b}
model.matrix(object = ~ log(x1) + poly(x1, 2), data = somedata)
```

<br>

Looking at the design matrix shows whether some transformations are happening behind the scenes.
This is important as when it happens model parameters should be interpreted accordingly.

## A design matrix with a factor

```{r pred matrix 2}
somedata$x2 <- c(10, 11, 12, 13, 14, 15)
somedata$x3 <- factor(c("b", "b", "a", "a", "c", "c"))
model.matrix(object = ~ x1 + x2 + x3, data = somedata)
```

<br>

This parameterization of factors such as ```x3``` is called the "treatment contrast".

It is the easiest one to interpret and the one use by default in R (but not in all software).

## An alternative parametrization for the factor

```{r pred matrix 3}
model.matrix(object = ~ x1 + x2 + x3, data = somedata, contrasts.arg = list(x3 = "contr.helmert"))
```

<br>

Alternative parameterizations of the contrasts can ease convergence in GLM(M) (for continuous variable) or can allow for testing specific hypotheses (for qualitative variables). 

It will not change the predictions, the likelihood, the AIC, etc... only the parameter values and their interpretation.



## A design matrix with interactions

```{r pred matrix 4}
model.matrix(object = ~ x1 * x2 + x1 * x3, data = somedata)
```

<br>

Interactions allow consideration of the effect of one predictor dependent on the value taken by other(s). If interactions are not considered, effects are assumed to be independent.

# The model parameters

## Model parameters:

* one dimension: $p \times 1$ vector
* fixed non-observable quantities

<br>

### They are used to convert each column from the design matrix into units of the response variable.

<br>

### The meaning of the parameters always depends on the model matrix!!!


## Alternative parameterizations of equivalent design matrices

Alternative parameterization of equivalent design matrices lead to the same predictions!

This function converts parameters between alternative (but equivalent) design matrices:
```{r convert}
conv_betaXA_to_betaXB <- function(XA, XB, betaXA) {
  ## Test that inputs are OK:
  if (!any(dim(XA) == dim(XB)))   stop("design matrices differ in size")
  if (ncol(XA) != length(betaXA)) stop("betaXA of wrong length")
  ## Identify parameters that need to be converted
  id_col_keep <- which(apply(XA != XB, 2, sum) != 0) ## index columns to keep
  if (all(XA[, 1] == 1)) id_col_keep <- c(1, id_col_keep) ## add intercept
  id_col_drop <- setdiff(1:ncol(XA), id_col_keep) ## index columns to discard
  ## Conversion per se:
  betaXB_temp <- solve(coef(lm.fit(XA[, id_col_keep], XB[, id_col_keep])), betaXA[id_col_keep])
  ## Put back non converted parameters into output
  betaXB <- numeric(length(betaXA))
  betaXB[id_col_keep] <- betaXB_temp
  betaXB[id_col_drop] <- betaXA[id_col_drop]
  ## Output
  return(betaXB)
}
## homemade: this function may break in some legitimate cases...
```


## The example of polynomials: raw

```{r pred matrix poly a}
(DM_raw <- model.matrix(object = ~ poly(x1, 2, raw = TRUE), data = somedata))
DM_raw %*% c(1, 2, -2)  ## predictions using matrix multiplication with c(1, 2, -2) the model parameters
```

## The example of polynomials: conversion between orthogonal and raw

```{r convert poly}
(DM_ortho <- model.matrix(object = ~ poly(x1, 2), data = somedata))
conv_betaXA_to_betaXB(XA = DM_raw, XB = DM_ortho, betaXA = c(1, 2, -2))
```

## The example of polynomials: predictions are identical!

```{r pred matrix poly b}
DM_raw %*% c(1, 2, -2)  ## predictions
round(DM_ortho %*% c(-22.33333, -50.19960, -12.22020), 3)
```


## Interim: the product XTX

The product XTX is essential for many computations around linear models.

It represents all pairwise products between the columns of the design matrix.

```{r XTX_explained}
X <- matrix(c(1, 1, 1, 1, 2, 3, 4, 5, 6), byrow = FALSE, ncol = 3)
(XTX_1 <- t(X) %*% X)
XTX_2 <- crossprod(X)

XTX_3 <- matrix(0, nrow = 3, ncol = 3)

for (i in 1:3) for (j in 1:3) XTX_3[i, j] <- sum(X[, i]*X[, j])

identical(XTX_1, XTX_2) & identical(XTX_1, XTX_3)
```


## Why making things more complex?

### For technical reasons

```{r XTX}
crossprod(DM_raw)  ## high values mean that predictors are colinear -> will be hard to fit
zapsmall(crossprod(DM_ortho))  ## 0 means that predictors are orthogonal -> will be easy to fit
```

## Why making things more complex?

### To ease the interpretation

TO DO: discuss diff contrasts!


## treatment contrasts vs Helmert

```{r helmert conv}
DM_treat <- model.matrix(object = ~ x1 + x2 + x3, data = somedata)
DM_Helm <- model.matrix(object = ~ x1 + x2 + x3, data = somedata, contrasts.arg = list(x3 = "contr.helmert"))
beta_treat <- matrix(c(50, 1.5, 20, 2, 3))
(beta_Helm <- conv_betaXA_to_betaXB(XA = DM_treat, XB = DM_Helm, betaXA = beta_treat))
```

<div  class="columns-2">

```{r param MA}
DM_treat %*% beta_treat ## predictions
```

<br>

```{r param MB}
DM_Helm %*% beta_Helm ## predictions
```

</div>


## Let's practice!

Are these different representations of gender equivalent?

* ```"boy"``` vs ```"girl"```
* ```"male"``` vs ```"female"```
* ```0``` vs ```1```
* ```1``` vs ```2```
* ```TRUE``` vs ```FALSE```

Create all corresponding design matrices to be sure!

<br>

Same question with:

* ```"baby"``` vs ```"child"``` vs ```"adult"```
* ```0``` vs ```1``` vs ```2```
* ```1``` vs ```2``` vs ```3```


## Understanding factorial design through the design matrix

```{r factorial}
data(poison, package = "fastR")  ##  ?fastR::poison for description
poison$treat <- factor(poison$Treatment)
poison$poison <- factor(poison$Poison)
X <- model.matrix( ~ poison + treat, data = poison)
head(X)
```

## Understanding factorial design through the design matrix

```{r factorial 2}
crossprod(X)  ## compute t(X) %*% X, i.e. x_ij = sum(X[, i]*X[, j])
with(poison, table(treat, poison))
```

# The errors

## The errors

* one dimension: $n \times 1$ vector
* error = observation - model approximation ($Y - X\beta$)
* random variable
* we assume it Gaussian (normally distributed), independent (no autocorrelation) and identically distributed (homoscedastic)

<center><font size="8"> $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$</font></center>

<br>

With the covariance matrix $\sigma^2 I$ being the following $n \times n$ matrix:

$$
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2 \\
\end{bmatrix}
$$


# Illustration with the Alien dataset

## Imagine an ideal (unrealistic) situation

We know that the process generating the height of aliens can be
approximated by the following linear model:

* $\texttt{height}_i = 50 + 1.5 \times \texttt{humans_eaten}_{i} + \epsilon_i$
* $\epsilon_i \sim \mathcal{N}(0, \sigma^2 = 25)$

<br>

We can thus generate the data corresponding to 6 aliens that have eaten
between 1 and 6 humans.

```{r alien data}
set.seed(123L)
Alien <- data.frame(humans_eaten = sample(1:6))
Alien$error <- rnorm(n = 6, mean = 0, sd = sqrt(25))
Alien$size <- 50 + 1.5*Alien$humans_eaten + Alien$error
```

We could also write the last 2 lines as the following line, but we want to keep the error distinct here.
```{r alien data2, eval = FALSE}
Alien$size <- rnorm(n = 6, mean = 50 + 1.5*Alien$humans_eaten, sd = sqrt(25))
```

## Why simulate Aliens?

* it clarifies the meaning of the model parameters
* it allows for the study of the power and robustness of LM


## Computing the response with both notations

```{r alien data show}
Alien
```
<div class="columns-2">

### Simple notation
* $y_1 = 50 + 1.5 \times `r Alien$humans_eaten[1]` + `r Alien$error[1]`$
* $y_2 = 50 + 1.5 \times `r Alien$humans_eaten[2]` + `r Alien$error[2]`$
* $y_3 = 50 + 1.5 \times `r Alien$humans_eaten[3]` + `r Alien$error[3]`$
* $y_4 = 50 + 1.5 \times `r Alien$humans_eaten[4]` + `r Alien$error[4]`$
* $y_5 = 50 + 1.5 \times `r Alien$humans_eaten[5]` + `r Alien$error[5]`$
* $y_6 = 50 + 1.5 \times `r Alien$humans_eaten[6]` + `r Alien$error[6]`$

### Matrix notation
$$
Y = \begin{bmatrix}
1 & `r Alien$humans_eaten[1]` \\
1 & `r Alien$humans_eaten[2]` \\
1 & `r Alien$humans_eaten[3]` \\
1 & `r Alien$humans_eaten[4]` \\
1 & `r Alien$humans_eaten[5]` \\
1 & `r Alien$humans_eaten[6]`
\end{bmatrix}
\begin{bmatrix}
50 \\
1.5
\end{bmatrix}
+
\begin{bmatrix}
`r Alien$error[1]`\\
`r Alien$error[2]`\\
`r Alien$error[3]`\\
`r Alien$error[4]`\\
`r Alien$error[5]`\\
`r Alien$error[6]`
\end{bmatrix}
$$

</div>


## Let's practice!

<br>

### Compute the response in R.

### Constraint: use a single line of code maximum for each notation.

# Summary

## What you need to remember

* how to write a linear model (using two notations)
* what a response, a design matrix and model parameters are
* how to get the design matrix
* that the meaning of the parameter depends on the design matrix
* how to simulate the data for a linear model


# Table of contents

## The Linear Model: LM

* 2.0 [Introduction](./LM_intro.html)
* 2.1 [Point estimates](./LM_point_estimates.html)
* 2.2 [Uncertainty in point estimates](./LM_uncertainty.html)
* 2.3 [Tests](./LM_tests.html)
* 2.4 [Assumptions and Outliers](./LM_assumptions.html)
* 2.5 [Let's practice more](./LM_practice.html)


