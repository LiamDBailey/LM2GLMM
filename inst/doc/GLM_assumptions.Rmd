---
title: "GLM: Residuals & Assumptions"
author: "Alexandre Courtiol"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
vignette: >
  %\VignetteIndexEntry{3.2 Residuals & Assumptions}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
library(LM2GLMM)
library(spaMM)
library(car)
library(DHARMa)
library(pscl)
options(width = 120)
knitr::opts_chunk$set(cache = TRUE, cache.path = "./cache_knitr/GLM_resid/", fig.path = "./fig_knitr/GLM_resid/", fig.width = 5, fig.height = 5, fig.align = "center", error = TRUE)
```

## You will learn in this session

* that many residuals can be computed for GLM and that they are often useless
* that computing residuals by parametric bootstraps in the way out
* that traditional goodness of fit tests are bad
* that most assumptions behing GLM are similar to those for LM
* that General Additive Models (GAM) can be useful to improve linearity
* how to diagnose and tackle overdispersion, zero-augmentation and separation


## Our data and toy models

```{r}
set.seed(1L)
Aliens <- simulate_Aliens_GLM()
mod_gauss <- glm(size  ~ humans_eaten, family = gaussian(), data = Aliens)
mod_poiss <- glm(eggs  ~ humans_eaten, family = poisson(),  data = Aliens)
mod_binar <- glm(happy ~ humans_eaten, family = binomial(), data = Aliens)
mod_binom <- glm(cbind(blue_eyes, pink_eyes) ~ humans_eaten, family = binomial(), data = Aliens)
```

# Residuals

## Can we still use ```plot(model)``` ?

```{r}
par(mfrow = c(2, 2))
plot(mod_poiss)
```


## Can we still use ```plot(model)``` ?

```{r}
par(mfrow = c(2, 2))
plot(mod_binar)
```


## Can we still use ```plot(model)``` ?

```{r}
par(mfrow = c(2, 2))
plot(mod_binom)
```

## Many types of residuals can be computed

<div class="columns-2">

```{r}
rbind(
  residuals(mod_poiss, type = "deviance")[1:2],
  residuals(mod_poiss, type = "pearson")[1:2],
  residuals(mod_poiss, type = "working")[1:2],
  residuals(mod_poiss, type = "response")[1:2])
```

```{r}
rbind(
  residuals(mod_binom, type = "deviance")[1:2],
  residuals(mod_binom, type = "pearson")[1:2],
  residuals(mod_binom, type = "working")[1:2],
  residuals(mod_binom, type = "response")[1:2])
```

</div>

<br>

Note 1: we can also compute partial residuals, which we shall see later.

Note 2: this is also true for LM and Gaussian GLM, but it makes no difference in such case (except for partial residuals).

## Distribution

### They are not necessarily normaly distributed!

<div class="columns-2">

```{r, echo = FALSE}
par(las = 1)
plot(ecdf(residuals(mod_poiss, type = "working")), col = 3, main = "mod_poiss")
plot(ecdf(residuals(mod_poiss, type = "deviance")), col = 1, add = TRUE)
plot(ecdf(residuals(mod_poiss, type = "pearson")), col = 2, add = TRUE)
plot(ecdf(residuals(mod_poiss, type = "response")), col = 4, add = TRUE)
legend("bottomright", fill = 1:4, bty = "n", legend = c("deviance", "pearson", "working", "response"), title = "Type of residuals:")
```

```{r, echo = FALSE}
par(las = 1)
plot(ecdf(residuals(mod_binom, type = "working")), col = 3, main = "mod_binom")
plot(ecdf(residuals(mod_binom, type = "deviance")), col = 1, add = TRUE)
plot(ecdf(residuals(mod_binom, type = "pearson")), col = 2, add = TRUE)
plot(ecdf(residuals(mod_binom, type = "response")), col = 4, add = TRUE)
legend("topleft", fill = 1:4, bty = "n", legend = c("deviance", "pearson", "working", "response"), title = "Type of residuals:")
```

</div>


## Response residuals

These are the residuals that we saw in LM: observed - predicted. Simple but pretty much useless.

```{r}
rbind(residuals(mod_gauss, type = "response")[1:2],
      (mod_gauss$y - mod_gauss$fitted.values)[1:2])
rbind(residuals(mod_poiss, type = "response")[1:2],
      (mod_poiss$y - mod_poiss$fitted.values)[1:2])
rbind(residuals(mod_binom, type = "response")[1:2],
      (mod_binom$y - mod_binom$fitted.values)[1:2])
```


## Pearson residuals

These are response residuals scaled by the square root of the variance function and accounting for their prior weights.

```{r}
variances <- poisson()$variance(mod_poiss$fitted.values)
rbind(residuals(mod_poiss, type = "pearson")[1:2],
      (residuals(mod_poiss, type = "response") * sqrt(mod_poiss$prior.weights / variances))[1:2])

variances <- binomial()$variance(mod_binom$fitted.values)
rbind(residuals(mod_binom, type = "pearson")[1:2],
      (residuals(mod_binom, type = "response") * sqrt(mod_binom$prior.weights / variances))[1:2])
```

These residuals are often use to check the quality of the fit.


## Pearson residuals

The sum of the squared Pearson residuals gives the Pearson $\chi^2$ statistic of goodness of fit:

```{r}
(X <- sum(residuals(mod_binom, type = "pearson")^2))
```

This statistic is sometimes used to test the goodness of fit of the model or to test for overdispersion, but since its usage is often not optimal (e.g. it requires a lot of replications within each grouping) and the power if this test is poor, I would not recommend you to use such test.

```{r}
1 - pchisq(X, mod_poiss$df.residual)  ## Pearson goodness of fit test
X / mod_poiss$df.residual  ## measure of overdispersion
```


## Deviance residuals

These are the residuals that are minimized after the fit.

Contrary to those computed by ```family()$dev.resids``` they are here not squared.

<br>

```{r}
residuals(mod_poiss, type = "deviance")[1:2]
rbind((residuals(mod_poiss, type = "deviance")[1:2])^2,
      with(mod_poiss, poisson()$dev.resids(y = y, mu = fitted.values, wt = prior.weights))[1:2])
```


## Deviance residuals

The sum of the squared deviance residuals gives the residual deviance of the model fit:

```{r}
c(sum(residuals(mod_gauss, type = "deviance")^2), deviance(mod_gauss))
c(sum(residuals(mod_poiss, type = "deviance")^2), deviance(mod_poiss))
c(sum(residuals(mod_binom, type = "deviance")^2), deviance(mod_binom))
```

<br>

These residuals are sometimes also used to check the quality of the fit, in the exact same fashion as the Pearson's residuals, but again (and for the same reasons), I would not recommend you to do that.


## Testing the robustness of the goodness of fit tests

For ```mod_poiss```

<div class = "columns-2">

```{r gof 1, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_poiss, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  1 - pchisq(X, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 2, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_poiss, data = d)
  dev <- deviance(m)
  1 - pchisq(dev, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Testing the robustness of the goodness of fit tests

For ```mod_binar```

<div class = "columns-2">

```{r gof 3, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binar, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  1 - pchisq(X, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 4, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binar, data = d)
  dev <- deviance(m)
  1 - pchisq(dev, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Testing the robustness of the goodness of fit tests

For ```mod_binom```

<div class = "columns-2">

```{r gof 5, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binom, data = d)
  X <- sum(residuals(m, type = "pearson")^2)
  1 - pchisq(X, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

```{r gof 6, fig.width = 3, fig.height = 3}
p <- replicate(1000, {
  d <- simulate_Aliens_GLM()
  m <- update(mod_binom, data = d)
  dev <- deviance(m)
  1 - pchisq(dev, m$df.residual)
})
plot(ecdf(p))
abline(0, 1, lty = 2, col = "green")
```

</div>


## Working residuals

These are the residuals used during the last iteration of the iterative fitting procedure. Useless otherwise.

```{r}
rbind(mod_poiss$residuals[1:2],
      ((mod_poiss$y - mod_poiss$fitted.values)/poisson()$mu.eta(mod_poiss$linear.predictors))[1:2])

rbind(mod_binom$residuals[1:2],
      ((mod_binom$y - mod_binom$fitted.values)/binomial()$mu.eta(mod_binom$linear.predictors))[1:2])
```


## Partial residuals

These are residuals expressed at the level of each predictor.

```{r}
mod_UK_small <- glm(milk ~ drink + sex + cigarettes, data = UK[1:10, ], family = poisson())
residuals(mod_UK_small, type = "partial")
```


## Partial residuals

These are residuals expressed at the level of each predictor.

Computation:
```{r}
(p <- predict(mod_UK_small, type = "terms")) ## prediction expressed per predictor
c(sum(p[1, ]) + attr(p, "constant"), predict(mod_UK_small, type = "link")[1])
```


## Partial residuals

These are residuals expressed at the level of each predictor.

Computation:
```{r}
rbind((p + residuals(mod_UK_small, type = "working"))[1, ],
      residuals(mod_UK_small, type = "partial")[1, ])
```

<br>

Partial residuals could be useful to check for departure from linearity (if they were not computed on the basis of working residuals...).

It is probably more useful for LM in which working residuals and response residuals are the same.


## Partial residuals

```{r, fig.width = 10, fig.height = 4}
library(car)
par(mfrow = c(1, 3))
crPlots(mod_UK_small, terms = ~ drink)
crPlots(mod_UK_small, terms = ~ sex)
crPlots(mod_UK_small, terms = ~ cigarettes)
```


## Partial residuals

```{r, fig.width = 10, fig.height = 4}
mod_UK <- glm(milk ~ drink + sex + cigarettes, data = UK, family = poisson())
par(mfrow = c(1, 3))
crPlots(mod_UK, terms = ~ drink)
crPlots(mod_UK, terms = ~ sex)
crPlots(mod_UK, terms = ~ cigarettes)
```


# Residuals by parametric bootstrap: the useful ones!

## Computing residuals by parametric bootstrap

```{r, fig.height = 4, fig.width = 4}
set.seed(1L)
s <- simulate(mod_poiss, 1000)
r <- sapply(s, function(i) i + runif(nrow(mod_poiss$model), min = -0.5, max = 0.5))
hist(r[1, ], main = "distrib of first fitted value", nclass = 30)
abline(v = mod_poiss$y[1] + runif(1, min = -0.5, max = 0.5), col = "red", lwd = 2, lty = 2)
```


## Computing residuals by parametric bootstrap

```{r, fig.height = 4, fig.width = 4}
plot(ecdf1 <- ecdf(r[1, ]), main = "cdf of first fitted value")
noise <- runif(1, min = -0.5, max = 0.5)
simulated_residual_1 <-  ecdf1(mod_poiss$y[1] + noise)
segments(x0 = mod_poiss$y[1] + noise, y0 = 0, y1 =  simulated_residual_1, col = "red", lwd = 2)
arrows(x0 = mod_poiss$y[1] + noise, x1 = -1, y0 = simulated_residual_1, col = "red", lwd = 2)
```


## Computing residuals by parametric bootstrap

```{r, fig.height = 4, fig.width = 4}
plot(ecdf2 <- ecdf(r[2, ]), main = "cdf of second fitted value")
noise <- runif(1, min = -0.5, max = 0.5)
simulated_residual_2 <-  ecdf2(mod_poiss$y[2] + noise)
segments(x0 = mod_poiss$y[2] + noise, y0 = 0, y1 =  simulated_residual_2, col = "red", lwd = 2)
arrows(x0 = mod_poiss$y[2] + noise, x1 = -1, y0 = simulated_residual_2, col = "red", lwd = 2)
```


## Computing residuals by parametric bootstrap

```{r simu resid, fig.height = 4, fig.width = 4}
simulated_residuals <- rep(NA, nrow(mod_poiss$model))
for (i in 1:nrow(mod_poiss$model)) {
  ecdf_fn <- ecdf(r[i, ])
  simulated_residuals[i] <- ecdf_fn(mod_poiss$y[i] + runif(1, min = -0.5, max = 0.5))
}
```

<div class="columns-2">

```{r, fig.height = 4, fig.width = 4}
plot(simulated_residuals)
```

```{r, fig.height = 4, fig.width = 4}
plot(ecdf(simulated_residuals))
```

</div>


## Simulating residuals with ```DHARMa```

```{r simres poiss, fig.width = 7, fig.height = 4}
library(DHARMa)
mod_poiss_simres <- simulateResiduals(mod_poiss)
plot(mod_poiss_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres binar, fig.width = 7, fig.height = 4}
mod_binar_simres <- simulateResiduals(mod_binar)
plot(mod_binar_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres binom, fig.width = 7, fig.height = 4}
mod_binom_simres <- simulateResiduals(mod_binom)
plot(mod_binom_simres)
```


## Simulating residuals with ```DHARMa```

```{r simres UK, fig.width = 7, fig.height = 4}
mod_UK_simres <- simulateResiduals(mod_UK)
plot(mod_UK_simres)
```


# Assumptions

## The main assumptions

### Model structure

* linearity (for the linear predictor)
* lack of perfect multicollinearity (design matrix of full rank)
* predictor variables have fixed values

### Errors

* independence (no serial autocorrelation)
* lack of overdispersion and underdispersion


## How to test for linearity?

### It is very difficult...

but we may try to

* plot partial residuals
* plot simulated residuals & run an uniformity test
* plot the predictions from a GAM model


## Example of non-linearity

```{r}
set.seed(1L)
Aliens2 <- data.frame(humans_eaten = runif(100, min = 0, max = 15))
Aliens2$eggs <- rpois( n = 100, lambda = exp(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2))
mod_poiss2bad <- glm(eggs ~ humans_eaten, data = Aliens2, family = poisson()) ## mispecified model
(mod_poiss2good <- glm(eggs ~ poly(humans_eaten, 2, raw = TRUE), data = Aliens2, family = poisson())) ## good model
```


## Example of non-linearity
```{r,  fig.width = 4, fig.height = 4}
plot(I(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2) ~ Aliens2$humans_eaten,
     ylab = "eta", ylim = c(-1, 2))
data.for.pred <- data.frame(humans_eaten = 0:15)
points(predict(mod_poiss2good, newdata = data.for.pred) ~ I(0:15), col = "blue")
points(predict(mod_poiss2bad, newdata = data.for.pred) ~ I(0:15), col = "red")
```


## Example of non-linearity
```{r,  fig.width = 4, fig.height = 4}
plot(exp(1 + 0.2 * Aliens2$humans_eaten - 0.02 *  Aliens2$humans_eaten^2) ~ Aliens2$humans_eaten,
     ylab = "predicted number of eggs", ylim = c(0, 6))
points(predict(mod_poiss2good, newdata = data.for.pred, type = "response") ~ I(0:15), col = "blue")
points(predict(mod_poiss2bad, newdata = data.for.pred, type = "response") ~ I(0:15), col = "red")
```


## Example of non-linearity

```{r}
crPlots(mod_poiss2bad, terms = ~ humans_eaten)
```


## Example of non-linearity

```{r}
crPlots(mod_poiss, terms = ~ humans_eaten)  ## cannot do that for mod_poiss2good
```


## Example of non-linearity

```{r, fig.width = 7, fig.height = 4}
plot(s_bad <- simulateResiduals(mod_poiss2bad))
```


## Example of non-linearity

```{r, fig.width = 7, fig.height = 4}
plot(s_good <- simulateResiduals(mod_poiss2good))
```


## Example of non-linearity

### The lack of linearity is unfortunatly not detected here:
```{r}
testUniformity(s_bad)
testUniformity(s_good)
```


## Example of non-linearity

```{r, fig.width = 4, fig.height = 4, message = FALSE}
library(mgcv)
mod_poiss2GAM <- gam(eggs ~ s(humans_eaten), data = Aliens2, family = poisson())
plot(mod_poiss2GAM)
```


## How to test for independence?

You can run the Durbin Watson test on the simulated residuals:

```{r}
testTemporalAutocorrelation(s_good, time = mod_poiss2good$fitted.values, plot = FALSE)
```


## How to test for independence?

You can run the Durbin Watson test on the simulated residuals:

```{r}
testTemporalAutocorrelation(s_good, time = Aliens2$humans_eaten, plot = FALSE)
```

<br>

Note: As for LM it is good practice to test for the lack of serial autocorrelation along all your predictors and not just along the fitted values.


## A particular legitimate case of dependence

Survival times series that are discrete and complete can be analysed using a binomial (binary!) GLM.

Example: the study of the influence of rainfall on mortality (here A dies at 5 yrs old and B at 3 yrs old).

```{r}
data.frame(age = c(1:5, 1:5),
      id = c(rep("A", 5), rep("B", 5)),
      death = c(0, 0, 0, 0, 1, 0, 0, 1, NA, NA),
      annual_rain = c(100, 120, 310, 50, 200, 45, 100, 320, 100, 120))
```

In such a case, a random effect for the identity of the individual is not needed! So there is no need for mixed model if individuals are all equally independent from each other.



## Overdispersion / Underdispersion

### A GLM assumes a particular relationship between mu and var(mu)

```{r, fig.width = 10, fig.height = 3}
p <- seq(0, 1, 0.1)
lambda <- 0:10
theta <- 0:10
v_b <- binomial()$variance(p)
v_p <- poisson()$variance(lambda)
v_G <- Gamma()$variance(theta)
par(mfrow = c(1, 3), las = 2)
plot(v_b ~ p); plot(v_p ~ lambda); plot(v_G ~ theta)
```


## Overdispersion / Underdispersion

### Overdispersion = more variance than expected

* very common $\rightarrow$ increases false positive
* specially relevant for Poisson and Binomial
* irrelevant for the binary case! (don't look for it)

Usual suspects:

* lack of linearity
* unobserved heterogeneity
* zero-augmentation

<br>

### Underdispersion = less variance than expected

* rather rare  $\rightarrow$ increases false negative


## Overdispersion / Underdispersion

### A toy example

```{r}
set.seed(1L)
popA <- data.frame(humans_eaten = runif(50, min = 0, max = 15))
popA$eggs <- rpois(n = 50, lambda = exp(-1 + 0.05 * popA$humans_eaten))
popA$pop <- "A"
popB <- data.frame(humans_eaten = runif(50, min = 0, max = 15))
popB$eggs <- rpois(n = 50, lambda = exp(-3 + 0.4 * popB$humans_eaten))
popB$pop <- "B"
AliensMix <- rbind(popA, popB)
(mod_poissMix <- glm(eggs ~ humans_eaten, family = poisson(), data = AliensMix))
```

## Overdispersion / Underdispersion

### How to test it?

A widely used not so good way:

```{r}
cbind(disp = mod_poissMix$deviance / mod_poissMix$df.residual,
      pv = 1 - pchisq(mod_poissMix$deviance, mod_poissMix$df.residual))
```

A slightly better way:

```{r}
cbind(disp = sum(residuals(mod_poissMix, type = "pearson")^2) / mod_poissMix$df.residual,
      pv = 1 - pchisq(sum(residuals(mod_poissMix, type = "pearson")^2), mod_poissMix$df.residual))
```


## Overdispersion / Underdispersion

### How to test it?

A better way (?)

```{r}
r <- simulateResiduals(mod_poissMix, refit = TRUE)
testOverdispersion(r)
```


# Solving dispersion problems

## Potential solutions

* fix linearity issues
* fix heterogeneity issues (if you have the data)

```{r}
mod_poissMix2 <- glm(eggs ~ pop*humans_eaten, family = poisson(), data = AliensMix)
r2 <- simulateResiduals(mod_poissMix2, refit = TRUE)
testOverdispersion(r2)  ## you can change options to test for underdispersion
```

## Potential solutions

* fix linearity issues
* fix heterogeneity issues (if you have the data)
* model the overdispersion
* try another probability distribution
* there are specific solutions if the origin is zero-augmentation


## Modeling simply overdispersion

### For Poisson

Variance = $k \times \mu$

```{r}
mod_poissMixQ <- glm(eggs ~ humans_eaten, family = quasipoisson(), data = AliensMix)
summary(mod_poissMixQ)$coef
Anova(mod_poissMixQ, test = "F")
```


## Modeling overdispersion

### For Poisson

Variance = $k \times \mu$

It works, but we cannot do much with these type of fit:
```{r}
logLik(mod_poissMixQ)
```


## Modeling overdispersion

### For binomial

* irrelevant for the binary case!
* there is also ```quasibinomial()```, with the same limit (no likelihood)
* can happen in the general case

<br>

### Solution

* add a random effect with one level per individual (see GLMM)


## Probability distributions for count data

* Poisson ($V(\mu) = \mu$)
* Negative binomial ($V(\mu) = \mu + \frac{\mu^2}{\theta}$, if $\theta = 1$ this is the geometric model)
* Conway-Maxwell-Poisson ($V(\mu) =$ something complex)


## Poisson

### For comparison

```{r}
mod_poissMix <- glm(eggs ~ humans_eaten, family = poisson(), data = AliensMix)
sum(residuals(mod_poissMix, type = "pearson")^2) / mod_poissMix$df.residual
summary(mod_poissMix)$coefficients
```


## Negative binomial with ```MASS```

```{r}
library(MASS)
mod_poissMixNB <- glm.nb(eggs ~ humans_eaten, data = AliensMix)
sum(residuals(mod_poissMixNB, type = "pearson")^2) / mod_poissMix$df.residual
summary(mod_poissMixNB)$coefficients
c(AIC(mod_poissMix), AIC(mod_poissMixNB), logLik(mod_poissMixNB))
```


## Negative binomial with ```spaMM```

```{r}
negbin <- spaMM::negbin ## otherwise spaMM tries to use negbin from mgcv, which won't work
mod_poissMixSpaMM   <- fitme(eggs ~ humans_eaten, family = poisson(), data = AliensMix)
mod_poissMixNBSpaMM <- fitme(eggs ~ humans_eaten, family = negbin(), data = AliensMix)
summary(mod_poissMixNBSpaMM)
c(AIC(mod_poissMixSpaMM), AIC(mod_poissMixNBSpaMM), logLik(mod_poissMixNBSpaMM))
```


## Conway-Maxwell-Poisson

### It can fit both over- and under- dispersion!

* overdispersion: nu < 1 (nu = 0 corresponds to the geometric distribution)
* expected dispersion (i.e. Poisson): nu = 1
* underdispersion: nu > 1

Replicating Poisson:

```{r COMPoisson, warning = FALSE}
mod_poissMixCP <- glm(eggs ~ humans_eaten, family = COMPoisson(nu = 1), data = AliensMix)
mod_poissMixCP$coef
c(AIC(mod_poissMix), AIC(mod_poissMixNB), AIC(mod_poissMixCP))
```


## Conway-Maxwell-Poisson

### It can fit both over- and under- dispersion!

```{r COMPoisson2, warning = FALSE}
mod_poissMixCPSpaMM <- fitme(eggs ~ humans_eaten, family = COMPoisson(), data = AliensMix)
summary(mod_poissMixCPSpaMM)
c(AIC(mod_poissMixSpaMM), AIC(mod_poissMixNBSpaMM), AIC(mod_poissMixCP), AIC(mod_poissMixCPSpaMM))
```

Note: here we estimate nu, so it can take (much) longer time to fit!


## Comparison

```{r, echo = FALSE, warning = FALSE, fig.height = 6, fig.width = 6}
d <- data.frame(humans_eaten = seq(0, 15, length = 1000))
p <- predict(mod_poissMixSpaMM, newdata = d, intervals = "predVar")
plot(p ~ d$humans_eaten, ylim = range(mod_poissMixSpaMM$data$eggs), type = "l", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l")
p <- predict(mod_poissMixNBSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "red", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
p <- predict(mod_poissMixCPSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "blue", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
legend("topleft", fill = c("black", "red", "blue"), legend = c("Poisson", "Negative Binomial", "Conway-Maxwell-Poisson "), bty = "n")
```

## Comparison


```{r, eval = FALSE}
d <- data.frame(humans_eaten = seq(0, 15, length = 1000))
p <- predict(mod_poissMixSpaMM, newdata = d, intervals = "predVar")
plot(p ~ d$humans_eaten, ylim = range(mod_poissMixSpaMM$data$eggs), type = "l", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l")
p <- predict(mod_poissMixNBSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "red", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "red")
p <- predict(mod_poissMixCPSpaMM, newdata = d, intervals = "predVar")
points(p ~ d$humans_eaten, type = "l", col = "blue", lwd = 3)
points(attr(p, "interval")[, 1] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
points(attr(p, "interval")[, 2] ~ d$humans_eaten, lty = 2, type = "l", col = "blue")
legend("topleft", fill = c("black", "red", "blue"),
       legend = c("Poisson", "Negative Binomial", "Conway-Maxwell-Poisson "), bty = "n")
```


# Zero-augmentation

## Zero-augmentation in brief

### What is it?

It occurs for binomial or Poisson events when too many zeros occur.

### Why does it occur?

It can occur when the response results from a 2 (or more) steps process

Examples:

* detection issue (low counts are less detected, e.g. counting cells on microscope)
* biological (e.g. infection, then spread of microbes)


## How to tackle zero-augmentation?

You may try again to use the negative binomial or the COM-Poisson distribution, but an alternative that is often best is to combine two models (during the fitting procedure, not sequentially)!

### We have 2 main options for this:

Fit an hurdle model

* binomial  (or truncated count distribution) + truncated Poisson or truncated negative binomial
* a single source of zeros
* e.g. number of offspring

<br>

Fit a zero-inflation model

* binomial (or truncated count distribution) + Poisson or negative binomial
* two sources of zeros
* e.g. number of viruses in individuals (0 for unexposed, 0 for exposed with strong immune system)

### If you are not sure where the zeros come from, try both!


## Zero-augmented data

```{r}
set.seed(1L)
AliensZ <- simulate_Aliens_GLM(N = 1000)
AliensZ$eggs <- AliensZ$eggs * AliensZ$happy ## unhappy Aliens loose their eggs :-(
barplot(table(AliensZ$eggs))
```


## Poisson fit of zero-augmented data

```{r}
mod_Zpoiss <- glm(eggs ~ humans_eaten, data = AliensZ, family = poisson())
r <- simulateResiduals(mod_Zpoiss)
testZeroInflation(r, plot = FALSE)
mean(sum(AliensZ$eggs == 0) / sum(dpois(0, fitted(mod_Zpoiss))))
```


## Poisson fit of zero-augmented data

```{r, cache = FALSE}
testZeroInflation(r, plot = TRUE)
```


## Negative binomial fit of zero-augmented data

```{r}
mod_Znb <- glm.nb(eggs ~ humans_eaten, data = AliensZ)
r <- simulateResiduals(mod_Znb)
testZeroInflation(r, plot = FALSE)
```


## Negative binomial fit of zero-augmented data

```{r}
testZeroInflation(r, plot = TRUE)
```


## Hurdle fit of zero-augmented data

```{r, message = FALSE}
library(pscl)
mod_Zhurd1 <- hurdle(eggs ~ humans_eaten | humans_eaten, dist = "poisson", zero.dist = "binomial",
                     data = AliensZ)
mod_Zhurd2 <- hurdle(eggs ~ humans_eaten | 1, dist = "poisson", zero.dist = "binomial", data = AliensZ)
lmtest::lrtest(mod_Zhurd1, mod_Zhurd2)
mean(sum(AliensZ$eggs == 0) / sum(predict(mod_Zhurd1, type = "prob")[, 1]))
```


## Zero-inflation fit of zero-augmented data

```{r, message = FALSE}
mod_Zzi1 <- zeroinfl(eggs ~ humans_eaten | humans_eaten, dist = "poisson", data = AliensZ)
mod_Zzi2 <- zeroinfl(eggs ~ humans_eaten | 1, dist = "poisson", data = AliensZ)
lmtest::lrtest(mod_Zzi1, mod_Zzi2)
mean(sum(AliensZ$eggs == 0) / sum(predict(mod_Zzi1, type = "prob")[, 1]))
```


## Comparison

```{r}
cbind(Poisson = AIC(mod_Zpoiss),
      NegBin  = AIC(mod_Znb),
      Hurdle  = AIC(mod_Zhurd1),
      ZeroInf = AIC(mod_Zzi1))
tab <- rbind(Poisson = c(mod_Zpoiss$coefficients, NA, NA),
             NegBin = c(mod_Znb$coefficients, NA, NA),
             Hurdle = unlist(mod_Zhurd1$coefficients),  ## NOTE: the hurdle part predicts positive counts, not zeros!!
             ZeroInfl = unlist(mod_Zzi1$coefficients),  ## NOTE: the binary part predicts zeros, not counts!!
             Truth = c(attr(AliensZ, "param.eta")$eggs, attr(AliensZ, "param.eta")$happy))
colnames(tab) <- c("Int.count", "Slope.count", "Int.bin", "Slope.bin")
tab
```


# One last trap: separation

## The problem of separation in Binomial

### What is it?

Separation occurs when a level or combination of levels for categorical predictor, or when a particular threshold along a continuous predictor, predicts the outcomes perfectly.

<br>

### Complete or quasi separation?

From Wikipedia:

*For example, if the predictor X is continuous, and the outcome y = 1 for all observed x > 2. If the outcome values are perfectly determined by the predictor (e.g., y = 0 when x ≤ 2) then the condition "complete separation" is said to occur. If instead there is some overlap (e.g., y = 0 when x < 2, but y has observed values of 0 and 1 when x = 2) then "quasi-complete separation" occurs. A 2 × 2 table with an empty cell is an example of quasi-complete separation.*

### Consequences

* sometimes the model cannot be fitted
* when it does fit, the estimates and standard errors are usually wrong!


## The problem of separation in Binomial

```{r}
set.seed(1L)
n <- 50
test <- data.frame(happy = rbinom(2*n, prob = c(rep(0, n), rep(0.75, n)), size = 1), 
                   sp = factor(c(rep("sp1", n), rep("sp2", n))))
table(test$happy, test$sp)
```


## The problem of separation in Binomial

```{r}
mod <- glm(happy ~ sp, data = test, family = binomial())
```


## The problem of separation in Binomial

### Let's tweak the data in a conservative way
```{r}
test$happy[1] <- 1
table(test$happy, test$sp)
mod2 <- glm(happy ~ sp, data = test, family = binomial())
```


## The problem of separation in Binomial

```{r}
summary(mod2)
```

## The problem of separation in Binomial

### A solution for the binary case (not for binomial in general)

```{r, message = FALSE}
test$eggs[1] <- 0  ## restore the original data
library(safeBinaryRegression)  ## overload the glm function
mod3 <- glm(happy ~ sp, data = test, family = binomial())
summary(mod3)$coef
AIC(mod3)
```


## The problem of separation in Binomial

### A solution for the binary case (not for binomial in general)

```{r, message = FALSE}
library(spaMM)  ## with package e1071 installed!
mod4 <- fitme(happy ~ sp, data = test, family = binomial())
summary(mod4)
AIC(mod4)
```


## The problem of separation in Binomial

### A solution for the binary case (as well as for binomial in general!)

```{r, message = FALSE}
library(brglm)  ## there is a new version in development brglm2 but it still has issues
mod5 <- brglm(happy ~ sp, data = test, family = binomial())
summary(mod5)
AIC(mod5)
```


## What you need to remember

* that many residuals can be computed for GLM and that they are often useless
* that computing residuals by parametric bootstraps in the way out
* that traditional goodness of fit tests are bad
* that most assumptions behing GLM are similar to those for LM
* that General Additive Models (GAM) can be useful to improve linearity
* how to diagnose and tackle overdispersion, zero-augmentation and separation


# Table of content

## The Generalized Linear Model: GLM

* 3.0 [Introduction](./GLM_intro.html)
* 3.1 [Intervals & Tests](./GLM_intervals.html)
* 3.2 [Residuals & Assumptions](./GLM_assumptions.html)
* 3.3 [Let's practice more](./GLM_practice.html)

